File: ./config.yaml
---
#
# 1. (Required) Cluster details - Cluster represents the Kubernetes cluster layer and any additional customizations
#

# (Optional) Cluster name; affects Cilium and Talos
#   Default is "home-kubernetes"
bootstrap_cluster_name: "thepatriots"

# (Required) Generated schematic id from https://factory.talos.dev/
bootstrap_schematic_id: "ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515"

# (Required) The CIDR your nodes are on (e.g. 192.168.1.0/24)
bootstrap_node_network: "10.10.10.0/20"

# (Required) Use only 1, 3 or more ODD number of controller nodes, recommended is 3
#   Worker nodes are optional
bootstrap_node_inventory:
  - name: "kjd111"
    address: "10.10.12.111"
    controller: true
    disk: "/dev/sda"
    mac_addr: "0269cb6f84a4"
    mtu: "1500"
  - name: "kjd112"
    address: "10.10.12.112"
    controller: true
    disk: "/dev/sda"
    mac_addr: "0249f55f0575"
    mtu: "1500"
  - name: "kjd113"
    address: "10.10.12.113"
    controller: true
    disk: "/dev/sda"
    mac_addr: "027ce94bfbb6"
    mtu: "1500"
  - name: "kawg121"
    address: "10.10.12.121"
    controller: false
    disk: "/dev/sda"
    mac_addr: "025ffcaf8d15"
    mtu: "1500"
  - name: "kawg122"
    address: "10.10.12.122"
    controller: false
    disk: "/dev/sda"
    mac_addr: "02448ef5d041"
    mtu: "1500"
  - name: "kawg123"
    address: "10.10.12.123"
    controller: false
    disk: "/dev/sda"
    mac_addr: "0263b2e00a63"
    mtu: "1500"
  - name: "kawg124"
    address: "10.10.12.124"
    controller: false
    disk: "/dev/sda"
    mac_addr: "021579a6b815"
    mtu: "1500"

# (Optional) The DNS servers to use for the cluster nodes.
#   Default is pulled from your DHCP server.
bootstrap_dns_servers:
  - "10.10.10.1"

# (Optional) The NTP servers to use for the cluster nodes.
#   Default is pulled from your DHCP server.
bootstrap_ntp_servers: [time.google.com]

# (Required) The pod CIDR for the cluster, this must NOT overlap with any
#   existing networks and is usually a /16 (64K IPs).
# If you want to use IPv6 check the advanced flags below and be aware of
# https://github.com/onedr0p/cluster-template/issues/1148
bootstrap_pod_network: "10.244.0.0/16"

# (Required) The service CIDR for the cluster, this must NOT overlap with any
#   existing networks and is usually a /16 (64K IPs).
# If you want to use IPv6 check the advanced flags below and be aware of
# https://github.com/onedr0p/cluster-template/issues/1148
bootstrap_service_network: "10.96.0.0/16"

# (Required) The IP address of the Kube API, choose an available IP in
#   your nodes host network that is NOT being used. This is announced over L2.
bootstrap_controller_vip: "10.10.12.105"

# (Optional) Add additional SANs to the Kube API cert, this is useful
#   if you want to call the Kube API by hostname rather than IP
bootstrap_tls_sans:
  - "ThePatriots.local"

# (Optional) The default gateway for the nodes
#   Default is .1 which is derived from bootstrap_node_network (e.g. 192.168.1.1)
bootstrap_node_default_gateway: "10.10.10.1"

# (Optional) Add vlan tag to network master device, this is not needed if you tag ports on your switch with the VLAN
#   See: https://www.talos.dev/latest/advanced/advanced-networking/#vlans
bootstrap_vlan: ""

# (Required) Age Public Key (e.g. age1...)
# 1. Generate a new key with the following command:
#    > task sops:age-keygen
# 2. Copy the PUBLIC key and paste it below
bootstrap_sops_age_pubkey: "age1zddtwjmvl4ed3jm9e0jd0cnptmmla0z8fjvcccvu6c8jkdd2x9zq0dctqy"

# (Optional) Use cilium BGP control plane when L2 announcements won't traverse VLAN network segments.
#   Needs a BGP capable router setup with the node IPs as peers.
#   See: https://docs.cilium.io/en/latest/network/bgp-control-plane/
bootstrap_bgp:
  enabled: false
  # (Optional) If using multiple BGP peers add them here.
  #   Default is .1 derived from host_network: ['x.x.x.1']
  peers: []
  # (Required) Set the BGP Autonomous System Number for the router(s) and nodes.
  #   If these match, iBGP will be used. If not, eBGP will be used.
  peer_asn: "" # Router(s) AS
  local_asn: "" # Node(s) AS
  peer_port: 179 # BGP Port - default is TCP port 179
  # (Required) The advertised CIDR for the cluster, this must NOT overlap with any
  #   existing networks and is usually a /16 (64K IPs).
  # If you want to use IPv6 check the advanced flags below
  advertised_network: ""

# (Optional) Secureboot and TPM-based disk encryption
bootstrap_secureboot:
  # (Optional) Enable secureboot on UEFI systems. Not supported on x86 platforms in BIOS mode.
  #   See: https://www.talos.dev/latest/talos-guides/install/bare-metal-platforms/secureboot
  enabled: false
  # (Optional) Enable TPM-based disk encryption. Requires TPM 2.0
  #   See: https://www.talos.dev/v1.6/talos-guides/install/bare-metal-platforms/secureboot/#disk-encryption-with-tpm
  encrypt_disk_with_tpm: false

#
# 2. (Required) Flux details - Flux is used to manage the cluster configuration.
#

# (Required) GitHub repository URL
#   For a public repo use the 'https://' URL (e.g. "https://github.com/onedr0p/cluster-template.git")
#   For a private repo use the 'ssh://' URL (e.g. "ssh://git@github.com/onedr0p/cluster-template.git")
#     If using a private repo make sure to follow the instructions with the 'bootstrap_github_private_key' option below.
bootstrap_github_address: "https://github.com/Deenyoro/AG-Talos-Kubernetes"

# (Required) GitHub repository branch
bootstrap_github_branch: "main"

# (Required) Token for GitHub push-based sync
# 1. Generate a new token with the following command:
#    > openssl rand -hex 16
# 2. Copy the token and paste it below
bootstrap_github_webhook_token: "b7c85b2947bda260d8fff020ee521572"

# (Optional) Private key for Flux to access the GitHub repository
#   1. Generate a new key with the following command:
#      > ssh-keygen -t ecdsa -b 521 -C "github-deploy-key" -f github-deploy.key -q -P ""
#   2. Make sure to paste the public key from "github-deploy.key.pub" into
#      the deploy keys section of your GitHub repository settings.
#   3. Uncomment and paste the private key below
# bootstrap_github_private_key: |
#   -----BEGIN OPENSSH PRIVATE KEY-----
#   ...
#   -----END OPENSSH PRIVATE KEY-----

#
# 3. (Optional) Cloudflare details - Cloudflare is used for DNS, TLS certificates and tunneling.
#

bootstrap_cloudflare:
  # (Required) Disable to manually set up and use a different DNS provider - setting this
  #   to false will not deploy a network namespace or the workloads contained within.
  enabled: true
  # (Required) Cloudflare Domain
  domain: "kawalink.com"
  # (Required) Cloudflare API Token (NOT API Key)
  #   1. Head over to Cloudflare and create an API Token by going to
  #      https://dash.cloudflare.com/profile/api-tokens
  #   2. Under the `API Tokens` section click the blue `Create Token` button.
  #   3. Click the blue `Use template` button for the `Edit zone DNS` template.
  #   4. Name your token something like `home-kubernetes`
  #   5. Under `Permissions`, click `+ Add More` and add each permission below:
  #      `Zone - DNS - Edit`
  #      `Account - Cloudflare Tunnel - Read`
  #   6. Limit the permissions to a specific account and zone resources.
  #   7. Click the blue `Continue to Summary` button and then the blue `Create Token` button.
  #   8. Copy the token and paste it below.
  token: "RWTmAdF45ROveuDsB0R_HiiWRxINwIAhodVnaUsR"
  # (Required) Optionals for Cloudflare Acme
  acme:
    # (Required) Any email you want to be associated with the ACME account (used for TLS certs via letsencrypt.org)
    email: "dean@kawalink.com"
    # (Required) Use the ACME production server when requesting the wildcard certificate.
    #   By default, the ACME staging server is used. This is to prevent being rate-limited.
    #   Update this option to `true` when you have verified the staging certificate
    #   works and then re-run `task configure` and push your changes to GitHub.
    production: true
  # (Required) Provide LAN access to the cluster ingresses for internal ingress classes
  # The Load balancer IP for internal ingress, choose an available IP
  #   in your nodes host network that is NOT being used. This is announced over L2.
  ingress_vip: "10.10.12.110"
  # (Required) Gateway is used for providing DNS to your cluster on LAN
  # The Load balancer IP for k8s_gateway, choose an available IP
  #   in your nodes host network that is NOT being used. This is announced over L2.
  gateway_vip: "10.10.12.109"
  # (Required) Options for Cloudflare Tunnel
  # There's two methods to create a tunnel, via the CLI or the Cloudflare dashboard.
  #   1. Authenticate cloudflared to your domain with the following command:
  #     > cloudflared tunnel login
  #   2. Create the tunnel with the following command:
  #     > cloudflared tunnel create k8s
  tunnel:
    # (Required) Get the Cloudflared Tunnel ID with the following command:
    #   > jq -r .TunnelID ~/.cloudflared/*.json
    id: "3802e685-7734-4293-b9f1-dbb836581ac5"
    # (Required) Get the Cloudflare Account ID with the following command:
    #   > jq -r .AccountTag ~/.cloudflared/*.json
    account_id: "11130f68de674f870b3391763d200b31"
    # (Required) Get the Cloudflared Tunnel Secret with the following command:
    #   > jq -r .TunnelSecret ~/.cloudflared/*.json
    secret: "wQjzbtbR5+Pc2wV57XpuepgTjRXdvw7EAVa1QTxIGfc="
    # (Required) Provide WAN access to the cluster ingresses for external ingress classes
    # The Load balancer IP for external ingress, choose an available IP
    #   in your nodes host network that is NOT being used. This is announced over L2.
    ingress_vip: "10.10.12.108"
# (Optional) Feature gates are used to enable experimental features
# bootstrap_feature_gates:
#   # Enable Dual Stack IPv4 first
#   # IMPORTANT: I am looking for people to help maintain IPv6 support since I cannot test it.
#   #   Ref: https://github.com/onedr0p/cluster-template/issues/1148
#   # Keep in mind that Cilium does not currently support IPv6 L2 announcements.
#   dual_stack_ipv4_first: false

File: ./ceph-rbd-sc.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ceph-rbd
parameters:
  clusterID: rook-ceph
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
  csi.storage.k8s.io/fstype: ext4
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  imageFeatures: layering
  imageFormat: "2"
  pool: SSD4x1_pool
provisioner: rook-ceph.rbd.csi.ceph.com
reclaimPolicy: Delete
volumeBindingMode: Immediate
allowVolumeExpansion: true

File: ./.venv/lib/python3.12/site-packages/markdown_it/port.yaml
- package: markdown-it/markdown-it
  version: 13.0.1
  commit: e843acc9edad115cbf8cf85e676443f01658be08
  date: May 3, 2022
  notes:
    - Rename variables that use python built-in names, e.g.
      - `max` -> `maximum`
      - `len` -> `length`
      - `str` -> `string`
    - |
      Convert JS `for` loops to `while` loops
      this is generally the main difference between the codes,
      because in python you can't do e.g. `for {i=1;i<x;i++} {}`
    - |
      `env` is a common Python dictionary, and so does not have attribute access to keys,
      as with JavaScript dictionaries.
      `options` have attribute access only to core markdownit configuration options
    - |
      `Token.attrs` is a dictionary, instead of a list of lists.
      Upstream the list format is only used to guarantee order: https://github.com/markdown-it/markdown-it/issues/142,
      but in Python 3.7+ order of dictionaries is guaranteed.
      One should anyhow use the `attrGet`, `attrSet`, `attrPush` and `attrJoin` methods
      to manipulate `Token.attrs`, which have an identical signature to those upstream.
    - Use python version of `charCodeAt`
    - |
      Use `str` units instead of `int`s to represent Unicode codepoints.
      This provides a significant performance boost
    - |
      In markdown_it/rules_block/reference.py,
      record line range in state.env["references"] and add state.env["duplicate_refs"]
      This is to allow renderers to report on issues regarding references
    - |
      The `MarkdownIt.__init__` signature is slightly different for updating options,
      since you must always specify the config first, e.g.
      use `MarkdownIt("commonmark", {"html": False})` instead of `MarkdownIt({"html": False})`
    - The default configuration preset for `MarkdownIt` is "commonmark" not "default"
    - Allow custom renderer to be passed to `MarkdownIt`
    - |
      change render method signatures
      `func(tokens, idx, options, env, slf)` to
      `func(self, tokens, idx, options, env)`
    - |
      Extensions add render methods by format
      `MarkdownIt.add_render_rule(name, function, fmt="html")`,
      rather than `MarkdownIt.renderer.rules[name] = function`
      and renderers should declare a class property `__output__ = "html"`.
      This allows for extensibility to more than just HTML renderers
    - inline tokens in tables are assigned a map (this is helpful for propagation to children)

File: ./Taskfile.yaml
---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: "3"

vars:
  # Directories
  BOOTSTRAP_DIR: "{{.ROOT_DIR}}/bootstrap"
  KUBERNETES_DIR: "{{.ROOT_DIR}}/kubernetes"
  PRIVATE_DIR: "{{.ROOT_DIR}}/.private"
  SCRIPTS_DIR: "{{.ROOT_DIR}}/scripts"
  # Files
  AGE_FILE: "$HOME/.config/sops/age/keys.txt"
  BOOTSTRAP_CONFIG_FILE: "{{.ROOT_DIR}}/config.yaml"
  KUBECONFIG_FILE: "{{.ROOT_DIR}}/kubeconfig"
  MAKEJINJA_CONFIG_FILE: "{{.ROOT_DIR}}/makejinja.toml"
  PIP_REQUIREMENTS_FILE: "{{.ROOT_DIR}}/requirements.txt"
  SOPS_CONFIG_FILE: "{{.ROOT_DIR}}/.sops.yaml"
  # Binaries
  PYTHON_BIN: python3

env:
  KUBECONFIG: "{{.KUBECONFIG_FILE}}"
  PYTHONDONTWRITEBYTECODE: "1"
  SOPS_AGE_KEY_FILE: "{{.AGE_FILE}}"
  VIRTUAL_ENV: "{{.ROOT_DIR}}/.venv"

includes:
  kubernetes: .taskfiles/Kubernetes
  flux: .taskfiles/Flux
  repository: .taskfiles/Repository
  talos: .taskfiles/Talos
  sops: .taskfiles/Sops
  workstation: .taskfiles/Workstation
  user:
    taskfile: .taskfiles/User
    optional: true

tasks:

  default: task --list

  init:
    desc: Initialize configuration files
    cmds:
      - cp -n {{.BOOTSTRAP_CONFIG_FILE | replace ".yaml" ".sample.yaml"}} {{.BOOTSTRAP_CONFIG_FILE}}
      - echo "=== Configuration file copied ==="
      - echo "Proceed with updating the configuration files..."
      - echo "{{.BOOTSTRAP_CONFIG_FILE}}"
    status:
      - test -f {{.BOOTSTRAP_CONFIG_FILE}}
    silent: true

  configure:
    desc: Configure repository from bootstrap vars
    prompt: Any conflicting config in the kubernetes directory will be overwritten... continue?
    deps: ["workstation:direnv", "workstation:venv", "sops:age-keygen", "init"]
    cmds:
      - task: .template
      - task: sops:encrypt
      - task: .validate
      - |
          chmod +x ./merge_config.sh
          ./merge_config.sh

  .template:
    internal: true
    cmds:
      - "{{.VIRTUAL_ENV}}/bin/makejinja"
    preconditions:
      - msg: Missing virtual environment
        sh: test -d {{.VIRTUAL_ENV}}
      - msg: Missing Makejinja config file
        sh: test -f {{.MAKEJINJA_CONFIG_FILE}}
      - msg: Missing Makejinja plugin file
        sh: test -f {{.BOOTSTRAP_DIR}}/scripts/plugin.py
      - msg: Missing bootstrap config file
        sh: test -f {{.BOOTSTRAP_CONFIG_FILE}}

  .validate:
    internal: true
    cmds:
      - task: kubernetes:kubeconform
      - echo "=== Done rendering and validating YAML ==="
      - |
          if [[ $KUBECONFIG != "{{.KUBECONFIG_FILE}}" ]]; then
            echo "WARNING: KUBECONFIG is not set to the expected value, this may cause conflicts."
          fi
      - |
          if [[ $SOPS_AGE_KEY_FILE != "{{.AGE_FILE}}" ]]; then
            echo "WARNING: SOPS_AGE_KEY_FILE is not set to the expected value, this may cause conflicts."
          fi
      - |
          if test -f ~/.config/sops/age/keys.txt; then
            echo "WARNING: SOPS Age key found in home directory, this may cause conflicts."
          fi
    silent: true

File: ./.github/workflows/e2e.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/github-workflow.json
name: "e2e"

on:
  workflow_dispatch:
  pull_request:
    branches: ["main"]

concurrency:
  group: ${{ github.workflow }}-${{ github.event.number || github.ref }}
  cancel-in-progress: true

jobs:
  configure:
    if: ${{ github.repository == 'onedr0p/cluster-template' }}
    name: configure
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        config-files:
          - talos
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Homebrew
        id: setup-homebrew
        uses: Homebrew/actions/setup-homebrew@master

      - name: Setup Python
        uses: actions/setup-python@v5
        id: setup-python
        with:
          python-version: "3.11" # minimum supported version

      - name: Cache homebrew packages
        if: ${{ github.event_name == 'pull_request' }}
        uses: actions/cache@v4
        id: cache-homebrew-packages
        with:
          key: homebrew-${{ runner.os }}-${{ steps.setup-homebrew.outputs.gems-hash }}-${{ hashFiles('.taskfiles/Workstation/Brewfile') }}
          path: /home/linuxbrew/.linuxbrew

      - name: Cache venv
        if: ${{ github.event_name == 'pull_request' }}
        uses: actions/cache@v4
        with:
          key: venv-${{ runner.os }}-${{ steps.setup-python.outputs.python-version }}-${{ hashFiles('requirements.txt', 'requirements.yaml') }}
          path: .venv

      - name: Setup Workflow Tools
        if: ${{ github.event_name == 'pull_request' && steps.cache-homebrew-packages.outputs.cache-hit != 'true' }}
        shell: bash
        run: brew install go-task

      - name: Run Workstation Brew tasks
        if: ${{ github.event_name == 'pull_request' && steps.cache-homebrew-packages.outputs.cache-hit != 'true' }}
        shell: bash
        run: task workstation:brew

      - name: Run Workstation venv tasks
        shell: bash
        run: task workstation:venv

      - name: Run Workstation direnv tasks
        shell: bash
        run: task workstation:direnv

      - name: Run Sops Age key task
        shell: bash
        run: task sops:age-keygen

      - name: Run init tasks
        shell: bash
        run: |
          task init
          cp ./.github/tests/config-${{ matrix.config-files }}.yaml ./config.yaml
          export BOOTSTRAP_AGE_PUBLIC_KEY=$(sed -n 's/# public key: //gp' age.key)
          envsubst < ./config.yaml | sponge ./config.yaml

      - name: Run configure task
        shell: bash
        run: task configure --yes

      - name: Run repo clean and reset tasks
        shell: bash
        run: |
          task repository:clean
          task repository:reset --yes

File: ./.github/workflows/devcontainer.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/github-workflow.json
name: "devcontainer"

on:
  workflow_dispatch:
  push:
    branches: ["main"]
    paths: [".devcontainer/ci/**"]
  pull_request:
    branches: ["main"]
    paths: [".devcontainer/ci/**"]
  schedule:
    - cron: "0 0 * * *"

concurrency:
  group: ${{ github.workflow }}-${{ github.event.number || github.ref }}
  cancel-in-progress: true

jobs:
  devcontainer:
    if: ${{ github.repository == 'onedr0p/cluster-template' }}
    name: publish
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        with:
          platforms: linux/amd64 #,linux/arm64

      - if: ${{ github.event_name != 'pull_request' }}
        name: Login to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push
        uses: devcontainers/ci@v0.3
        env:
          BUILDX_NO_DEFAULT_ATTESTATIONS: true
        with:
          imageName: ghcr.io/${{ github.repository }}/devcontainer
          # cacheFrom: ghcr.io/${{ github.repository }}/devcontainer
          imageTag: base,latest
          platform: linux/amd64 #,linux/arm64
          configFile: .devcontainer/ci/devcontainer.json
          push: ${{ github.event_name == 'pull_request' && 'never' || 'always' }}

File: ./.github/workflows/release.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/github-workflow.json
name: "Release"

on:
  workflow_dispatch:
  schedule:
    - cron: "0 0 1 * *"

jobs:
  release:
    if: ${{ github.repository == 'onedr0p/cluster-template' }}
    name: Release
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Create Release
        shell: bash
        env:
          GITHUB_TOKEN: "${{ secrets.GITHUB_TOKEN }}"
        run: |
          # Retrieve previous release tag
          previous_tag="$(gh release list --limit 1 | awk '{ print $1 }')"
          previous_major="${previous_tag%%\.*}"
          previous_minor="${previous_tag#*.}"
          previous_minor="${previous_minor%.*}"
          previous_patch="${previous_tag##*.}"
          # Determine next release tag
          next_major_minor="$(date +'%Y').$(date +'%-m')"
          if [[ "${previous_major}.${previous_minor}" == "${next_major_minor}" ]]; then
              echo "Month release already exists for year, incrementing patch number by 1"
              next_patch="$((previous_patch + 1))"
          else
              echo "Month release does not exist for year, setting patch number to 0"
              next_patch="0"
          fi
          # Create release
          release_tag="${next_major_minor}.${next_patch}"
          gh release create "${release_tag}" \
              --repo="${GITHUB_REPOSITORY}" \
              --title="${release_tag}" \
              --generate-notes

File: ./.github/workflows/flux-diff.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/github-workflow.json
name: "Flux Diff"

on:
  pull_request:
    branches: ["main"]
    paths: ["kubernetes/**"]

concurrency:
  group: ${{ github.workflow }}-${{ github.event.number || github.ref }}
  cancel-in-progress: true

jobs:
  flux-diff:
    name: Flux Diff
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write
    strategy:
      matrix:
        paths: ["kubernetes"]
        resources: ["helmrelease", "kustomization"]
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          path: pull

      - name: Checkout Default Branch
        uses: actions/checkout@v4
        with:
          ref: "${{ github.event.repository.default_branch }}"
          path: default

      - name: Diff Resources
        uses: docker://ghcr.io/allenporter/flux-local:v6.1.1
        with:
          args: >-
            diff ${{ matrix.resources }}
            --unified 6
            --path /github/workspace/pull/${{ matrix.paths }}/flux
            --path-orig /github/workspace/default/${{ matrix.paths }}/flux
            --strip-attrs "helm.sh/chart,checksum/config,app.kubernetes.io/version,chart"
            --limit-bytes 10000
            --all-namespaces
            --sources "thepatriots"
            --output-file diff.patch

      - name: Generate Diff
        id: diff
        run: |
          cat diff.patch
          echo "diff<<EOF" >> $GITHUB_OUTPUT
          cat diff.patch >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - if: ${{ steps.diff.outputs.diff != '' }}
        name: Add comment
        uses: mshick/add-pr-comment@v2
        with:
          message-id: "${{ github.event.pull_request.number }}/${{ matrix.paths }}/${{ matrix.resources }}"
          message-failure: Diff was not successful
          message: |
            ```diff
            ${{ steps.diff.outputs.diff }}
            ```

File: ./.github/workflows/label-sync.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/github-workflow.json
name: "Label Sync"

on:
  workflow_dispatch:
  push:
    branches: ["main"]
    paths: [".github/labels.yaml"]

jobs:
  label-sync:
    name: Label Sync
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Sync Labels
        uses: EndBug/label-sync@v2
        with:
          config-file: .github/labels.yaml
          delete-other-labels: true

File: ./.github/workflows/labeler.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/github-workflow.json
name: "Labeler"

on:
  workflow_dispatch:
  pull_request_target:
    branches: ["main"]

jobs:
  labeler:
    name: Labeler
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write
    steps:
      - name: Labeler
        uses: actions/labeler@v5
        with:
          configuration-path: .github/labeler.yaml

File: ./.github/release.yaml
changelog:
  exclude:
    authors:
      - renovate

File: ./.github/labels.yaml
---
# Area
- { name: "area/bootstrap",          color: "0e8a16" }
- { name: "area/github",             color: "0e8a16" }
- { name: "area/kubernetes",         color: "0e8a16" }
- { name: "area/taskfile",           color: "0e8a16" }
# Distro
- { name: "distro/talos",            color: "ffc300" }
# Renovate
- { name: "renovate/container",      color: "027fa0" }
- { name: "renovate/github-action",  color: "027fa0" }
- { name: "renovate/github-release", color: "027fa0" }
- { name: "renovate/helm",           color: "027fa0" }
# Semantic Type
- { name: "type/patch",              color: "ffec19" }
- { name: "type/minor",              color: "ff9800" }
- { name: "type/major",              color: "f6412d" }
- { name: "type/break",              color: "f6412d" }
# Uncategorized
- { name: "hold/upstream",           color: "ee0701" }

File: ./.github/labeler.yaml
---
area/bootstrap:
  - changed-files:
      - any-glob-to-any-file: bootstrap/**/*
area/github:
  - changed-files:
      - any-glob-to-any-file: .github/**/*
area/kubernetes:
  - changed-files:
      - any-glob-to-any-file: kubernetes/**/*
area/taskfile:
  - changed-files:
      - any-glob-to-any-file: .taskfiles/**/*
      - any-glob-to-any-file: Taskfile*

File: ./.github/tests/config-talos.yaml
---
skip_tests: true

boostrap_talos:
  schematic_id: "376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba"
bootstrap_node_network: 10.10.10.0/24
bootstrap_node_default_gateway: 10.10.10.1
bootstrap_node_inventory:
  - name: k8s-controller-0
    address: 10.10.10.100
    controller: true
    disk: fake
    mac_addr: fake
  - name: k8s-worker-0
    address: 10.10.10.101
    controller: false
    disk: fake
    mac_addr: fake
bootstrap_dns_servers: ["1.1.1.1", "1.0.0.1"]
bootstrap_ntp_servers: ["time.cloudflare.com"]
bootstrap_pod_network: 10.69.0.0/16
bootstrap_service_network: 10.96.0.0/16
bootstrap_controller_vip: 10.10.10.254
bootstrap_tls_sans: ["fake"]
bootstrap_sops_age_pubkey: $BOOTSTRAP_AGE_PUBLIC_KEY
bootstrap_bgp:
  enabled: false
bootstrap_github_address: https://github.com/onedr0p/cluster-template
bootstrap_github_branch: main
bootstrap_github_webhook_token: fake
bootstrap_cloudflare:
  enabled: true
  domain: fake
  token: take
  acme:
    email: fake@example.com
    production: false
  tunnel:
    account_id: fake
    id: fake
    secret: fake
    ingress_vip: 10.10.10.252
  ingress_vip: 10.10.10.251
  gateway_vip: 10.10.10.253

File: ./config.sample.yaml
---

#
# 1. (Required) Cluster details - Cluster represents the Kubernetes cluster layer and any additional customizations
#

# (Optional) Cluster name; affects Cilium and Talos
#   Default is "home-kubernetes"
bootstrap_cluster_name: ""

# (Required) Generated schematic id from https://factory.talos.dev/
bootstrap_schematic_id: ""

# (Required) The CIDR your nodes are on (e.g. 192.168.1.0/24)
bootstrap_node_network: ""

# (Required) Use only 1, 3 or more ODD number of controller nodes, recommended is 3
#   Worker nodes are optional
bootstrap_node_inventory: []
    # - name: ""            # (Required) Name of the node (must match [a-z0-9-\]+)
    #   address: ""         # (Optional) IP address of the node (Remove if node has a static DHCP reservation)
    #   controller: true    # (Required) Set to true if this is a controller node
    #   disk: ""            # (Required) Device path or serial number of the disk for this node (talosctl disks -n <ip> --insecure)
    #   mac_addr: ""        # (Required) MAC address of the NIC for this node (talosctl get links -n <ip> --insecure)
    #   schematic_id: ""    # (Optional) Override the 'bootstrap_schematic_id' with a node specific schematic ID from https://factory.talos.dev/
    #   mtu: ""             # (Optional) MTU for the NIC, default is 1500
    #   manifests:          # (Optional) Additional manifests to include after MachineConfig
    #     - extra.yaml      #            See: https://www.talos.dev/v1.7/reference/configuration/extensions/extensionserviceconfig/
    #   extension_services: # (Optional) Additional talhelper ExtensionServices (supports talenv.sops.yaml envsubst)
    #     - name: name
    #       configFiles:
    #         - content: |-
    #             ...
    #       mountPath: ...
    #       environment:
    #         - key=value
    # ...

# (Optional) The DNS servers to use for the cluster nodes.
#   Default is pulled from your DHCP server.
# If using a local DNS server make sure it meets the following requirements:
#   1. your nodes can reach it
#   2. it is configured to forward requests to a public DNS server
#   3. you are not force redirecting DNS requests to it - this will break cert generation over DNS01
# If using multiple DNS servers make sure they are setup the same way, there is no
#   guarantee that the first DNS server will always be used for every lookup.
bootstrap_dns_servers: []

# (Optional) The NTP servers to use for the cluster nodes.
#   Default is pulled from your DHCP server.
bootstrap_ntp_servers: []

# (Required) The pod CIDR for the cluster, this must NOT overlap with any
#   existing networks and is usually a /16 (64K IPs).
# If you want to use IPv6 check the advanced flags below and be aware of
# https://github.com/onedr0p/cluster-template/issues/1148
bootstrap_pod_network: "10.69.0.0/16"

# (Required) The service CIDR for the cluster, this must NOT overlap with any
#   existing networks and is usually a /16 (64K IPs).
# If you want to use IPv6 check the advanced flags below and be aware of
# https://github.com/onedr0p/cluster-template/issues/1148
bootstrap_service_network: "10.96.0.0/16"

# (Required) The IP address of the Kube API, choose an available IP in
#   your nodes host network that is NOT being used. This is announced over L2.
bootstrap_controller_vip: ""

# (Optional) Add additional SANs to the Kube API cert, this is useful
#   if you want to call the Kube API by hostname rather than IP
bootstrap_tls_sans: []

# (Optional) The default gateway for the nodes
#   Default is .1 which is derrived from bootstrap_node_network (e.g. 192.168.1.1)
bootstrap_node_default_gateway: ""

# (Optional) Add vlan tag to network master device, this is not needed if you tag ports on your switch with the VLAN
#   See: https://www.talos.dev/latest/advanced/advanced-networking/#vlans
bootstrap_vlan: ""

# (Required) Age Public Key (e.g. age1...)
# 1. Generate a new key with the following command:
#    > task sops:age-keygen
# 2. Copy the PUBLIC key and paste it below
bootstrap_sops_age_pubkey: ""

# (Optional) Use cilium BGP control plane when L2 announcements won't traverse VLAN network segments.
#   Needs a BGP capable router setup with the node IPs as peers.
#   See: https://docs.cilium.io/en/latest/network/bgp-control-plane/
bootstrap_bgp:
  enabled: false
  # (Optional) If using multiple BGP peers add them here.
  #   Default is .1 derrived from host_network: ['x.x.x.1']
  peers: []
  # (Required) Set the BGP Autonomous System Number for the router(s) and nodes.
  #   If these match, iBGP will be used. If not, eBGP will be used.
  peer_asn: ""   # Router(s) AS
  local_asn: ""  # Node(s) AS
  peer_port: 179 # BGP Port - default is TCP port 179
  # (Required) The advertised CIDR for the cluster, this must NOT overlap with any
  #   existing networks and is usually a /16 (64K IPs).
  # If you want to use IPv6 check the advanced flags below
  advertised_network: ""

# (Optional) Secureboot and TPM-based disk encryption
bootstrap_secureboot:
  # (Optional) Enable secureboot on UEFI systems. Not supported on x86 platforms in BIOS mode.
  #   See: https://www.talos.dev/latest/talos-guides/install/bare-metal-platforms/secureboot
  enabled: false
  # (Optional) Enable TPM-based disk encryption. Requires TPM 2.0
  #   See: https://www.talos.dev/v1.6/talos-guides/install/bare-metal-platforms/secureboot/#disk-encryption-with-tpm
  encrypt_disk_with_tpm: false

#
# 2. (Required) Flux details - Flux is used to manage the cluster configuration.
#

# (Required) GitHub repository URL
#   For a public repo use the 'https://' URL (e.g. "https://github.com/onedr0p/cluster-template.git")
#   For a private repo use the 'ssh://' URL (e.g. "ssh://git@github.com/onedr0p/cluster-template.git")
#     If using a private repo make sure to following the instructions with the 'bootstrap_github_private_key' option below.
bootstrap_github_address: ""

# (Required) GitHub repository branch
bootstrap_github_branch: "main"

# (Required) Token for GitHub push-based sync
# 1. Generate a new token with the following command:
#    > openssl rand -hex 16
# 2. Copy the token and paste it below
bootstrap_github_webhook_token: ""

# (Optional) Private key for Flux to access the GitHub repository
#   1. Generate a new key with the following command:
#      > ssh-keygen -t ecdsa -b 521 -C "github-deploy-key" -f github-deploy.key -q -P ""
#   2. Make sure to paste public key from "github-deploy.key.pub" into
#      the deploy keys section of your GitHub repository settings.
#   3. Uncomment and paste the private key below
#   4. Optionally set your repository on GitHub to private
# bootstrap_github_private_key: |
#   -----BEGIN OPENSSH PRIVATE KEY-----
#   ...
#   -----END OPENSSH PRIVATE KEY-----

#
# 3. (Optional) Cloudflare details - Cloudflare is used for DNS, TLS certificates and tunneling.
#

bootstrap_cloudflare:
  # (Required) Disable to manually setup and use a different DNS provider - setting this
  #   to false will not deploy a network namespace or the workloads contained within.
  enabled: true
  # (Required) Cloudflare Domain
  domain: ""
  # (Required) Cloudflare API Token (NOT API Key)
  #   1. Head over to Cloudflare and create a API Token by going to
  #      https://dash.cloudflare.com/profile/api-tokens
  #   2. Under the `API Tokens` section click the blue `Create Token` button.
  #   3. Click the blue `Use template` button for the `Edit zone DNS` template.
  #   4. Name your token something like `home-kubernetes`
  #   5. Under `Permissions`, click `+ Add More` and add each permission below:
  #      `Zone - DNS - Edit`
  #      `Account - Cloudflare Tunnel - Read`
  #   6. Limit the permissions to a specific account and zone resources.
  #   7. Click the blue `Continue to Summary` button and then the blue `Create Token` button.
  #   8. Copy the token and paste it below.
  token: ""
  # (Required) Optionals for Cloudflare Acme
  acme:
    # (Required) Any email you want to be associated with the ACME account (used for TLS certs via letsencrypt.org)
    email: ""
    # (Required) Use the ACME production server when requesting the wildcard certificate.
    #   By default the ACME staging server is used. This is to prevent being rate-limited.
    #   Update this option to `true` when you have verified the staging certificate
    #   works and then re-run `task configure` and push your changes to Github.
    production: false
  # (Required) Provide LAN access to the cluster ingresses for internal ingress classes
  # The Load balancer IP for internal ingress, choose an available IP
  #   in your nodes host network that is NOT being used. This is announced over L2.
  ingress_vip: ""
  # (Required) Gateway is used for providing DNS to your cluster on LAN
  # The Load balancer IP for k8s_gateway, choose an available IP
  #   in your nodes host network that is NOT being used. This is announced over L2.
  gateway_vip: ""
  # (Required) Options for Cloudflare Tunnel
  # There's two methods to create a tunnel, via the CLI or the Cloudflare dashboard.
  #   1. Authenticate cloudflared to your domain with the following command:
  #     > cloudflared tunnel login
  #   2. Create the tunnel with the following command:
  #     > cloudflared tunnel create k8s
  tunnel:
    # (Required) Get the Cloudflared Tunnel ID with the following command:
    #   > jq -r .TunnelID ~/.cloudflared/*.json
    id: ""
    # (Required) Get the Cloudflare Account ID with the following command:
    #   > jq -r .AccountTag ~/.cloudflared/*.json
    account_id: ""
    # (Required) Get the Cloudflared Tunnel Secret with the following command:
    #   > jq -r .TunnelSecret ~/.cloudflared/*.json
    secret: ""
    # (Required) Provide WAN access to the cluster ingresses for external ingress classes
    # The Load balancer IP for external ingress, choose an available IP
    #   in your nodes host network that is NOT being used. This is announced over L2.
    ingress_vip: ""

# (Optional) Feature gates are used to enable experimental features
# bootstrap_feature_gates:
#   # Enable Dual Stack IPv4 first
#   # IMPORTANT: I am looking for people to help maintain IPv6 support since I cannot test it.
#   #   Ref: https://github.com/onedr0p/cluster-template/issues/1148
#   # Keep in mind that Cilium does not currently support IPv6 L2 announcements.
#   dual_stack_ipv4_first: false

File: ./test-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: invoiceninja-db-backup
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: ceph-rbd

File: ./kubernetes/bootstrap/talos/talsecret.sops.yaml
cluster:
id:
secret:
secrets:
bootstraptoken:
secretboxencryptionsecret:
trustdinfo:
token:
certs:
etcd:
crt:
key:
k8s:
crt:
key:
k8saggregator:
crt:
key:
k8sserviceaccount:
key:
os:
crt:
key:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
lastmodified:
mac:
pgp:
unencrypted_suffix:
version:

File: ./kubernetes/bootstrap/talos/clusterconfig/thepatriots-kcp113tr.yaml
version: v1alpha1
debug: false
persist: true
machine:
  type: controlplane
  token: 09jiom.ngx540hi270uj4s2
  ca:
    crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBaThMVzBsbnFHQmVCNmJwOHkraEx2REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qUXdOek14TWpFeE5ERTVXaGNOTXpRd056STVNakV4TkRFNVdqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQVBrdm0xRGxoZTBpZVRjY1VOQ2Q4eStCTCtldDVtRFJFRU4zCmpLekphWEpYbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRlAzbFl4eHdndi9RTXdtYQovME9LVXBrQlFUZklNQVVHQXl0bGNBTkJBSTZHZXlrRFVTbWJxUG1zV3FqUGtVTTQ1V3BlTlRmMXRMZ3VIMGpJCnE4NmhJRVNYU1lFeVlsZlkzMFQycTZjQmtsbmJMN3B0NVA5ZmhSU21lcTBNZHdvPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    key: LS0tLS1CRUdJTiBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0KTUM0Q0FRQXdCUVlESzJWd0JDSUVJTFZ4L2p5N0lpZGFlMHpoTlhMQkZFYmRoNUNGZ3VUS3dNK0IwSVV6Z3MwbwotLS0tLUVORCBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0K
  certSANs:
    - 10.10.12.105
    - 127.0.0.1
    - ThePatriots.local
  kubelet:
    image: ghcr.io/siderolabs/kubelet:v1.30.3
    extraArgs:
      rotate-server-certificates: "true"
    extraMounts:
      - destination: /var/openebs/local
        type: bind
        source: /var/openebs/local
        options:
          - bind
          - rshared
          - rw
    defaultRuntimeSeccompProfileEnabled: true
    nodeIP:
      validSubnets:
        - 10.10.12.0/20
    disableManifestsDirectory: true
  network:
    hostname: kcp113tr
    interfaces:
      - interface: bond0
        addresses:
          - 10.10.12.113/20
        routes:
          - network: 0.0.0.0/0
            gateway: 10.10.10.1
        bond:
          interfaces:
            - enx027ce94bfbb6
          mode: active-backup
        mtu: 1500
        vip:
          ip: 10.10.12.105
    nameservers:
      - 10.10.10.1
      - 10.10.10.1
    disableSearchDomain: true
  install:
    diskSelector:
      size: <=89GB
    image: factory.talos.dev/installer/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba:v1.7.5
    wipe: false
  files:
    - content: |-
        [plugins."io.containerd.grpc.v1.cri"]
          enable_unprivileged_ports = true
          enable_unprivileged_icmp = true
        [plugins."io.containerd.grpc.v1.cri".containerd]
          discard_unpacked_layers = false
        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          discard_unpacked_layers = false
      permissions: 0o0
      path: /etc/cri/conf.d/20-customization.part
      op: create
  time:
    disabled: false
    servers:
      - time.google.com
  sysctls:
    fs.inotify.max_queued_events: "65536"
    fs.inotify.max_user_instances: "8192"
    fs.inotify.max_user_watches: "524288"
    net.core.rmem_max: "2500000"
    net.core.wmem_max: "2500000"
  features:
    rbac: true
    stableHostname: true
    kubernetesTalosAPIAccess:
      enabled: true
      allowedRoles:
        - os:admin
      allowedKubernetesNamespaces:
        - system-upgrade
    apidCheckExtKeyUsage: true
    diskQuotaSupport: true
    kubePrism:
      enabled: true
      port: 7445
    hostDNS:
      enabled: true
      forwardKubeDNSToHost: true
      resolveMemberNames: true
cluster:
  id: opACGiNcYEtH87Og16A_PMiPC990hfMdAP46G0jxE3I=
  secret: +6uIL4OFR+KMnku6+IO4MbImrrETgd1ikUp7ENfxDhA=
  controlPlane:
    endpoint: https://10.10.12.105:6443
  clusterName: thepatriots
  network:
    cni:
      name: none
    dnsDomain: cluster.local
    podSubnets:
      - 10.244.0.0/16
    serviceSubnets:
      - 10.96.0.0/16
  token: kbn95j.d3v7cjbauqwb6b6a
  secretboxEncryptionSecret: qHwgxRuYyLn7d9cK4GePMjlrds7VrCzavxjTVEthc3I=
  ca:
    crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJpekNDQVRDZ0F3SUJBZ0lSQUtOWXNhZ0VzTk51WlZwK214Qnd0b013Q2dZSUtvWkl6ajBFQXdJd0ZURVQKTUJFR0ExVUVDaE1LYTNWaVpYSnVaWFJsY3pBZUZ3MHlOREEzTXpFeU1URTBNVGhhRncwek5EQTNNamt5TVRFMApNVGhhTUJVeEV6QVJCZ05WQkFvVENtdDFZbVZ5Ym1WMFpYTXdXVEFUQmdjcWhrak9QUUlCQmdncWhrak9QUU1CCkJ3TkNBQVR5VkduZ0RqdHFtaWJ1STZHRVE3dXVQSWpLT2JUbVorY29pTkRMV2trdDU2SW1iUytrUFVGYVppRFUKV005Nm41UmhlaHhsSjl2MmVKR3Bwd3ZDNzRaSW8yRXdYekFPQmdOVkhROEJBZjhFQkFNQ0FvUXdIUVlEVlIwbApCQll3RkFZSUt3WUJCUVVIQXdFR0NDc0dBUVVGQndNQ01BOEdBMVVkRXdFQi93UUZNQU1CQWY4d0hRWURWUjBPCkJCWUVGSEdpR2c2VklBL0ZkaGp0MUVrVHp2M0tBOEdiTUFvR0NDcUdTTTQ5QkFNQ0Ewa0FNRVlDSVFDQnRKVDIKTUJ3Y1RBQk4vZTN0emNSbm0zUHhZQlFuREdjYVM0RnErSXJyZmdJaEFQTUl6enJwOEQ1T1MySXBqQ3BWTnlScgpleitSdldxUEJBVlNCN3hPWHhDYQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    key: LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSU1ZekhKVFhoRHlVaWVPaWFOdXB4VEwybFd0LzJza2Z4TWpiK3d2a0pQcmtvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFOGxScDRBNDdhcG9tN2lPaGhFTzdyanlJeWptMDVtZm5LSWpReTFwSkxlZWlKbTB2cEQxQgpXbVlnMUZqUGVwK1VZWG9jWlNmYjluaVJxYWNMd3UrR1NBPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=
  aggregatorCA:
    crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJZRENDQVFhZ0F3SUJBZ0lSQVB0V2FRWWp3Qk9BL01qM1hNTnNKQ2N3Q2dZSUtvWkl6ajBFQXdJd0FEQWUKRncweU5EQTNNekV5TVRFME1UaGFGdzB6TkRBM01qa3lNVEUwTVRoYU1BQXdXVEFUQmdjcWhrak9QUUlCQmdncQpoa2pPUFFNQkJ3TkNBQVNEZmQzc2FUeDlLNGxxZXhodFRuZGpNejBicnFtUXArKzFRVVUwNkFQZDk0Z09QRDNEClNUTzZhVkxWZGJTS0gxRjk5MXpzb0hoWGFhVHFaR0N4cmNWd28yRXdYekFPQmdOVkhROEJBZjhFQkFNQ0FvUXcKSFFZRFZSMGxCQll3RkFZSUt3WUJCUVVIQXdFR0NDc0dBUVVGQndNQ01BOEdBMVVkRXdFQi93UUZNQU1CQWY4dwpIUVlEVlIwT0JCWUVGRU11S2kxRm9MNVN3N3RpdkcxQjd0YUVlN2dGTUFvR0NDcUdTTTQ5QkFNQ0EwZ0FNRVVDCklHSm1tMGRqMFE1RFJtamZ3dlhyajhCaGhPYkNxaTMrdFBMOTNuNVkrUE5WQWlFQXN0bnpERXh0blhkb0l6VjEKRlpaY0pNdnpYQTFjSk1UK3FBaWlmbGlLWEowPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    key: LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSUxMdnhQeXdwd0JYT0FtY2VlOEZJaDdWSzd0TEZZOHNLOHdKckt0TXBUZE1vQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFZzMzZDdHazhmU3VKYW5zWWJVNTNZek05RzY2cGtLZnZ0VUZGTk9nRDNmZUlEanc5dzBregp1bWxTMVhXMGloOVJmZmRjN0tCNFYybWs2bVJnc2EzRmNBPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=
  serviceAccount:
    key: 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
  apiServer:
    image: registry.k8s.io/kube-apiserver:v1.30.3
    certSANs:
      - 10.10.12.105
      - 10.10.12.105
      - 127.0.0.1
      - ThePatriots.local
    disablePodSecurityPolicy: true
    auditPolicy:
      apiVersion: audit.k8s.io/v1
      kind: Policy
      rules:
        - level: Metadata
  controllerManager:
    image: registry.k8s.io/kube-controller-manager:v1.30.3
    extraArgs:
      bind-address: 0.0.0.0
  proxy:
    disabled: true
    image: registry.k8s.io/kube-proxy:v1.30.3
  scheduler:
    image: registry.k8s.io/kube-scheduler:v1.30.3
    extraArgs:
      bind-address: 0.0.0.0
  discovery:
    enabled: true
    registries:
      kubernetes:
        disabled: false
      service:
        disabled: false
  etcd:
    ca:
      crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJmekNDQVNTZ0F3SUJBZ0lSQVBXNUFLOG9LaC94eTV0WGVpZGZJUWN3Q2dZSUtvWkl6ajBFQXdJd0R6RU4KTUFzR0ExVUVDaE1FWlhSalpEQWVGdzB5TkRBM016RXlNVEUwTVRoYUZ3MHpOREEzTWpreU1URTBNVGhhTUE4eApEVEFMQmdOVkJBb1RCR1YwWTJRd1dUQVRCZ2NxaGtqT1BRSUJCZ2dxaGtqT1BRTUJCd05DQUFTYU15SUV4b2NWCkFXK21sK3dwQi83THpaS1V1ODY4L29xOW92ZXQxTTE2eFhDbHA3ZWdjbCs2b0xtQUpFWnk5alVEOHY0dnJuZUQKZHBrdWh3NW9INmJhbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSApBd0VHQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkxOS2RmbEVIaHJDCjRGSE5mQW9PaUQ2TU83WFZNQW9HQ0NxR1NNNDlCQU1DQTBrQU1FWUNJUUNvWkZRWkVLazdzY3hUS3pNNmczY0cKbzFnM3F5aDVIQk5PWW02MG9YWm5QQUloQU5Wc1QxdUNKQWU5RnFQV2RxWG0rcHZUblhaNzVLa2xUYUxDS1NPdQpWajQyCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
      key: LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSUdXeEhwaWJJdnhZYUN5eDJZQnNlSW02Q2hoQSthZnZxVXMyRWFndVBSbUxvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFbWpNaUJNYUhGUUZ2cHBmc0tRZit5ODJTbEx2T3ZQNkt2YUwzcmRUTmVzVndwYWUzb0hKZgp1cUM1Z0NSR2N2WTFBL0wrTDY1M2czYVpMb2NPYUIrbTJnPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=
    extraArgs:
      listen-metrics-urls: http://0.0.0.0:2381
    advertisedSubnets:
      - 10.10.12.0/20
  coreDNS:
    disabled: true
  allowSchedulingOnControlPlanes: true

File: ./kubernetes/bootstrap/talos/clusterconfig/thepatriots-kcp111gw.yaml
version: v1alpha1
debug: false
persist: true
machine:
  type: controlplane
  token: 09jiom.ngx540hi270uj4s2
  ca:
    crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBaThMVzBsbnFHQmVCNmJwOHkraEx2REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qUXdOek14TWpFeE5ERTVXaGNOTXpRd056STVNakV4TkRFNVdqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQVBrdm0xRGxoZTBpZVRjY1VOQ2Q4eStCTCtldDVtRFJFRU4zCmpLekphWEpYbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRlAzbFl4eHdndi9RTXdtYQovME9LVXBrQlFUZklNQVVHQXl0bGNBTkJBSTZHZXlrRFVTbWJxUG1zV3FqUGtVTTQ1V3BlTlRmMXRMZ3VIMGpJCnE4NmhJRVNYU1lFeVlsZlkzMFQycTZjQmtsbmJMN3B0NVA5ZmhSU21lcTBNZHdvPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    key: LS0tLS1CRUdJTiBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0KTUM0Q0FRQXdCUVlESzJWd0JDSUVJTFZ4L2p5N0lpZGFlMHpoTlhMQkZFYmRoNUNGZ3VUS3dNK0IwSVV6Z3MwbwotLS0tLUVORCBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0K
  certSANs:
    - 10.10.12.105
    - 127.0.0.1
    - ThePatriots.local
  kubelet:
    image: ghcr.io/siderolabs/kubelet:v1.30.3
    extraArgs:
      rotate-server-certificates: "true"
    extraMounts:
      - destination: /var/openebs/local
        type: bind
        source: /var/openebs/local
        options:
          - bind
          - rshared
          - rw
    defaultRuntimeSeccompProfileEnabled: true
    nodeIP:
      validSubnets:
        - 10.10.12.0/20
    disableManifestsDirectory: true
  network:
    hostname: kcp111gw
    interfaces:
      - interface: bond0
        addresses:
          - 10.10.12.111/20
        routes:
          - network: 0.0.0.0/0
            gateway: 10.10.10.1
        bond:
          interfaces:
            - enx0269cb6f84a4
          mode: active-backup
        mtu: 1500
        vip:
          ip: 10.10.12.105
    nameservers:
      - 10.10.10.1
      - 10.10.10.1
    disableSearchDomain: true
  install:
    diskSelector:
      size: <=89GB
    image: factory.talos.dev/installer/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba:v1.7.5
    wipe: false
  files:
    - content: |-
        [plugins."io.containerd.grpc.v1.cri"]
          enable_unprivileged_ports = true
          enable_unprivileged_icmp = true
        [plugins."io.containerd.grpc.v1.cri".containerd]
          discard_unpacked_layers = false
        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          discard_unpacked_layers = false
      permissions: 0o0
      path: /etc/cri/conf.d/20-customization.part
      op: create
  time:
    disabled: false
    servers:
      - time.google.com
  sysctls:
    fs.inotify.max_queued_events: "65536"
    fs.inotify.max_user_instances: "8192"
    fs.inotify.max_user_watches: "524288"
    net.core.rmem_max: "2500000"
    net.core.wmem_max: "2500000"
  features:
    rbac: true
    stableHostname: true
    kubernetesTalosAPIAccess:
      enabled: true
      allowedRoles:
        - os:admin
      allowedKubernetesNamespaces:
        - system-upgrade
    apidCheckExtKeyUsage: true
    diskQuotaSupport: true
    kubePrism:
      enabled: true
      port: 7445
    hostDNS:
      enabled: true
      forwardKubeDNSToHost: true
      resolveMemberNames: true
cluster:
  id: opACGiNcYEtH87Og16A_PMiPC990hfMdAP46G0jxE3I=
  secret: +6uIL4OFR+KMnku6+IO4MbImrrETgd1ikUp7ENfxDhA=
  controlPlane:
    endpoint: https://10.10.12.105:6443
  clusterName: thepatriots
  network:
    cni:
      name: none
    dnsDomain: cluster.local
    podSubnets:
      - 10.244.0.0/16
    serviceSubnets:
      - 10.96.0.0/16
  token: kbn95j.d3v7cjbauqwb6b6a
  secretboxEncryptionSecret: qHwgxRuYyLn7d9cK4GePMjlrds7VrCzavxjTVEthc3I=
  ca:
    crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJpekNDQVRDZ0F3SUJBZ0lSQUtOWXNhZ0VzTk51WlZwK214Qnd0b013Q2dZSUtvWkl6ajBFQXdJd0ZURVQKTUJFR0ExVUVDaE1LYTNWaVpYSnVaWFJsY3pBZUZ3MHlOREEzTXpFeU1URTBNVGhhRncwek5EQTNNamt5TVRFMApNVGhhTUJVeEV6QVJCZ05WQkFvVENtdDFZbVZ5Ym1WMFpYTXdXVEFUQmdjcWhrak9QUUlCQmdncWhrak9QUU1CCkJ3TkNBQVR5VkduZ0RqdHFtaWJ1STZHRVE3dXVQSWpLT2JUbVorY29pTkRMV2trdDU2SW1iUytrUFVGYVppRFUKV005Nm41UmhlaHhsSjl2MmVKR3Bwd3ZDNzRaSW8yRXdYekFPQmdOVkhROEJBZjhFQkFNQ0FvUXdIUVlEVlIwbApCQll3RkFZSUt3WUJCUVVIQXdFR0NDc0dBUVVGQndNQ01BOEdBMVVkRXdFQi93UUZNQU1CQWY4d0hRWURWUjBPCkJCWUVGSEdpR2c2VklBL0ZkaGp0MUVrVHp2M0tBOEdiTUFvR0NDcUdTTTQ5QkFNQ0Ewa0FNRVlDSVFDQnRKVDIKTUJ3Y1RBQk4vZTN0emNSbm0zUHhZQlFuREdjYVM0RnErSXJyZmdJaEFQTUl6enJwOEQ1T1MySXBqQ3BWTnlScgpleitSdldxUEJBVlNCN3hPWHhDYQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    key: LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSU1ZekhKVFhoRHlVaWVPaWFOdXB4VEwybFd0LzJza2Z4TWpiK3d2a0pQcmtvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFOGxScDRBNDdhcG9tN2lPaGhFTzdyanlJeWptMDVtZm5LSWpReTFwSkxlZWlKbTB2cEQxQgpXbVlnMUZqUGVwK1VZWG9jWlNmYjluaVJxYWNMd3UrR1NBPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=
  aggregatorCA:
    crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJZRENDQVFhZ0F3SUJBZ0lSQVB0V2FRWWp3Qk9BL01qM1hNTnNKQ2N3Q2dZSUtvWkl6ajBFQXdJd0FEQWUKRncweU5EQTNNekV5TVRFME1UaGFGdzB6TkRBM01qa3lNVEUwTVRoYU1BQXdXVEFUQmdjcWhrak9QUUlCQmdncQpoa2pPUFFNQkJ3TkNBQVNEZmQzc2FUeDlLNGxxZXhodFRuZGpNejBicnFtUXArKzFRVVUwNkFQZDk0Z09QRDNEClNUTzZhVkxWZGJTS0gxRjk5MXpzb0hoWGFhVHFaR0N4cmNWd28yRXdYekFPQmdOVkhROEJBZjhFQkFNQ0FvUXcKSFFZRFZSMGxCQll3RkFZSUt3WUJCUVVIQXdFR0NDc0dBUVVGQndNQ01BOEdBMVVkRXdFQi93UUZNQU1CQWY4dwpIUVlEVlIwT0JCWUVGRU11S2kxRm9MNVN3N3RpdkcxQjd0YUVlN2dGTUFvR0NDcUdTTTQ5QkFNQ0EwZ0FNRVVDCklHSm1tMGRqMFE1RFJtamZ3dlhyajhCaGhPYkNxaTMrdFBMOTNuNVkrUE5WQWlFQXN0bnpERXh0blhkb0l6VjEKRlpaY0pNdnpYQTFjSk1UK3FBaWlmbGlLWEowPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    key: LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSUxMdnhQeXdwd0JYT0FtY2VlOEZJaDdWSzd0TEZZOHNLOHdKckt0TXBUZE1vQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFZzMzZDdHazhmU3VKYW5zWWJVNTNZek05RzY2cGtLZnZ0VUZGTk9nRDNmZUlEanc5dzBregp1bWxTMVhXMGloOVJmZmRjN0tCNFYybWs2bVJnc2EzRmNBPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=
  serviceAccount:
    key: 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
  apiServer:
    image: registry.k8s.io/kube-apiserver:v1.30.3
    certSANs:
      - 10.10.12.105
      - 10.10.12.105
      - 127.0.0.1
      - ThePatriots.local
    disablePodSecurityPolicy: true
    auditPolicy:
      apiVersion: audit.k8s.io/v1
      kind: Policy
      rules:
        - level: Metadata
  controllerManager:
    image: registry.k8s.io/kube-controller-manager:v1.30.3
    extraArgs:
      bind-address: 0.0.0.0
  proxy:
    disabled: true
    image: registry.k8s.io/kube-proxy:v1.30.3
  scheduler:
    image: registry.k8s.io/kube-scheduler:v1.30.3
    extraArgs:
      bind-address: 0.0.0.0
  discovery:
    enabled: true
    registries:
      kubernetes:
        disabled: false
      service:
        disabled: false
  etcd:
    ca:
      crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJmekNDQVNTZ0F3SUJBZ0lSQVBXNUFLOG9LaC94eTV0WGVpZGZJUWN3Q2dZSUtvWkl6ajBFQXdJd0R6RU4KTUFzR0ExVUVDaE1FWlhSalpEQWVGdzB5TkRBM016RXlNVEUwTVRoYUZ3MHpOREEzTWpreU1URTBNVGhhTUE4eApEVEFMQmdOVkJBb1RCR1YwWTJRd1dUQVRCZ2NxaGtqT1BRSUJCZ2dxaGtqT1BRTUJCd05DQUFTYU15SUV4b2NWCkFXK21sK3dwQi83THpaS1V1ODY4L29xOW92ZXQxTTE2eFhDbHA3ZWdjbCs2b0xtQUpFWnk5alVEOHY0dnJuZUQKZHBrdWh3NW9INmJhbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSApBd0VHQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkxOS2RmbEVIaHJDCjRGSE5mQW9PaUQ2TU83WFZNQW9HQ0NxR1NNNDlCQU1DQTBrQU1FWUNJUUNvWkZRWkVLazdzY3hUS3pNNmczY0cKbzFnM3F5aDVIQk5PWW02MG9YWm5QQUloQU5Wc1QxdUNKQWU5RnFQV2RxWG0rcHZUblhaNzVLa2xUYUxDS1NPdQpWajQyCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
      key: LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSUdXeEhwaWJJdnhZYUN5eDJZQnNlSW02Q2hoQSthZnZxVXMyRWFndVBSbUxvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFbWpNaUJNYUhGUUZ2cHBmc0tRZit5ODJTbEx2T3ZQNkt2YUwzcmRUTmVzVndwYWUzb0hKZgp1cUM1Z0NSR2N2WTFBL0wrTDY1M2czYVpMb2NPYUIrbTJnPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=
    extraArgs:
      listen-metrics-urls: http://0.0.0.0:2381
    advertisedSubnets:
      - 10.10.12.0/20
  coreDNS:
    disabled: true
  allowSchedulingOnControlPlanes: true

File: ./kubernetes/bootstrap/talos/clusterconfig/thepatriots-kawg124.yaml
version: v1alpha1
debug: false
persist: true
machine:
  type: worker
  token: 09jiom.ngx540hi270uj4s2
  ca:
    crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBaThMVzBsbnFHQmVCNmJwOHkraEx2REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qUXdOek14TWpFeE5ERTVXaGNOTXpRd056STVNakV4TkRFNVdqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQVBrdm0xRGxoZTBpZVRjY1VOQ2Q4eStCTCtldDVtRFJFRU4zCmpLekphWEpYbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRlAzbFl4eHdndi9RTXdtYQovME9LVXBrQlFUZklNQVVHQXl0bGNBTkJBSTZHZXlrRFVTbWJxUG1zV3FqUGtVTTQ1V3BlTlRmMXRMZ3VIMGpJCnE4NmhJRVNYU1lFeVlsZlkzMFQycTZjQmtsbmJMN3B0NVA5ZmhSU21lcTBNZHdvPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    key: ""
  certSANs:
    - 10.10.12.105
    - 127.0.0.1
    - ThePatriots.local
  kubelet:
    image: ghcr.io/siderolabs/kubelet:v1.30.3
    extraArgs:
      rotate-server-certificates: "true"
    extraMounts:
      - destination: /var/openebs/local
        type: bind
        source: /var/openebs/local
        options:
          - bind
          - rshared
          - rw
    defaultRuntimeSeccompProfileEnabled: true
    nodeIP:
      validSubnets:
        - 10.10.10.0/20
    disableManifestsDirectory: true
  network:
    hostname: kawg124
    interfaces:
      - interface: bond0
        addresses:
          - 10.10.12.124/20
        routes:
          - network: 0.0.0.0/0
            gateway: 10.10.10.1
        bond:
          interfaces:
            - enx021579a6b815
          mode: active-backup
        mtu: 1500
      - interface: bond1
        addresses:
          - 192.168.90.124/24
        routes:
          - network: 0.0.0.0/0
            gateway: 192.168.90.1
        bond:
          interfaces:
            - enxbc241128e2c4
          mode: active-backup
        mtu: 9000
    nameservers:
      - 10.10.10.1
      - 10.10.10.1
    disableSearchDomain: true
  install:
    diskSelector:
      size: <=89GB
    image: factory.talos.dev/installer/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba:v1.7.6
    wipe: false
  files:
    - content: |-
        [plugins."io.containerd.grpc.v1.cri"]
          enable_unprivileged_ports = true
          enable_unprivileged_icmp = true
        [plugins."io.containerd.grpc.v1.cri".containerd]
          discard_unpacked_layers = false
        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          discard_unpacked_layers = false
      permissions: 0o0
      path: /etc/cri/conf.d/20-customization.part
      op: create
  time:
    disabled: false
    servers:
      - time.google.com
  sysctls:
    fs.inotify.max_queued_events: "65536"
    fs.inotify.max_user_instances: "8192"
    fs.inotify.max_user_watches: "524288"
    net.core.rmem_max: "2500000"
    net.core.wmem_max: "2500000"
  features:
    rbac: true
    stableHostname: true
    apidCheckExtKeyUsage: true
    diskQuotaSupport: true
    kubePrism:
      enabled: true
      port: 7445
    hostDNS:
      enabled: true
      forwardKubeDNSToHost: true
      resolveMemberNames: true
cluster:
  id: opACGiNcYEtH87Og16A_PMiPC990hfMdAP46G0jxE3I=
  secret: +6uIL4OFR+KMnku6+IO4MbImrrETgd1ikUp7ENfxDhA=
  controlPlane:
    endpoint: https://10.10.12.105:6443
  network:
    cni:
      name: none
    dnsDomain: cluster.local
    podSubnets:
      - 10.244.0.0/16
    serviceSubnets:
      - 10.96.0.0/16
  token: kbn95j.d3v7cjbauqwb6b6a
  ca:
    crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJpekNDQVRDZ0F3SUJBZ0lSQUtOWXNhZ0VzTk51WlZwK214Qnd0b013Q2dZSUtvWkl6ajBFQXdJd0ZURVQKTUJFR0ExVUVDaE1LYTNWaVpYSnVaWFJsY3pBZUZ3MHlOREEzTXpFeU1URTBNVGhhRncwek5EQTNNamt5TVRFMApNVGhhTUJVeEV6QVJCZ05WQkFvVENtdDFZbVZ5Ym1WMFpYTXdXVEFUQmdjcWhrak9QUUlCQmdncWhrak9QUU1CCkJ3TkNBQVR5VkduZ0RqdHFtaWJ1STZHRVE3dXVQSWpLT2JUbVorY29pTkRMV2trdDU2SW1iUytrUFVGYVppRFUKV005Nm41UmhlaHhsSjl2MmVKR3Bwd3ZDNzRaSW8yRXdYekFPQmdOVkhROEJBZjhFQkFNQ0FvUXdIUVlEVlIwbApCQll3RkFZSUt3WUJCUVVIQXdFR0NDc0dBUVVGQndNQ01BOEdBMVVkRXdFQi93UUZNQU1CQWY4d0hRWURWUjBPCkJCWUVGSEdpR2c2VklBL0ZkaGp0MUVrVHp2M0tBOEdiTUFvR0NDcUdTTTQ5QkFNQ0Ewa0FNRVlDSVFDQnRKVDIKTUJ3Y1RBQk4vZTN0emNSbm0zUHhZQlFuREdjYVM0RnErSXJyZmdJaEFQTUl6enJwOEQ1T1MySXBqQ3BWTnlScgpleitSdldxUEJBVlNCN3hPWHhDYQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    key: ""
  discovery:
    enabled: true
    registries:
      kubernetes:
        disabled: false
      service:
        disabled: false

File: ./kubernetes/bootstrap/talos/clusterconfig/thepatriots-kawg122.yaml
version: v1alpha1
debug: false
persist: true
machine:
  type: worker
  token: 09jiom.ngx540hi270uj4s2
  ca:
    crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBaThMVzBsbnFHQmVCNmJwOHkraEx2REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qUXdOek14TWpFeE5ERTVXaGNOTXpRd056STVNakV4TkRFNVdqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQVBrdm0xRGxoZTBpZVRjY1VOQ2Q4eStCTCtldDVtRFJFRU4zCmpLekphWEpYbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRlAzbFl4eHdndi9RTXdtYQovME9LVXBrQlFUZklNQVVHQXl0bGNBTkJBSTZHZXlrRFVTbWJxUG1zV3FqUGtVTTQ1V3BlTlRmMXRMZ3VIMGpJCnE4NmhJRVNYU1lFeVlsZlkzMFQycTZjQmtsbmJMN3B0NVA5ZmhSU21lcTBNZHdvPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    key: ""
  certSANs:
    - 10.10.12.105
    - 127.0.0.1
    - ThePatriots.local
  kubelet:
    image: ghcr.io/siderolabs/kubelet:v1.30.3
    extraArgs:
      rotate-server-certificates: "true"
    extraMounts:
      - destination: /var/openebs/local
        type: bind
        source: /var/openebs/local
        options:
          - bind
          - rshared
          - rw
    defaultRuntimeSeccompProfileEnabled: true
    nodeIP:
      validSubnets:
        - 10.10.10.0/20
    disableManifestsDirectory: true
  network:
    hostname: kawg122
    interfaces:
      - interface: bond0
        addresses:
          - 10.10.12.122/20
        routes:
          - network: 0.0.0.0/0
            gateway: 10.10.10.1
        bond:
          interfaces:
            - enx02448ef5d041
          mode: active-backup
        mtu: 1500
      - interface: bond1
        addresses:
          - 192.168.90.122/24
        routes:
          - network: 0.0.0.0/0
            gateway: 192.168.90.1
        bond:
          interfaces:
            - enxbc241180885f
          mode: active-backup
        mtu: 9000
    nameservers:
      - 10.10.10.1
      - 10.10.10.1
    disableSearchDomain: true
  install:
    diskSelector:
      size: <=89GB
    image: factory.talos.dev/installer/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba:v1.7.6
    wipe: false
  files:
    - content: |-
        [plugins."io.containerd.grpc.v1.cri"]
          enable_unprivileged_ports = true
          enable_unprivileged_icmp = true
        [plugins."io.containerd.grpc.v1.cri".containerd]
          discard_unpacked_layers = false
        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          discard_unpacked_layers = false
      permissions: 0o0
      path: /etc/cri/conf.d/20-customization.part
      op: create
  time:
    disabled: false
    servers:
      - time.google.com
  sysctls:
    fs.inotify.max_queued_events: "65536"
    fs.inotify.max_user_instances: "8192"
    fs.inotify.max_user_watches: "524288"
    net.core.rmem_max: "2500000"
    net.core.wmem_max: "2500000"
  features:
    rbac: true
    stableHostname: true
    apidCheckExtKeyUsage: true
    diskQuotaSupport: true
    kubePrism:
      enabled: true
      port: 7445
    hostDNS:
      enabled: true
      forwardKubeDNSToHost: true
      resolveMemberNames: true
cluster:
  id: opACGiNcYEtH87Og16A_PMiPC990hfMdAP46G0jxE3I=
  secret: +6uIL4OFR+KMnku6+IO4MbImrrETgd1ikUp7ENfxDhA=
  controlPlane:
    endpoint: https://10.10.12.105:6443
  network:
    cni:
      name: none
    dnsDomain: cluster.local
    podSubnets:
      - 10.244.0.0/16
    serviceSubnets:
      - 10.96.0.0/16
  token: kbn95j.d3v7cjbauqwb6b6a
  ca:
    crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJpekNDQVRDZ0F3SUJBZ0lSQUtOWXNhZ0VzTk51WlZwK214Qnd0b013Q2dZSUtvWkl6ajBFQXdJd0ZURVQKTUJFR0ExVUVDaE1LYTNWaVpYSnVaWFJsY3pBZUZ3MHlOREEzTXpFeU1URTBNVGhhRncwek5EQTNNamt5TVRFMApNVGhhTUJVeEV6QVJCZ05WQkFvVENtdDFZbVZ5Ym1WMFpYTXdXVEFUQmdjcWhrak9QUUlCQmdncWhrak9QUU1CCkJ3TkNBQVR5VkduZ0RqdHFtaWJ1STZHRVE3dXVQSWpLT2JUbVorY29pTkRMV2trdDU2SW1iUytrUFVGYVppRFUKV005Nm41UmhlaHhsSjl2MmVKR3Bwd3ZDNzRaSW8yRXdYekFPQmdOVkhROEJBZjhFQkFNQ0FvUXdIUVlEVlIwbApCQll3RkFZSUt3WUJCUVVIQXdFR0NDc0dBUVVGQndNQ01BOEdBMVVkRXdFQi93UUZNQU1CQWY4d0hRWURWUjBPCkJCWUVGSEdpR2c2VklBL0ZkaGp0MUVrVHp2M0tBOEdiTUFvR0NDcUdTTTQ5QkFNQ0Ewa0FNRVlDSVFDQnRKVDIKTUJ3Y1RBQk4vZTN0emNSbm0zUHhZQlFuREdjYVM0RnErSXJyZmdJaEFQTUl6enJwOEQ1T1MySXBqQ3BWTnlScgpleitSdldxUEJBVlNCN3hPWHhDYQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    key: ""
  discovery:
    enabled: true
    registries:
      kubernetes:
        disabled: false
      service:
        disabled: false

File: ./kubernetes/bootstrap/talos/clusterconfig/thepatriots-kawg121.yaml
version: v1alpha1
debug: false
persist: true
machine:
  type: worker
  token: 09jiom.ngx540hi270uj4s2
  ca:
    crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBaThMVzBsbnFHQmVCNmJwOHkraEx2REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qUXdOek14TWpFeE5ERTVXaGNOTXpRd056STVNakV4TkRFNVdqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQVBrdm0xRGxoZTBpZVRjY1VOQ2Q4eStCTCtldDVtRFJFRU4zCmpLekphWEpYbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRlAzbFl4eHdndi9RTXdtYQovME9LVXBrQlFUZklNQVVHQXl0bGNBTkJBSTZHZXlrRFVTbWJxUG1zV3FqUGtVTTQ1V3BlTlRmMXRMZ3VIMGpJCnE4NmhJRVNYU1lFeVlsZlkzMFQycTZjQmtsbmJMN3B0NVA5ZmhSU21lcTBNZHdvPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    key: ""
  certSANs:
    - 10.10.12.105
    - 127.0.0.1
    - ThePatriots.local
  kubelet:
    image: ghcr.io/siderolabs/kubelet:v1.30.3
    extraArgs:
      rotate-server-certificates: "true"
    extraMounts:
      - destination: /var/openebs/local
        type: bind
        source: /var/openebs/local
        options:
          - bind
          - rshared
          - rw
    defaultRuntimeSeccompProfileEnabled: true
    nodeIP:
      validSubnets:
        - 10.10.10.0/20
    disableManifestsDirectory: true
  network:
    hostname: kawg121
    interfaces:
      - interface: bond0
        addresses:
          - 10.10.12.121/20
        routes:
          - network: 0.0.0.0/0
            gateway: 10.10.10.1
        bond:
          interfaces:
            - enx025ffcaf8d15
          mode: active-backup
        mtu: 1500
      - interface: bond1
        addresses:
          - 192.168.90.121/24
        routes:
          - network: 0.0.0.0/0
            gateway: 192.168.90.1
        bond:
          interfaces:
            - enxbc2411506ff7
          mode: active-backup
        mtu: 9000
    nameservers:
      - 10.10.10.1
      - 10.10.10.1
    disableSearchDomain: true
  install:
    diskSelector:
      size: <=89GB
    image: factory.talos.dev/installer/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba:v1.7.6
    wipe: false
  files:
    - content: |-
        [plugins."io.containerd.grpc.v1.cri"]
          enable_unprivileged_ports = true
          enable_unprivileged_icmp = true
        [plugins."io.containerd.grpc.v1.cri".containerd]
          discard_unpacked_layers = false
        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          discard_unpacked_layers = false
      permissions: 0o0
      path: /etc/cri/conf.d/20-customization.part
      op: create
  time:
    disabled: false
    servers:
      - time.google.com
  sysctls:
    fs.inotify.max_queued_events: "65536"
    fs.inotify.max_user_instances: "8192"
    fs.inotify.max_user_watches: "524288"
    net.core.rmem_max: "2500000"
    net.core.wmem_max: "2500000"
  features:
    rbac: true
    stableHostname: true
    apidCheckExtKeyUsage: true
    diskQuotaSupport: true
    kubePrism:
      enabled: true
      port: 7445
    hostDNS:
      enabled: true
      forwardKubeDNSToHost: true
      resolveMemberNames: true
cluster:
  id: opACGiNcYEtH87Og16A_PMiPC990hfMdAP46G0jxE3I=
  secret: +6uIL4OFR+KMnku6+IO4MbImrrETgd1ikUp7ENfxDhA=
  controlPlane:
    endpoint: https://10.10.12.105:6443
  network:
    cni:
      name: none
    dnsDomain: cluster.local
    podSubnets:
      - 10.244.0.0/16
    serviceSubnets:
      - 10.96.0.0/16
  token: kbn95j.d3v7cjbauqwb6b6a
  ca:
    crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJpekNDQVRDZ0F3SUJBZ0lSQUtOWXNhZ0VzTk51WlZwK214Qnd0b013Q2dZSUtvWkl6ajBFQXdJd0ZURVQKTUJFR0ExVUVDaE1LYTNWaVpYSnVaWFJsY3pBZUZ3MHlOREEzTXpFeU1URTBNVGhhRncwek5EQTNNamt5TVRFMApNVGhhTUJVeEV6QVJCZ05WQkFvVENtdDFZbVZ5Ym1WMFpYTXdXVEFUQmdjcWhrak9QUUlCQmdncWhrak9QUU1CCkJ3TkNBQVR5VkduZ0RqdHFtaWJ1STZHRVE3dXVQSWpLT2JUbVorY29pTkRMV2trdDU2SW1iUytrUFVGYVppRFUKV005Nm41UmhlaHhsSjl2MmVKR3Bwd3ZDNzRaSW8yRXdYekFPQmdOVkhROEJBZjhFQkFNQ0FvUXdIUVlEVlIwbApCQll3RkFZSUt3WUJCUVVIQXdFR0NDc0dBUVVGQndNQ01BOEdBMVVkRXdFQi93UUZNQU1CQWY4d0hRWURWUjBPCkJCWUVGSEdpR2c2VklBL0ZkaGp0MUVrVHp2M0tBOEdiTUFvR0NDcUdTTTQ5QkFNQ0Ewa0FNRVlDSVFDQnRKVDIKTUJ3Y1RBQk4vZTN0emNSbm0zUHhZQlFuREdjYVM0RnErSXJyZmdJaEFQTUl6enJwOEQ1T1MySXBqQ3BWTnlScgpleitSdldxUEJBVlNCN3hPWHhDYQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    key: ""
  discovery:
    enabled: true
    registries:
      kubernetes:
        disabled: false
      service:
        disabled: false

File: ./kubernetes/bootstrap/talos/clusterconfig/thepatriots-kawg123.yaml
version: v1alpha1
debug: false
persist: true
machine:
  type: worker
  token: 09jiom.ngx540hi270uj4s2
  ca:
    crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBaThMVzBsbnFHQmVCNmJwOHkraEx2REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qUXdOek14TWpFeE5ERTVXaGNOTXpRd056STVNakV4TkRFNVdqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQVBrdm0xRGxoZTBpZVRjY1VOQ2Q4eStCTCtldDVtRFJFRU4zCmpLekphWEpYbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRlAzbFl4eHdndi9RTXdtYQovME9LVXBrQlFUZklNQVVHQXl0bGNBTkJBSTZHZXlrRFVTbWJxUG1zV3FqUGtVTTQ1V3BlTlRmMXRMZ3VIMGpJCnE4NmhJRVNYU1lFeVlsZlkzMFQycTZjQmtsbmJMN3B0NVA5ZmhSU21lcTBNZHdvPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    key: ""
  certSANs:
    - 10.10.12.105
    - 127.0.0.1
    - ThePatriots.local
  kubelet:
    image: ghcr.io/siderolabs/kubelet:v1.30.3
    extraArgs:
      rotate-server-certificates: "true"
    extraMounts:
      - destination: /var/openebs/local
        type: bind
        source: /var/openebs/local
        options:
          - bind
          - rshared
          - rw
    defaultRuntimeSeccompProfileEnabled: true
    nodeIP:
      validSubnets:
        - 10.10.10.0/20
    disableManifestsDirectory: true
  network:
    hostname: kawg123
    interfaces:
      - interface: bond0
        addresses:
          - 10.10.12.123/20
        routes:
          - network: 0.0.0.0/0
            gateway: 10.10.10.1
        bond:
          interfaces:
            - enx0263b2e00a63
          mode: active-backup
        mtu: 1500
      - interface: bond1
        addresses:
          - 192.168.90.123/24
        routes:
          - network: 0.0.0.0/0
            gateway: 192.168.90.1
        bond:
          interfaces:
            - enxbc2411b16bd5
          mode: active-backup
        mtu: 9000
    nameservers:
      - 10.10.10.1
      - 10.10.10.1
    disableSearchDomain: true
  install:
    diskSelector:
      size: <=89GB
    image: factory.talos.dev/installer/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba:v1.7.6
    wipe: false
  files:
    - content: |-
        [plugins."io.containerd.grpc.v1.cri"]
          enable_unprivileged_ports = true
          enable_unprivileged_icmp = true
        [plugins."io.containerd.grpc.v1.cri".containerd]
          discard_unpacked_layers = false
        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          discard_unpacked_layers = false
      permissions: 0o0
      path: /etc/cri/conf.d/20-customization.part
      op: create
  time:
    disabled: false
    servers:
      - time.google.com
  sysctls:
    fs.inotify.max_queued_events: "65536"
    fs.inotify.max_user_instances: "8192"
    fs.inotify.max_user_watches: "524288"
    net.core.rmem_max: "2500000"
    net.core.wmem_max: "2500000"
  features:
    rbac: true
    stableHostname: true
    apidCheckExtKeyUsage: true
    diskQuotaSupport: true
    kubePrism:
      enabled: true
      port: 7445
    hostDNS:
      enabled: true
      forwardKubeDNSToHost: true
      resolveMemberNames: true
cluster:
  id: opACGiNcYEtH87Og16A_PMiPC990hfMdAP46G0jxE3I=
  secret: +6uIL4OFR+KMnku6+IO4MbImrrETgd1ikUp7ENfxDhA=
  controlPlane:
    endpoint: https://10.10.12.105:6443
  network:
    cni:
      name: none
    dnsDomain: cluster.local
    podSubnets:
      - 10.244.0.0/16
    serviceSubnets:
      - 10.96.0.0/16
  token: kbn95j.d3v7cjbauqwb6b6a
  ca:
    crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJpekNDQVRDZ0F3SUJBZ0lSQUtOWXNhZ0VzTk51WlZwK214Qnd0b013Q2dZSUtvWkl6ajBFQXdJd0ZURVQKTUJFR0ExVUVDaE1LYTNWaVpYSnVaWFJsY3pBZUZ3MHlOREEzTXpFeU1URTBNVGhhRncwek5EQTNNamt5TVRFMApNVGhhTUJVeEV6QVJCZ05WQkFvVENtdDFZbVZ5Ym1WMFpYTXdXVEFUQmdjcWhrak9QUUlCQmdncWhrak9QUU1CCkJ3TkNBQVR5VkduZ0RqdHFtaWJ1STZHRVE3dXVQSWpLT2JUbVorY29pTkRMV2trdDU2SW1iUytrUFVGYVppRFUKV005Nm41UmhlaHhsSjl2MmVKR3Bwd3ZDNzRaSW8yRXdYekFPQmdOVkhROEJBZjhFQkFNQ0FvUXdIUVlEVlIwbApCQll3RkFZSUt3WUJCUVVIQXdFR0NDc0dBUVVGQndNQ01BOEdBMVVkRXdFQi93UUZNQU1CQWY4d0hRWURWUjBPCkJCWUVGSEdpR2c2VklBL0ZkaGp0MUVrVHp2M0tBOEdiTUFvR0NDcUdTTTQ5QkFNQ0Ewa0FNRVlDSVFDQnRKVDIKTUJ3Y1RBQk4vZTN0emNSbm0zUHhZQlFuREdjYVM0RnErSXJyZmdJaEFQTUl6enJwOEQ1T1MySXBqQ3BWTnlScgpleitSdldxUEJBVlNCN3hPWHhDYQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    key: ""
  discovery:
    enabled: true
    registries:
      kubernetes:
        disabled: false
      service:
        disabled: false

File: ./kubernetes/bootstrap/talos/clusterconfig/thepatriots-kcp112tj.yaml
version: v1alpha1
debug: false
persist: true
machine:
  type: controlplane
  token: 09jiom.ngx540hi270uj4s2
  ca:
    crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBaThMVzBsbnFHQmVCNmJwOHkraEx2REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qUXdOek14TWpFeE5ERTVXaGNOTXpRd056STVNakV4TkRFNVdqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQVBrdm0xRGxoZTBpZVRjY1VOQ2Q4eStCTCtldDVtRFJFRU4zCmpLekphWEpYbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRlAzbFl4eHdndi9RTXdtYQovME9LVXBrQlFUZklNQVVHQXl0bGNBTkJBSTZHZXlrRFVTbWJxUG1zV3FqUGtVTTQ1V3BlTlRmMXRMZ3VIMGpJCnE4NmhJRVNYU1lFeVlsZlkzMFQycTZjQmtsbmJMN3B0NVA5ZmhSU21lcTBNZHdvPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    key: LS0tLS1CRUdJTiBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0KTUM0Q0FRQXdCUVlESzJWd0JDSUVJTFZ4L2p5N0lpZGFlMHpoTlhMQkZFYmRoNUNGZ3VUS3dNK0IwSVV6Z3MwbwotLS0tLUVORCBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0K
  certSANs:
    - 10.10.12.105
    - 127.0.0.1
    - ThePatriots.local
  kubelet:
    image: ghcr.io/siderolabs/kubelet:v1.30.3
    extraArgs:
      rotate-server-certificates: "true"
    extraMounts:
      - destination: /var/openebs/local
        type: bind
        source: /var/openebs/local
        options:
          - bind
          - rshared
          - rw
    defaultRuntimeSeccompProfileEnabled: true
    nodeIP:
      validSubnets:
        - 10.10.12.0/20
    disableManifestsDirectory: true
  network:
    hostname: kcp112tj
    interfaces:
      - interface: bond0
        addresses:
          - 10.10.12.112/20
        routes:
          - network: 0.0.0.0/0
            gateway: 10.10.10.1
        bond:
          interfaces:
            - enx0249f55f0575
          mode: active-backup
        mtu: 1500
        vip:
          ip: 10.10.12.105
    nameservers:
      - 10.10.10.1
      - 10.10.10.1
    disableSearchDomain: true
  install:
    diskSelector:
      size: <=89GB
    image: factory.talos.dev/installer/376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba:v1.7.5
    wipe: false
  files:
    - content: |-
        [plugins."io.containerd.grpc.v1.cri"]
          enable_unprivileged_ports = true
          enable_unprivileged_icmp = true
        [plugins."io.containerd.grpc.v1.cri".containerd]
          discard_unpacked_layers = false
        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          discard_unpacked_layers = false
      permissions: 0o0
      path: /etc/cri/conf.d/20-customization.part
      op: create
  time:
    disabled: false
    servers:
      - time.google.com
  sysctls:
    fs.inotify.max_queued_events: "65536"
    fs.inotify.max_user_instances: "8192"
    fs.inotify.max_user_watches: "524288"
    net.core.rmem_max: "2500000"
    net.core.wmem_max: "2500000"
  features:
    rbac: true
    stableHostname: true
    kubernetesTalosAPIAccess:
      enabled: true
      allowedRoles:
        - os:admin
      allowedKubernetesNamespaces:
        - system-upgrade
    apidCheckExtKeyUsage: true
    diskQuotaSupport: true
    kubePrism:
      enabled: true
      port: 7445
    hostDNS:
      enabled: true
      forwardKubeDNSToHost: true
      resolveMemberNames: true
cluster:
  id: opACGiNcYEtH87Og16A_PMiPC990hfMdAP46G0jxE3I=
  secret: +6uIL4OFR+KMnku6+IO4MbImrrETgd1ikUp7ENfxDhA=
  controlPlane:
    endpoint: https://10.10.12.105:6443
  clusterName: thepatriots
  network:
    cni:
      name: none
    dnsDomain: cluster.local
    podSubnets:
      - 10.244.0.0/16
    serviceSubnets:
      - 10.96.0.0/16
  token: kbn95j.d3v7cjbauqwb6b6a
  secretboxEncryptionSecret: qHwgxRuYyLn7d9cK4GePMjlrds7VrCzavxjTVEthc3I=
  ca:
    crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJpekNDQVRDZ0F3SUJBZ0lSQUtOWXNhZ0VzTk51WlZwK214Qnd0b013Q2dZSUtvWkl6ajBFQXdJd0ZURVQKTUJFR0ExVUVDaE1LYTNWaVpYSnVaWFJsY3pBZUZ3MHlOREEzTXpFeU1URTBNVGhhRncwek5EQTNNamt5TVRFMApNVGhhTUJVeEV6QVJCZ05WQkFvVENtdDFZbVZ5Ym1WMFpYTXdXVEFUQmdjcWhrak9QUUlCQmdncWhrak9QUU1CCkJ3TkNBQVR5VkduZ0RqdHFtaWJ1STZHRVE3dXVQSWpLT2JUbVorY29pTkRMV2trdDU2SW1iUytrUFVGYVppRFUKV005Nm41UmhlaHhsSjl2MmVKR3Bwd3ZDNzRaSW8yRXdYekFPQmdOVkhROEJBZjhFQkFNQ0FvUXdIUVlEVlIwbApCQll3RkFZSUt3WUJCUVVIQXdFR0NDc0dBUVVGQndNQ01BOEdBMVVkRXdFQi93UUZNQU1CQWY4d0hRWURWUjBPCkJCWUVGSEdpR2c2VklBL0ZkaGp0MUVrVHp2M0tBOEdiTUFvR0NDcUdTTTQ5QkFNQ0Ewa0FNRVlDSVFDQnRKVDIKTUJ3Y1RBQk4vZTN0emNSbm0zUHhZQlFuREdjYVM0RnErSXJyZmdJaEFQTUl6enJwOEQ1T1MySXBqQ3BWTnlScgpleitSdldxUEJBVlNCN3hPWHhDYQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    key: LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSU1ZekhKVFhoRHlVaWVPaWFOdXB4VEwybFd0LzJza2Z4TWpiK3d2a0pQcmtvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFOGxScDRBNDdhcG9tN2lPaGhFTzdyanlJeWptMDVtZm5LSWpReTFwSkxlZWlKbTB2cEQxQgpXbVlnMUZqUGVwK1VZWG9jWlNmYjluaVJxYWNMd3UrR1NBPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=
  aggregatorCA:
    crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJZRENDQVFhZ0F3SUJBZ0lSQVB0V2FRWWp3Qk9BL01qM1hNTnNKQ2N3Q2dZSUtvWkl6ajBFQXdJd0FEQWUKRncweU5EQTNNekV5TVRFME1UaGFGdzB6TkRBM01qa3lNVEUwTVRoYU1BQXdXVEFUQmdjcWhrak9QUUlCQmdncQpoa2pPUFFNQkJ3TkNBQVNEZmQzc2FUeDlLNGxxZXhodFRuZGpNejBicnFtUXArKzFRVVUwNkFQZDk0Z09QRDNEClNUTzZhVkxWZGJTS0gxRjk5MXpzb0hoWGFhVHFaR0N4cmNWd28yRXdYekFPQmdOVkhROEJBZjhFQkFNQ0FvUXcKSFFZRFZSMGxCQll3RkFZSUt3WUJCUVVIQXdFR0NDc0dBUVVGQndNQ01BOEdBMVVkRXdFQi93UUZNQU1CQWY4dwpIUVlEVlIwT0JCWUVGRU11S2kxRm9MNVN3N3RpdkcxQjd0YUVlN2dGTUFvR0NDcUdTTTQ5QkFNQ0EwZ0FNRVVDCklHSm1tMGRqMFE1RFJtamZ3dlhyajhCaGhPYkNxaTMrdFBMOTNuNVkrUE5WQWlFQXN0bnpERXh0blhkb0l6VjEKRlpaY0pNdnpYQTFjSk1UK3FBaWlmbGlLWEowPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    key: LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSUxMdnhQeXdwd0JYT0FtY2VlOEZJaDdWSzd0TEZZOHNLOHdKckt0TXBUZE1vQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFZzMzZDdHazhmU3VKYW5zWWJVNTNZek05RzY2cGtLZnZ0VUZGTk9nRDNmZUlEanc5dzBregp1bWxTMVhXMGloOVJmZmRjN0tCNFYybWs2bVJnc2EzRmNBPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=
  serviceAccount:
    key: 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
  apiServer:
    image: registry.k8s.io/kube-apiserver:v1.30.3
    certSANs:
      - 10.10.12.105
      - 10.10.12.105
      - 127.0.0.1
      - ThePatriots.local
    disablePodSecurityPolicy: true
    auditPolicy:
      apiVersion: audit.k8s.io/v1
      kind: Policy
      rules:
        - level: Metadata
  controllerManager:
    image: registry.k8s.io/kube-controller-manager:v1.30.3
    extraArgs:
      bind-address: 0.0.0.0
  proxy:
    disabled: true
    image: registry.k8s.io/kube-proxy:v1.30.3
  scheduler:
    image: registry.k8s.io/kube-scheduler:v1.30.3
    extraArgs:
      bind-address: 0.0.0.0
  discovery:
    enabled: true
    registries:
      kubernetes:
        disabled: false
      service:
        disabled: false
  etcd:
    ca:
      crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJmekNDQVNTZ0F3SUJBZ0lSQVBXNUFLOG9LaC94eTV0WGVpZGZJUWN3Q2dZSUtvWkl6ajBFQXdJd0R6RU4KTUFzR0ExVUVDaE1FWlhSalpEQWVGdzB5TkRBM016RXlNVEUwTVRoYUZ3MHpOREEzTWpreU1URTBNVGhhTUE4eApEVEFMQmdOVkJBb1RCR1YwWTJRd1dUQVRCZ2NxaGtqT1BRSUJCZ2dxaGtqT1BRTUJCd05DQUFTYU15SUV4b2NWCkFXK21sK3dwQi83THpaS1V1ODY4L29xOW92ZXQxTTE2eFhDbHA3ZWdjbCs2b0xtQUpFWnk5alVEOHY0dnJuZUQKZHBrdWh3NW9INmJhbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSApBd0VHQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkxOS2RmbEVIaHJDCjRGSE5mQW9PaUQ2TU83WFZNQW9HQ0NxR1NNNDlCQU1DQTBrQU1FWUNJUUNvWkZRWkVLazdzY3hUS3pNNmczY0cKbzFnM3F5aDVIQk5PWW02MG9YWm5QQUloQU5Wc1QxdUNKQWU5RnFQV2RxWG0rcHZUblhaNzVLa2xUYUxDS1NPdQpWajQyCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
      key: LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSUdXeEhwaWJJdnhZYUN5eDJZQnNlSW02Q2hoQSthZnZxVXMyRWFndVBSbUxvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFbWpNaUJNYUhGUUZ2cHBmc0tRZit5ODJTbEx2T3ZQNkt2YUwzcmRUTmVzVndwYWUzb0hKZgp1cUM1Z0NSR2N2WTFBL0wrTDY1M2czYVpMb2NPYUIrbTJnPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=
    extraArgs:
      listen-metrics-urls: http://0.0.0.0:2381
    advertisedSubnets:
      - 10.10.12.0/20
  coreDNS:
    disabled: true
  allowSchedulingOnControlPlanes: true

File: ./kubernetes/bootstrap/talos/talconfig_backup.yaml
# yaml-language-server: $schema=https://raw.githubusercontent.com/budimanjojo/talhelper/master/pkg/config/schemas/talconfig.json
---
# renovate: datasource=docker depName=ghcr.io/siderolabs/installer
talosVersion: v1.9.0
# renovate: datasource=docker depName=ghcr.io/siderolabs/kubelet
kubernetesVersion: v1.31.2

clusterName: "thepatriots"
endpoint: https://10.10.12.105:6443
clusterPodNets:
  - "10.244.0.0/16"
clusterSvcNets:
  - "10.96.0.0/16"
additionalApiServerCertSans: &sans
  - "10.10.12.105"
  - 127.0.0.1 # KubePrism
  - "ThePatriots.local"
additionalMachineCertSans: *sans

# Disable built-in Flannel to use Cilium
cniConfig:
  name: none

nodes:
  - hostname: "kcp111"
    ipAddress: "10.10.12.111"
    installDisk: "/dev/sda"
    talosImageURL: factory.talos.dev/installer/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515
    controlPlane: true
    networkInterfaces:
      - deviceSelector:
          hardwareAddr: "0269cb6f84a4"
        dhcp: false
        addresses:
          - "10.10.12.111/20"
        routes:
          - network: 0.0.0.0/0
            gateway: "10.10.10.1"
        mtu: 1500
        vip:
          ip: "10.10.12.105"
  - hostname: "kcp112"
    ipAddress: "10.10.12.112"
    installDisk: "/dev/sda"
    talosImageURL: factory.talos.dev/installer/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515
    controlPlane: true
    networkInterfaces:
      - deviceSelector:
          hardwareAddr: "0249f55f0575"
        dhcp: false
        addresses:
          - "10.10.12.112/20"
        routes:
          - network: 0.0.0.0/0
            gateway: "10.10.10.1"
        mtu: 1500
        vip:
          ip: "10.10.12.105"
  - hostname: "kcp113"
    ipAddress: "10.10.12.113"
    installDisk: "/dev/sda"
    talosImageURL: factory.talos.dev/installer/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515
    controlPlane: true
    networkInterfaces:
      - deviceSelector:
          hardwareAddr: "027ce94bfbb6"
        dhcp: false
        addresses:
          - "10.10.12.113/20"
        routes:
          - network: 0.0.0.0/0
            gateway: "10.10.10.1"
        mtu: 1500
        vip:
          ip: "10.10.12.105"
  - hostname: "kawg121"
    ipAddress: "10.10.12.121"
    installDisk: "/dev/sda"
    talosImageURL: factory.talos.dev/installer/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515
    controlPlane: false
    networkInterfaces:
      - deviceSelector:
          hardwareAddr: "025ffcaf8d15"
        dhcp: false
        addresses:
          - "10.10.12.121/20"
        routes:
          - network: 0.0.0.0/0
            gateway: "10.10.10.1"
        mtu: 1500
  - hostname: "kawg122"
    ipAddress: "10.10.12.122"
    installDisk: "/dev/sda"
    talosImageURL: factory.talos.dev/installer/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515
    controlPlane: false
    networkInterfaces:
      - deviceSelector:
          hardwareAddr: "02448ef5d041"
        dhcp: false
        addresses:
          - "10.10.12.122/20"
        routes:
          - network: 0.0.0.0/0
            gateway: "10.10.10.1"
        mtu: 1500
  - hostname: "kawg123"
    ipAddress: "10.10.12.123"
    installDisk: "/dev/sda"
    talosImageURL: factory.talos.dev/installer/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515
    controlPlane: false
    networkInterfaces:
      - deviceSelector:
          hardwareAddr: "0263b2e00a63"
        dhcp: false
        addresses:
          - "10.10.12.123/20"
        routes:
          - network: 0.0.0.0/0
            gateway: "10.10.10.1"
        mtu: 1500
  - hostname: "kawg124"
    ipAddress: "10.10.12.124"
    installDisk: "/dev/sda"
    talosImageURL: factory.talos.dev/installer/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515
    controlPlane: false
    networkInterfaces:
      - deviceSelector:
          hardwareAddr: "021579a6b815"
        dhcp: false
        addresses:
          - "10.10.12.124/20"
        routes:
          - network: 0.0.0.0/0
            gateway: "10.10.10.1"
        mtu: 1500

# Global patches
patches:
  - # Force nameserver
    |-
    machine:
      network:
        nameservers:
          - 10.10.10.1
  - # Configure NTP
    |-
    machine:
      time:
        disabled: false
        servers:
          - time.google.com
  - "@./patches/global/cluster-discovery.yaml"
  - "@./patches/global/containerd.yaml"
  - "@./patches/global/disable-search-domain.yaml"
  - "@./patches/global/hostdns.yaml"
  - "@./patches/global/kubelet.yaml"
  - "@./patches/global/openebs-local.yaml"
  - "@./patches/global/sysctl.yaml"

# Controller patches
controlPlane:
  patches:
    - "@./patches/controller/api-access.yaml"
    - "@./patches/controller/cluster.yaml"
    - "@./patches/controller/disable-admission-controller.yaml"
    - "@./patches/controller/etcd.yaml"


File: ./kubernetes/bootstrap/talos/talconfig.yaml
# yaml-language-server: $schema=https://raw.githubusercontent.com/budimanjojo/talhelper/master/pkg/config/schemas/talconfig.json
---
# renovate: datasource=docker depName=ghcr.io/siderolabs/installer
talosVersion: v1.9.0
# renovate: datasource=docker depName=ghcr.io/siderolabs/kubelet
kubernetesVersion: v1.31.2
clusterName: "thepatriots"
endpoint: https://10.10.12.105:6443
clusterPodNets:
  - "10.244.0.0/16"
clusterSvcNets:
  - "10.96.0.0/16"
additionalApiServerCertSans: &sans
  - "10.10.12.105"
  - 127.0.0.1 # KubePrism
  - "ThePatriots.local"
additionalMachineCertSans: *sans
# Disable built-in Flannel to use Cilium
cniConfig:
  name: none
nodes:
  - hostname: "kcp111"
    ipAddress: 10.10.12.111
    controlPlane: true
    installDiskSelector:
      size: "<=89GB"
    networkInterfaces:
      - interface: bond0
        bond:
          interfaces:
            - eth0
          mode: active-backup
        mtu: 9000
        addresses:
          - 10.10.12.111/24
        routes:
          - network: 0.0.0.0/0
            gateway: 10.10.12.1
        vip:
          ip: "10.10.12.105"
    nameservers:
      - 10.10.12.1
  - hostname: "kcp112"
    ipAddress: 10.10.12.112
    controlPlane: true
    installDiskSelector:
      size: "<=89GB"
    networkInterfaces:
      - interface: bond0
        bond:
          interfaces:
            - eth0
          mode: active-backup
        mtu: 9000
        addresses:
          - 10.10.12.112/24
        routes:
          - network: 0.0.0.0/0
            gateway: 10.10.12.1
        vip:
          ip: "10.10.12.105"
    nameservers:
      - 10.10.12.1
  - hostname: "kcp113"
    ipAddress: 10.10.12.113
    controlPlane: true
    installDiskSelector:
      size: "<=89GB"
    networkInterfaces:
      - interface: bond0
        bond:
          interfaces:
            - eth0
          mode: active-backup
        mtu: 9000
        addresses:
          - 10.10.12.113/24
        routes:
          - network: 0.0.0.0/0
            gateway: 10.10.12.1
        vip:
          ip: "10.10.12.105"
    nameservers:
      - 10.10.12.1
  - hostname: "kawg121"
    ipAddress: 10.10.12.121
    controlPlane: false
    installDiskSelector:
      size: "<=89GB"
    networkInterfaces:
      - interface: bond0
        bond:
          interfaces:
            - eth0
          mode: active-backup
        mtu: 9000
        addresses:
          - 10.10.12.121/24
        routes:
          - network: 0.0.0.0/0
            gateway: 10.10.12.1
      - interface: bond1
        bond:
          interfaces:
            - eth1
          mode: active-backup
        mtu: 9000
        addresses:
          - 192.168.90.121/24
    nameservers:
      - 10.10.12.1
  - hostname: "kawg122"
    ipAddress: 10.10.12.122
    controlPlane: false
    installDiskSelector:
      size: "<=89GB"
    networkInterfaces:
      - interface: bond0
        bond:
          interfaces:
            - eth0
          mode: active-backup
        mtu: 9000
        addresses:
          - 10.10.12.122/24
        routes:
          - network: 0.0.0.0/0
            gateway: 10.10.12.1
      - interface: bond1
        bond:
          interfaces:
            - eth1
          mode: active-backup
        mtu: 9000
        addresses:
          - 192.168.90.122/24
    nameservers:
      - 10.10.12.1
  - hostname: "kawg123"
    ipAddress: 10.10.12.123
    controlPlane: false
    installDiskSelector:
      size: "<=89GB"
    networkInterfaces:
      - interface: bond0
        bond:
          interfaces:
            - eth0
          mode: active-backup
        mtu: 9000
        addresses:
          - 10.10.12.123/24
        routes:
          - network: 0.0.0.0/0
            gateway: 10.10.12.1
      - interface: bond1
        bond:
          interfaces:
            - eth1
          mode: active-backup
        mtu: 9000
        addresses:
          - 192.168.90.123/24
    nameservers:
      - 10.10.12.1
  - hostname: "kawg124"
    ipAddress: 10.10.12.124
    controlPlane: false
    installDiskSelector:
      size: "<=89GB"
    networkInterfaces:
      - interface: bond0
        bond:
          interfaces:
            - eth0
          mode: active-backup
        mtu: 9000
        addresses:
          - 10.10.12.124/24
        routes:
          - network: 0.0.0.0/0
            gateway: 10.10.12.1
      - interface: bond1
        bond:
          interfaces:
            - eth1
          mode: active-backup
        mtu: 9000
        addresses:
          - 192.168.90.124/24
    nameservers:
      - 10.10.12.1
  - hostname: "kawg125"
    ipAddress: 10.10.12.125
    controlPlane: false
    installDiskSelector:
      size: "<=89GB"
    networkInterfaces:
      - interface: bond0
        bond:
          interfaces:
            - eth0
          mode: active-backup
        mtu: 9000
        addresses:
          - 10.10.12.125/24
        routes:
          - network: 0.0.0.0/0
            gateway: 10.10.12.1
      - interface: bond1
        bond:
          interfaces:
            - eth1
          mode: active-backup
        mtu: 9000
        addresses:
          - 192.168.90.125/24
    nameservers:
      - 10.10.12.1

# Global patches
patches:
  # Force nameserver
  - |-
    machine:
      network:
        nameservers:
          - 10.10.12.1
  # Configure NTP
  - |-
    machine:
      time:
        disabled: false
        servers:
          - time.google.com
  - "@./patches/global/cluster-discovery.yaml"
  - "@./patches/global/containerd.yaml"
  - "@./patches/global/disable-search-domain.yaml"
  - "@./patches/global/hostdns.yaml"
  - "@./patches/global/kubelet.yaml"
  - "@./patches/global/openebs-local.yaml"
  - "@./patches/global/sysctl.yaml"
# Controller patches
controlPlane:
  patches:
    - "@./patches/controller/api-access.yaml"
    - "@./patches/controller/cluster.yaml"
    - "@./patches/controller/disable-admission-controller.yaml"
    - "@./patches/controller/etcd.yaml"

File: ./kubernetes/bootstrap/talos/.sops.yaml
creation_rules:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
lastmodified:
mac:
pgp:
unencrypted_suffix:
version:

File: ./kubernetes/bootstrap/talos/patches/controller/etcd.yaml
cluster:
  etcd:
    extraArgs:
      listen-metrics-urls: http://0.0.0.0:2381
    advertisedSubnets:
      - 10.10.12.0/24

File: ./kubernetes/bootstrap/talos/patches/controller/api-access.yaml
machine:
  features:
    kubernetesTalosAPIAccess:
      enabled: true
      allowedRoles:
        - os:admin
      allowedKubernetesNamespaces:
        - system-upgrade

File: ./kubernetes/bootstrap/talos/patches/controller/cluster.yaml
cluster:
  allowSchedulingOnControlPlanes: true
  controllerManager:
    extraArgs:
      bind-address: 0.0.0.0
  coreDNS:
    disabled: true
  proxy:
    disabled: true
  scheduler:
    extraArgs:
      bind-address: 0.0.0.0

File: ./kubernetes/bootstrap/talos/patches/controller/disable-admission-controller.yaml
- op: remove
  path: /cluster/apiServer/admissionControl

File: ./kubernetes/bootstrap/talos/patches/global/kubelet.yaml
machine:
  kubelet:
    extraArgs:
      rotate-server-certificates: true
    nodeIP:
      validSubnets:
        - 10.10.10.0/20

File: ./kubernetes/bootstrap/talos/patches/global/cluster-discovery.yaml
cluster:
  discovery:
    registries:
      kubernetes:
        disabled: false
      service:
        disabled: false

File: ./kubernetes/bootstrap/talos/patches/global/hostdns.yaml
machine:
  features:
    hostDNS:
      enabled: true
      resolveMemberNames: true
      forwardKubeDNSToHost: true # Requires Cilium `bpf.masquerade: false`

File: ./kubernetes/bootstrap/talos/patches/global/containerd.yaml
machine:
  files:
    - op: create
      path: /etc/cri/conf.d/20-customization.part
      content: |-
        [plugins."io.containerd.grpc.v1.cri"]
          enable_unprivileged_ports = true
          enable_unprivileged_icmp = true
        [plugins."io.containerd.grpc.v1.cri".containerd]
          discard_unpacked_layers = false
        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          discard_unpacked_layers = false

File: ./kubernetes/bootstrap/talos/patches/global/openebs-local.yaml
machine:
  kubelet:
    extraMounts:
      - destination: /var/openebs/local
        type: bind
        source: /var/openebs/local
        options:
          - bind
          - rshared
          - rw

File: ./kubernetes/bootstrap/talos/patches/global/disable-search-domain.yaml
machine:
  network:
    disableSearchDomain: true

File: ./kubernetes/bootstrap/talos/patches/global/sysctl.yaml
machine:
  sysctls:
    fs.inotify.max_queued_events: "65536"
    fs.inotify.max_user_watches: "524288"
    fs.inotify.max_user_instances: "8192"
    net.core.rmem_max: "2500000"
    net.core.wmem_max: "2500000"

File: ./kubernetes/bootstrap/flux/kustomization.yaml
# IMPORTANT: This file is not tracked by flux and should never be. Its
# purpose is to only install the Flux components and CRDs into your cluster.
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - github.com/fluxcd/flux2/manifests/install?ref=v2.4.0
patches:
  # Remove the default network policies
  - patch: |-
      $patch: delete
      apiVersion: networking.k8s.io/v1
      kind: NetworkPolicy
      metadata:
        name: not-used
    target:
      group: networking.k8s.io
      kind: NetworkPolicy
  # Resources renamed to match those installed by oci://ghcr.io/fluxcd/flux-manifests
  - target:
      kind: ResourceQuota
      name: critical-pods
    patch: |
      - op: replace
        path: /metadata/name
        value: critical-pods-flux-system
  - target:
      kind: ClusterRoleBinding
      name: cluster-reconciler
    patch: |
      - op: replace
        path: /metadata/name
        value: cluster-reconciler-flux-system
  - target:
      kind: ClusterRoleBinding
      name: crd-controller
    patch: |
      - op: replace
        path: /metadata/name
        value: crd-controller-flux-system
  - target:
      kind: ClusterRole
      name: crd-controller
    patch: |
      - op: replace
        path: /metadata/name
        value: crd-controller-flux-system
  - target:
      kind: ClusterRole
      name: flux-edit
    patch: |
      - op: replace
        path: /metadata/name
        value: flux-edit-flux-system
  - target:
      kind: ClusterRole
      name: flux-view
    patch: |
      - op: replace
        path: /metadata/name
        value: flux-view-flux-system

File: ./kubernetes/bootstrap/helmfile.yaml
---
helmDefaults:
  wait: true
  waitForJobs: true
  timeout: 600
  recreatePods: true
  force: true

repositories:
  - name: cilium
    url: https://helm.cilium.io
  - name: coredns
    url: https://coredns.github.io/helm
  - name: postfinance
    url: https://postfinance.github.io/kubelet-csr-approver

releases:
  - name: prometheus-operator-crds
    namespace: observability
    chart: oci://ghcr.io/prometheus-community/charts/prometheus-operator-crds
    version: 17.0.2
  - name: cilium
    namespace: kube-system
    chart: cilium/cilium
    version: 1.16.5
    values:
      - ../apps/kube-system/cilium/app/helm-values.yaml
    needs:
      - observability/prometheus-operator-crds
  - name: coredns
    namespace: kube-system
    chart: coredns/coredns
    version: 1.36.1
    values:
      - ../apps/kube-system/coredns/app/helm-values.yaml
    needs:
      - observability/prometheus-operator-crds
      - kube-system/cilium
  - name: kubelet-csr-approver
    namespace: kube-system
    chart: postfinance/kubelet-csr-approver
    version: 1.2.4
    values:
      - ../apps/kube-system/kubelet-csr-approver/app/helm-values.yaml
    needs:
      - observability/prometheus-operator-crds
      - kube-system/cilium
      - kube-system/coredns
  - name: spegel
    namespace: kube-system
    chart: oci://ghcr.io/spegel-org/helm-charts/spegel
    version: v0.0.28
    values:
      - ../apps/kube-system/spegel/app/helm-values.yaml
    needs:
      - observability/prometheus-operator-crds
      - kube-system/cilium
      - kube-system/coredns
      - kube-system/kubelet-csr-approver

File: ./kubernetes/apps/meshcentral/app/configmap.sops.yaml
apiVersion:
kind:
metadata:
name:
data:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/meshcentral/app/pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: meshcentral-v2
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
  storageClassName: ceph-rbd

File: ./kubernetes/apps/meshcentral/app/oidc-creds.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
OIDC_CLIENT_ID:
OIDC_CLIENT_SECRET:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/meshcentral/app/helmrelease.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: meshcentral
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 3.6.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  values:
    controllers:
      meshcentral:
        annotations:
          reloader.stakater.com/auto: "true"
        # pod:
        #   annotations:
        #     k8s.v1.cni.cncf.io/networks: |
        #       [{
        #         "name":"macvlan-lan",
        #         "interface": "eth1",
        #         "namespace": "kube-system",
        #         "ips": ["10.10.12.224/24"]
        #       }]
        containers:
          app:
            image:
              repository: ghcr.io/ylianst/meshcentral
              tag: 1.1.37
            env:
              TZ: ${TIMEZONE}
              NODE_ENV: production
              HOSTNAME: ${HOSTNAME}
            # command:
            #  - /bin/sleep
            #  - infinity
            resources:
              requests:
                cpu: 100m
              limits:
                memory: 2Gi
    service:
      app:
        controller: meshcentral
        ports:
          http:
            port: ${PORT}
    ingress:
      external:
        className: external
        annotations:
          external-dns.alpha.kubernetes.io/target: "external.${SECRET_DOMAIN}"
          cert-manager.io/cluster-issuer: "letsencrypt-production"
        hosts:
          - host: ${HOSTNAME}
            paths:
              - path: /
                service:
                  identifier: app
                  port: http
        tls:
          - hosts:
              - ${HOSTNAME}
            secretName: "${HOSTNAME/./-}-tls"

      internal:
        className: internal
        annotations:
          cert-manager.io/cluster-issuer: "letsencrypt-production"
        hosts:
          - host: ${HOSTNAME}
            paths:
              - path: /
                service:
                  identifier: app
                  port: http
        tls:
          - hosts:
              - ${HOSTNAME}
            secretName: "${HOSTNAME/./-}-tls"
    persistence:
      config:
        type: configMap
        name: meshcentral-config
        advancedMounts:
          meshcentral:
            app:
              - path: /opt/meshcentral/meshcentral-data/config.json
                subPath: config.json
#      cert:
#        type: secret
#        name: ${HOSTNAME/./-}-tls
#        advancedMounts:
#          meshcentral:
#            app:
#              - path: /opt/meshcentral/meshcentral-data/webserver-cert-public.crt
#                subPath: webserver-cert-public.crt
      data:
        existingClaim: meshcentral-v2
        globalMounts:
          - path: /root
            subPath: rootdir
          - path: /opt/meshcentral/meshcentral-data
            subPath: data
          - path: /opt/meshcentral/meshcentral-files
            subPath: userfiles
          - path: /opt/meshcentral/meshcentral-backups
            subPath: backups

File: ./kubernetes/apps/meshcentral/app/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - pvc.yaml
  - configmap.sops.yaml
  - helmrelease.yaml
generatorOptions:
  disableNameSuffixHash: true

File: ./kubernetes/apps/meshcentral/volsync-backup/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
RESTIC_REPOSITORY:
RESTIC_PASSWORD:
AWS_ACCESS_KEY_ID:
AWS_SECRET_ACCESS_KEY:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/meshcentral/volsync-backup/replicationsource.yaml
apiVersion: volsync.backube/v1alpha1
kind: ReplicationSource
metadata:
  name: meshcentral
spec:
  restic:
    cacheCapacity: 2Gi
    cacheStorageClassName: local-hostpath
    copyMethod: Snapshot
    moverSecurityContext:
      fsGroup: 0
      runAsGroup: 0
      runAsUser: 0
    pruneIntervalDays: 7
    repository: meshcentral-volsync-r2-secret
    retain:
      daily: 7
      weekly: 4
    storageClassName: ceph-rbd
    volumeSnapshotClassName: csi-ceph-blockpool
  sourcePVC: meshcentral-v2
  trigger:
    schedule: "0 4 * * *"

File: ./kubernetes/apps/meshcentral/volsync-backup/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./secret.sops.yaml
  - ./replicationsource.yaml

File: ./kubernetes/apps/meshcentral/ks.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd/kustomize-controller/main/config/crd/bases/kustomize.toolkit.fluxcd.io_kustomizations.yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: meshcentral
  namespace: flux-system
spec:
  targetNamespace: &ns default
  interval: 10m
  timeout: 2m
  path: "./kubernetes/apps/meshcentral/app"
  prune: true
  wait: false
  sourceRef:
    kind: GitRepository
    name: thepatriots
  postBuild:
    substitute:
      TIMEZONE: "America/New_York"
      HOSTNAME: "meshcentral.${SECRET_DOMAIN}"
      PORT: "4430"
      ALIAS_PORT: "443"
      RedirPORT: "800"
      MpsPORT: "44330"
      MpsALIAS_PORT: "4433"

File: ./kubernetes/apps/meshcentral/ks-backup.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: meshcentral-backup
  namespace: flux-system
spec:
  targetNamespace: default
  interval: 10m
  timeout: 2m
  path: "./kubernetes/apps/meshcentral/volsync-backup"
  prune: true
  wait: false
  sourceRef:
    kind: GitRepository
    name: thepatriots

File: ./kubernetes/apps/bookstack/app/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
namespace:
stringData:
APP_URL:
DB_HOST:
DB_PORT:
DB_DATABASE:
DB_USERNAME:
DB_PASSWORD:
MAIL_DRIVER:
MAIL_HOST:
MAIL_PORT:
MAIL_ENCRYPTION:
MAIL_VERIFY_SSL:
MAIL_FROM:
MAIL_FROM_NAME:
AUTH_METHOD:
OIDC_NAME:
OIDC_DISPLAY_NAME_CLAIMS:
OIDC_CLIENT_ID:
OIDC_CLIENT_SECRET:
OIDC_ISSUER:
OIDC_END_SESSION_ENDPOINT:
OIDC_ISSUER_DISCOVER:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/bookstack/app/.decrypted~secret.sops.yaml
apiVersion:
kind:
metadata:
name:
namespace:
stringData:
APP_URL:
DB_HOST:
DB_PORT:
DB_DATABASE:
DB_USERNAME:
DB_PASSWORD:
MAIL_DRIVER:
MAIL_HOST:
MAIL_PORT:
MAIL_ENCRYPTION:
MAIL_VERIFY_SSL:
MAIL_FROM:
MAIL_FROM_NAME:
AUTH_METHOD:
OIDC_NAME:
OIDC_DISPLAY_NAME_CLAIMS:
OIDC_CLIENT_ID:
OIDC_CLIENT_SECRET:
OIDC_ISSUER:
OIDC_END_SESSION_ENDPOINT:
OIDC_ISSUER_DISCOVER:

File: ./kubernetes/apps/bookstack/app/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: bookstack
  namespace: default
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
  storageClassName: cephfs

File: ./kubernetes/apps/bookstack/app/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./hr.yaml
  - ./secret.sops.yaml
  - ./pvc.yaml

File: ./kubernetes/apps/bookstack/app/hr.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: bookstack
  namespace: default
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 3.6.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  uninstall:
    keepHistory: false
  values:
    controllers:
      bookstack:
        annotations:
          reloader.stakater.com/auto: "true"
        containers:
          app:
            image:
              repository: ghcr.io/linuxserver/bookstack
              tag: v24.05.4-ls166
            env:
              OIDC_DUMP_USER_DETAILS: "false"
              OIDC_ADDITIONAL_SCOPES: "groups"
              OIDC_GROUPS_CLAIM: "groups"
              OIDC_USER_TO_GROUPS: "true"
            envFrom:
              - secretRef:
                  name: bookstack-secret
            resources:
              requests:
                cpu: 10m
                memory: 50Mi
              limits:
                memory: 256Mi

    service:
      app:
        controller: bookstack
        ports:
          http:
            port: 80

    ingress:
      app:
        enabled: true
        className: external
        annotations:
          external-dns.alpha.kubernetes.io/target: "external.${SECRET_DOMAIN}"
          nginx.ingress.kubernetes.io/proxy-body-size: 4G
          nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
        hosts:
          - host: "documentation.${SECRET_DOMAIN}"
            paths:
              - path: /
                pathType: Prefix
                service:
                  identifier: app
                  port: http
        tls:
          - hosts:
              - "documentation.${SECRET_DOMAIN}"
            secretName: "documentation.${SECRET_DOMAIN}-tls"

      internal:
        enabled: true
        className: internal
        annotations:
          nginx.ingress.kubernetes.io/proxy-body-size: 4G
          nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
        hosts:
          - host: "documentation.${SECRET_DOMAIN}"
            paths:
              - path: /
                pathType: Prefix
                service:
                  identifier: app
                  port: http
        tls:
          - hosts:
              - "documentation.${SECRET_DOMAIN}"
            secretName: "documentation.${SECRET_DOMAIN}-tls"

    persistence:
      config:
        existingClaim: bookstack
        globalMounts:
          - path: /config

File: ./kubernetes/apps/bookstack/ks.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app bookstack
  namespace: flux-system
spec:
  targetNamespace: default
  dependsOn:
    - name: mariadb
  path: "./kubernetes/apps/bookstack/app"
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false
  interval: 30m
  retryInterval: 1m
  timeout: 5m
  postBuild:
    substitute:
      APP: *app
      VOLSYNC_CAPACITY: 512Mi

File: ./kubernetes/apps/authentik/app/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
namespace:
stringData:
SECRET_AUTHENTIK_SECRET_KEY:
SECRET_AUTHENTIK_POSTGRES_PASSWORD:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/authentik/app/redirect-and-webfinger.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    meta.helm.sh/release-name: authentik
    nginx.ingress.kubernetes.io/permanent-redirect: https://auth.kawalink.com/.well-known/webfinger
    #nginx.ingress.kubernetes.io/configuration-snippet: |
    # return 200 '{\"subject\": null, \"links\": [{\"rel\": \"http://openid.net/specs/connect/1.0/issuer\", \"href\": \"https://auth.kawalink.com/application/o/tailscale/\"}]}';
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/instance: authentik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: authentik
    app.kubernetes.io/part-of: authentik
  name: authentik-webfinger
spec:
  ingressClassName: external
  rules:
    - host: kawalink.com
      http:
        paths:
          - backend:
              service:
                name: authentik-server
                port:
                  number: 80
            path: /.well-known/webfinger
            pathType: Exact
  tls:
    - hosts:
        - kawalink.com
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    meta.helm.sh/release-name: authentik
    nginx.ingress.kubernetes.io/permanent-redirect: https://auth.kawalink.com
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/instance: authentik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: authentik
    app.kubernetes.io/part-of: authentik
  name: authentik-root-domain-redirect
spec:
  ingressClassName: external
  rules:
    - host: kawalink.com
      http:
        paths:
          - backend:
              service:
                name: authentik-server
                port:
                  number: 80
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - kawalink.com

File: ./kubernetes/apps/authentik/app/int-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: authentik-internal
  namespace: default
spec:
  ingressClassName: internal
  rules:
    - host: &host "auth.${SECRET_DOMAIN}"
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: authentik-server
                port:
                  number: 80
  tls:
    - hosts:
        - *host
      secretName: "auth-${SECRET_DOMAIN/./-}-tls"

File: ./kubernetes/apps/authentik/app/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: authentik
  namespace: default
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 500Mi
  storageClassName: cephfs

File: ./kubernetes/apps/authentik/app/secret-init.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
INIT_POSTGRES_DBNAME:
INIT_POSTGRES_HOST:
INIT_POSTGRES_USER:
INIT_POSTGRES_PASS:
INIT_POSTGRES_SUPER_PASS:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/authentik/app/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./hr.yaml
  - ./int-ingress.yaml
#  - ./secret.sops.yaml
  - ./secret-init.sops.yaml
  - ./redirect-and-webfinger.yaml
  - ./pvc.yaml

File: ./kubernetes/apps/authentik/app/hr.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: authentik
  namespace: default
spec:
  interval: 5m
  chart:
    spec:
      chart: authentik
      version: 2024.12.0
      sourceRef:
        kind: HelmRepository
        name: authentik
        namespace: flux-system
  install:
    remediation:
      retries: 5
  upgrade:
    remediation:
      retries: 5
      remediateLastFailure: true
    cleanupOnFail: true
  rollback:
    timeout: 10m
    recreate: true
    cleanupOnFail: true
  values:
    global:
      image:
        repository: ghcr.io/goauthentik/server
        tag: 2024.12.1
      fullnameOverride: authentik
    authentik:
      log_level: info
      avatars: "initials"
      email:
        host: "smtp-relay.default.svc.cluster.local"
        port: 587
        use_tls: false
        from: "noreply@${SECRET_DOMAIN_COMP}"
      secret_key: "${SECRET_AUTHENTIK_SECRET_KEY}"
      error_reporting:
        enable: false
        send_pii: false
      postgresql:
        host: "postgres16-rw.database.svc.cluster.local"
        name: "authentik"
        user: "authentik"
        password: "${SECRET_AUTHENTIK_POSTGRES_PASSWORD}"
      redis:
        host: "dragonfly.database.svc.cluster.local"

    server:
      replicas: 2
      initContainers:
       - name: 01-init-db
         image: ghcr.io/onedr0p/postgres-init:16
         envFrom:
              - secretRef:
                  name: authentik-init-secret
      volumes:
        - name: custom
          existingClaim: authentik
      volumeMounts:
        - name: custom
          mountPath: /media
      ingress:
        enabled: true
        ingressClassName: external
        annotations:
          external-dns.alpha.kubernetes.io/target: "external.${SECRET_DOMAIN}"
        hosts:
          - &host "auth.${SECRET_DOMAIN}"
        paths:
          - /
        tls:
          - hosts:
              - *host
            secretName: "auth-${SECRET_DOMAIN/./-}-tls"
      resources:
        requests:
          cpu: 100m
          memory: 512Mi
        limits:
          memory: 2Gi
    worker:
      replicas: 2
      resources:
        requests:
          cpu: 50m
          memory: 512Mi
        limits:
          memory: 1Gi

File: ./kubernetes/apps/authentik/ks.yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: authentik
  namespace: flux-system
spec:
  dependsOn:
    - name: cnpg-cluster16
    - name: dragonfly-cluster
  targetNamespace: default
  path: ./kubernetes/apps/authentik/app
  prune: true
  sourceRef:
    kind: GitRepository
    name: flux-system
  interval: 30m
  timeout: 5m
  wait: true

File: ./kubernetes/apps/volsync/app/operator/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - hr.yaml

File: ./kubernetes/apps/volsync/app/operator/hr.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: volsync
  namespace: volsync-system
spec:
  interval: 30m
  chart:
    spec:
      chart: volsync
      version: 0.11.0
      sourceRef:
        kind: HelmRepository
        name: backube
        namespace: flux-system
  values:
    manageCRDs: true

File: ./kubernetes/apps/volsync/app/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - operator/
  - cloudflare/

File: ./kubernetes/apps/volsync/app/cloudflare/cloudflare-secrets.sops.yaml
apiVersion:
kind:
metadata:
name:
namespace:
type:
data:
AWS_ACCESS_KEY_ID:
AWS_SECRET_ACCESS_KEY:
RESTIC_PASSWORD:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/volsync/app/cloudflare/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - cloudflare-secrets.sops.yaml

File: ./kubernetes/apps/volsync/ks.yaml
---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/kustomize.toolkit.fluxcd.io/kustomization_v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &appname volsync
  namespace: flux-system
spec:
  targetNamespace: volsync-system
  path: ./kubernetes/apps/volsync/app
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/rook-ceph/rook/operator/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./hr.yaml

File: ./kubernetes/apps/rook-ceph/rook/operator/hr.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: rook-ceph-operator
  namespace: rook-ceph
spec:
  interval: 30m
  chart:
    spec:
      chart: rook-ceph
      version: v1.15.6
      sourceRef:
        kind: HelmRepository
        name: rook-release
        namespace: flux-system
  maxHistory: 2
  install:
    createNamespace: true
    crds: CreateReplace
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    crds: CreateReplace
    remediation:
      retries: 3
  uninstall:
    keepHistory: false
  values:
    logLevel: DEBUG # INFO
    csi:
      cephFSKernelMountOptions: ms_mode=secure # Set when encryption/compression is enabled
      logLevel: 4
      enableMetadata: true
    crds:
      enabled: true
    pspEnable: false
    resources:
      requests:
        cpu: 300m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 1Gi

File: ./kubernetes/apps/rook-ceph/rook/ks-cluster.yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: rook-ceph-cluster
  namespace: flux-system
spec:
  path: ./kubernetes/apps/rook-ceph/rook/cluster
  prune: false
  sourceRef:
    kind: GitRepository
    name: thepatriots
    namespace: flux-system
  healthChecks:
    - apiVersion: helm.toolkit.fluxcd.io/v2
      kind: HelmRelease
      name: rook-ceph-cluster
      namespace: rook-ceph
  interval: 30m
  retryInterval: 1m
  timeout: 3m

File: ./kubernetes/apps/rook-ceph/rook/ks-operator.yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: rook-ceph-operator
  namespace: flux-system
spec:
  path: ./kubernetes/apps/rook-ceph/rook/operator
  prune: false
  sourceRef:
    kind: GitRepository
    name: thepatriots
    namespace: flux-system
  healthChecks:
    - apiVersion: helm.toolkit.fluxcd.io/v2
      kind: HelmRelease
      name: rook-ceph-operator
      namespace: rook-ceph
  interval: 30m
  retryInterval: 1m
  timeout: 3m

File: ./kubernetes/apps/rook-ceph/rook/cluster/media-storageclass-replicated.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
provisioner: rook-ceph.cephfs.csi.ceph.com
reclaimPolicy: Retain
volumeBindingMode: Immediate
allowVolumeExpansion: true
metadata:
  name: cephfs-media
parameters:
  clusterID: rook-ceph
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  fsName: QuadSquad_cephfs
  pool: QuadSquad_cephfs_Replicated

File: ./kubernetes/apps/rook-ceph/rook/cluster/media-storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
provisioner: rook-ceph.cephfs.csi.ceph.com
reclaimPolicy: Retain
volumeBindingMode: Immediate
allowVolumeExpansion: true
metadata:
  name: cephfs-media-ec
parameters:
  clusterID: rook-ceph
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  fsName: QuadSquad_cephfs
  pool: QuadSquad_cephfs_EC



File: ./kubernetes/apps/rook-ceph/rook/cluster/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./hr.yaml
#  - ./media-storageclass.yaml
#  - ./media-storageclass-replicated.yaml

File: ./kubernetes/apps/rook-ceph/rook/cluster/hr.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
spec:
  interval: 30m
  timeout: 15m
  chart:
    spec:
      chart: rook-ceph-cluster
      version: v1.15.6
      sourceRef:
        kind: HelmRepository
        name: rook-release
        namespace: flux-system
  dependsOn:
    - name: rook-ceph-operator
  values:
    toolbox:
      enabled: true
      image: quay.io/ceph/ceph:v18.2.4
    cephClusterSpec:
      cephVersion:
        image: quay.io/ceph/ceph:v18.2.4
      external:
        enable: true
      crashCollector:
        disable: true
      healthCheck:
        daemonHealth:
          mon:
            disabled: false
            interval: 45s
      network:
        provider: host
        connections:
          encryption:
            enabled: true

    cephBlockPools: {}
    cephFileSystems: {}
    cephObjectStores: {}

File: ./kubernetes/apps/rook-ceph/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./ns.yaml
  - ./rook/ks-operator.yaml
  - ./rook/ks-cluster.yaml

File: ./kubernetes/apps/rook-ceph/ns.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: rook-ceph

File: ./kubernetes/apps/gpu-system/intel-device-plugin/operator/helmrelease.yaml
---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: intel-device-plugin-operator
spec:
  interval: 30m
  chart:
    spec:
      chart: intel-device-plugins-operator
      version: 0.31.1
      sourceRef:
        kind: HelmRepository
        name: intel
        namespace: flux-system
  install:
    crds: CreateReplace
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    crds: CreateReplace
    remediation:
      strategy: rollback
      retries: 3
  dependsOn:
    - name: node-feature-discovery
      namespace: kube-system

File: ./kubernetes/apps/gpu-system/intel-device-plugin/operator/kustomization.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/kustomization
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helmrelease.yaml

File: ./kubernetes/apps/gpu-system/intel-device-plugin/ks.yaml
---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/kustomize.toolkit.fluxcd.io/kustomization_v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app intel-device-plugin
  namespace: flux-system
spec:
  targetNamespace: gpu-system
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  path: ./kubernetes/apps/gpu-system/intel-device-plugin/operator
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false
  interval: 30m
  retryInterval: 1m
  timeout: 5m
---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/kustomize.toolkit.fluxcd.io/kustomization_v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app intel-device-plugin-gpu
  namespace: flux-system
spec:
  targetNamespace: gpu-system
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  path: ./kubernetes/apps/gpu-system/intel-device-plugin/gpu
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/gpu-system/intel-device-plugin/gpu/helmrelease.yaml
---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: intel-device-plugin-gpu
spec:
  interval: 30m
  chart:
    spec:
      chart: intel-device-plugins-gpu
      version: 0.31.1
      sourceRef:
        kind: HelmRepository
        name: intel
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  dependsOn:
    - name: intel-device-plugin-operator
      namespace: gpu-system
  values:
    name: intel-gpu-plugin
    sharedDevNum: 3
    nodeFeatureRule: false

File: ./kubernetes/apps/gpu-system/intel-device-plugin/gpu/kustomization.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/kustomization
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helmrelease.yaml

File: ./kubernetes/apps/gpu-system/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: gpu-system

File: ./kubernetes/apps/gpu-system/kustomization.yaml
# yaml-language-server: $schema=https://json.schemastore.org/kustomization
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  # Pre Flux-Kustomizations
  - ./namespace.yaml
  # Flux-Kustomizations
  - ./intel-device-plugin/ks.yaml

File: ./kubernetes/apps/network/external-dns/external/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
api-token:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/network/external-dns/external/helmrelease.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app external-dns-cf
spec:
  interval: 30m
  chart:
    spec:
      chart: external-dns
      version: 1.15.0
      sourceRef:
        kind: HelmRepository
        name: external-dns
        namespace: flux-system
  install:
    crds: CreateReplace
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    crds: CreateReplace
    remediation:
      strategy: rollback
      retries: 3
  values:
    fullnameOverride: *app
    provider: cloudflare
    env:
      - name: CF_API_TOKEN
        valueFrom:
          secretKeyRef:
            name: external-dns-secret
            key: api-token
    extraArgs:
      - --ingress-class=external
      - --cloudflare-proxied
      - --crd-source-apiversion=externaldns.k8s.io/v1alpha1
      - --crd-source-kind=DNSEndpoint
    policy: sync
    sources: ["crd", "ingress"]
    txtPrefix: k8s.
    txtOwnerId: default
    domainFilters: ["${SECRET_DOMAIN}", "${SECRET_DOMAIN_CHAT}"]
    serviceMonitor:
      enabled: true
    podAnnotations:
      secret.reloader.stakater.com/reload: external-dns-secret

File: ./kubernetes/apps/network/external-dns/external/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./secret.sops.yaml
  - ./helmrelease.yaml

File: ./kubernetes/apps/network/external-dns/ks-external.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app external-dns-cf
  namespace: flux-system
spec:
  targetNamespace: network
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  path: ./kubernetes/apps/network/external-dns/external
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: true
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/network/external-dns/internal/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
bind_rndc_algorithm:
bind_rndc_secret:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/network/external-dns/internal/helmrelease.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app external-dns-internal
spec:
  interval: 30m
  chart:
    spec:
      chart: external-dns
      version: 1.15.0
      sourceRef:
        kind: HelmRepository
        name: external-dns
        namespace: flux-system
  install:
    crds: CreateReplace
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    crds: CreateReplace
    remediation:
      strategy: rollback
      retries: 3
  values:
    fullnameOverride: *app

    domainFilters:
      - ${SECRET_DOMAIN}

    env:
    - name: EXTERNAL_DNS_RFC2136_HOST
      value: "172.16.10.3"
    - name: EXTERNAL_DNS_RFC2136_PORT
      value: "53"
    - name: EXTERNAL_DNS_RFC2136_ZONE
      value: "${SECRET_DOMAIN}"
    - name: EXTERNAL_DNS_RFC2136_TSIG_AXFR
      value: "true"
    - name: EXTERNAL_DNS_RFC2136_TSIG_KEYNAME
      value: externaldns
    - name: EXTERNAL_DNS_RFC2136_TSIG_SECRET_ALG
      valueFrom:
        secretKeyRef:
          name: externaldns-internal-secrets
          key: bind_rndc_algorithm
    - name: EXTERNAL_DNS_RFC2136_TSIG_SECRET
      valueFrom:
        secretKeyRef:
          name: externaldns-internal-secrets
          key: bind_rndc_secret

    podAnnotations:
      secret.reloader.stakater.com/reload: externaldns-internal-secrets

    policy: sync
    provider: rfc2136

    resources:
      requests:
        cpu: 16m
        memory: 90M
      limits:
        memory: 90M

    sources:
      - ingress
      - crd

    extraArgs:
      - --ingress-class=internal
      - --crd-source-apiversion=externaldns.k8s.io/v1alpha1
      - --crd-source-kind=DNSEndpoint

    txtPrefix: "k8s."
    serviceMonitor:
      enabled: true





File: ./kubernetes/apps/network/external-dns/internal/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./secret.sops.yaml
  - ./helmrelease.yaml

File: ./kubernetes/apps/network/external-dns/ks-internal.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app external-dns-internal
  namespace: flux-system
spec:
  targetNamespace: network
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  path: ./kubernetes/apps/network/external-dns/internal
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: true
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/network/cloudflared/app/configs/config.yaml
---
originRequest:
  originServerName: "external.${SECRET_DOMAIN}"

ingress:
  - hostname: "${SECRET_DOMAIN}"
    service: https://ingress-nginx-external-controller.network.svc.cluster.local:443
  - hostname: "*.${SECRET_DOMAIN}"
    service: https://ingress-nginx-external-controller.network.svc.cluster.local:443
  - hostname: "${SECRET_DOMAIN_CHAT}"
    service: https://ingress-nginx-external-controller.network.svc.cluster.local:443
  - hostname: "*.${SECRET_DOMAIN_CHAT}"
    service: https://ingress-nginx-external-controller.network.svc.cluster.local:443
  - service: http_status:404

File: ./kubernetes/apps/network/cloudflared/app/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
TUNNEL_ID:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/network/cloudflared/app/helmrelease.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: cloudflared
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 3.6.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    controllers:
      cloudflared:
        strategy: RollingUpdate
        annotations:
          reloader.stakater.com/auto: "true"
        containers:
          app:
            image:
              repository: docker.io/cloudflare/cloudflared
              tag: 2025.1.0
            env:
              NO_AUTOUPDATE: true
              TUNNEL_CRED_FILE: /etc/cloudflared/creds/credentials.json
              TUNNEL_METRICS: 0.0.0.0:8080
              TUNNEL_ORIGIN_ENABLE_HTTP2: true
              TUNNEL_TRANSPORT_PROTOCOL: quic
              TUNNEL_POST_QUANTUM: true
              TUNNEL_ID:
                valueFrom:
                  secretKeyRef:
                    name: cloudflared-secret
                    key: TUNNEL_ID
            args:
              - tunnel
              - --config
              - /etc/cloudflared/config/config.yaml
              - run
              - "$(TUNNEL_ID)"
            probes:
              liveness: &probes
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /ready
                    port: &port 8080
                  initialDelaySeconds: 0
                  periodSeconds: 10
                  timeoutSeconds: 1
                  failureThreshold: 3
              readiness: *probes
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              capabilities: { drop: ["ALL"] }
            resources:
              requests:
                cpu: 10m
              limits:
                memory: 256Mi
    defaultPodOptions:
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
        runAsGroup: 65534
        seccompProfile: { type: RuntimeDefault }
    service:
      app:
        controller: cloudflared
        ports:
          http:
            port: *port
    serviceMonitor:
      app:
        serviceName: cloudflared
        endpoints:
          - port: http
            scheme: http
            path: /metrics
            interval: 1m
            scrapeTimeout: 10s
    persistence:
      config:
        type: configMap
        name: cloudflared-configmap
        globalMounts:
          - path: /etc/cloudflared/config/config.yaml
            subPath: config.yaml
            readOnly: true
      creds:
        type: secret
        name: cloudflared-secret
        globalMounts:
          - path: /etc/cloudflared/creds/credentials.json
            subPath: credentials.json
            readOnly: true

File: ./kubernetes/apps/network/cloudflared/app/dnsendpoint.yaml
---
apiVersion: externaldns.k8s.io/v1alpha1
kind: DNSEndpoint
metadata:
  name: cloudflared
spec:
  endpoints:
    - dnsName: "external.${SECRET_DOMAIN}"
      recordType: CNAME
      targets: ["${SECRET_CLOUDFLARE_TUNNEL_ID}.cfargotunnel.com"]

File: ./kubernetes/apps/network/cloudflared/app/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./dnsendpoint.yaml
  - ./secret.sops.yaml
  - ./helmrelease.yaml
configMapGenerator:
  - name: cloudflared-configmap
    files:
      - ./configs/config.yaml
generatorOptions:
  disableNameSuffixHash: true

File: ./kubernetes/apps/network/cloudflared/ks.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app cloudflared
  namespace: flux-system
spec:
  targetNamespace: network
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  dependsOn:
    - name: external-dns-cf
  path: ./kubernetes/apps/network/cloudflared/app
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/network/ingress-nginx/external/helmrelease.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: ingress-nginx-external
spec:
  interval: 30m
  chart:
    spec:
      chart: ingress-nginx
      version: 4.12.0
      sourceRef:
        kind: HelmRepository
        name: ingress-nginx
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  dependsOn:
    - name: cloudflared
      namespace: network
  values:
    fullnameOverride: ingress-nginx-external
    controller:
      service:
        annotations:
          external-dns.alpha.kubernetes.io/hostname: "external.${SECRET_DOMAIN}"
          io.cilium/lb-ipam-ips: "10.10.12.247"
        externalTrafficPolicy: Cluster
      ingressClassResource:
        name: external
        default: false
        controllerValue: k8s.io/external
      admissionWebhooks:
        objectSelector:
          matchExpressions:
            - key: ingress-class
              operator: In
              values: ["external"]
      config:
        client-body-buffer-size: 100M
        client-body-timeout: 120
        client-header-timeout: 120
        enable-brotli: "true"
        enable-real-ip: "true"
        hsts-max-age: 31449600
        keep-alive-requests: 10000
        keep-alive: 120
        log-format-escape-json: "true"
        log-format-upstream: >
          {"time": "$time_iso8601", "remote_addr": "$proxy_protocol_addr", "x_forwarded_for": "$proxy_add_x_forwarded_for",
          "request_id": "$req_id", "remote_user": "$remote_user", "bytes_sent": $bytes_sent, "request_time": $request_time,
          "status": $status, "vhost": "$host", "request_proto": "$server_protocol", "path": "$uri", "request_query": "$args",
          "request_length": $request_length, "duration": $request_time, "method": "$request_method", "http_referrer": "$http_referer",
          "http_user_agent": "$http_user_agent"}
        proxy-body-size: 0
        proxy-buffer-size: 16k
        ssl-protocols: TLSv1.3 TLSv1.2
        hide-headers: Server,X-Powered-By
      metrics:
        enabled: true
        serviceMonitor:
          enabled: true
          namespaceSelector:
            any: true
      extraArgs:
        default-ssl-certificate: "network/${SECRET_DOMAIN/./-}-production-tls"
      resources:
        requests:
          cpu: 100m
          memory: 2Gi
        limits:
          memory: 4Gi

File: ./kubernetes/apps/network/ingress-nginx/external/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helmrelease.yaml

File: ./kubernetes/apps/network/ingress-nginx/ks.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app ingress-nginx-certificates
  namespace: flux-system
spec:
  targetNamespace: network
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  dependsOn:
    - name: cert-manager-issuers
  path: ./kubernetes/apps/network/ingress-nginx/certificates
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: true
  interval: 30m
  retryInterval: 1m
  timeout: 5m
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app ingress-nginx-internal
  namespace: flux-system
spec:
  targetNamespace: network
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  dependsOn:
    - name: ingress-nginx-certificates
  path: ./kubernetes/apps/network/ingress-nginx/internal
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false
  interval: 30m
  retryInterval: 1m
  timeout: 5m
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app ingress-nginx-external
  namespace: flux-system
spec:
  targetNamespace: network
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  dependsOn:
    - name: ingress-nginx-certificates
  path: ./kubernetes/apps/network/ingress-nginx/external
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/network/ingress-nginx/certificates/legacy.yaml
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: "${SECRET_DOMAIN_LEGACY/./-}-production"
spec:
  secretName: "${SECRET_DOMAIN_LEGACY/./-}-production-tls"
  issuerRef:
    name: letsencrypt-production
    kind: ClusterIssuer
  commonName: "${SECRET_DOMAIN_LEGACY}"
  dnsNames:
    - "${SECRET_DOMAIN_LEGACY}"
    - "*.${SECRET_DOMAIN_LEGACY}"
  secretTemplate:
    annotations:
      reflector.v1.k8s.emberstack.com/reflection-allowed: "true"
      reflector.v1.k8s.emberstack.com/reflection-allowed-namespaces: "media"
      reflector.v1.k8s.emberstack.com/reflection-auto-enabled: "true"

File: ./kubernetes/apps/network/ingress-nginx/certificates/production.yaml
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: "${SECRET_DOMAIN/./-}-production"
spec:
  secretName: "${SECRET_DOMAIN/./-}-production-tls"
  issuerRef:
    name: letsencrypt-production
    kind: ClusterIssuer
  commonName: "${SECRET_DOMAIN}"
  dnsNames:
    - "${SECRET_DOMAIN}"
    - "*.${SECRET_DOMAIN}"

File: ./kubernetes/apps/network/ingress-nginx/certificates/staging.yaml
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: "${SECRET_DOMAIN/./-}-staging"
spec:
  secretName: "${SECRET_DOMAIN/./-}-staging-tls"
  issuerRef:
    name: letsencrypt-staging
    kind: ClusterIssuer
  commonName: "${SECRET_DOMAIN}"
  dnsNames:
    - "${SECRET_DOMAIN}"
    - "*.${SECRET_DOMAIN}"

File: ./kubernetes/apps/network/ingress-nginx/certificates/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./staging.yaml
  - ./production.yaml
  - ./legacy.yaml

File: ./kubernetes/apps/network/ingress-nginx/internal/helmrelease.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: ingress-nginx-internal
  namespace: network
spec:
  interval: 30m
  chart:
    spec:
      chart: ingress-nginx
      version: 4.12.0
      sourceRef:
        kind: HelmRepository
        name: ingress-nginx
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    fullnameOverride: ingress-nginx-internal
    controller:
      service:
        annotations:
          io.cilium/lb-ipam-ips: "10.10.12.246"
        externalTrafficPolicy: Cluster
      ingressClassResource:
        name: internal
        default: true
        controllerValue: k8s.io/internal
      admissionWebhooks:
        objectSelector:
          matchExpressions:
            - key: ingress-class
              operator: In
              values: ["internal"]
      config:
        client-body-buffer-size: 100M
        client-body-timeout: 120
        client-header-timeout: 120
        enable-brotli: "true"
        enable-real-ip: "true"
        hsts-max-age: 31449600
        keep-alive-requests: 10000
        keep-alive: 120
        log-format-escape-json: "true"
        log-format-upstream: >
          {"time": "$time_iso8601", "remote_addr": "$proxy_protocol_addr", "x_forwarded_for": "$proxy_add_x_forwarded_for",
          "request_id": "$req_id", "remote_user": "$remote_user", "bytes_sent": $bytes_sent, "request_time": $request_time,
          "status": $status, "vhost": "$host", "request_proto": "$server_protocol", "path": "$uri", "request_query": "$args",
          "request_length": $request_length, "duration": $request_time, "method": "$request_method", "http_referrer": "$http_referer",
          "http_user_agent": "$http_user_agent"}
        proxy-body-size: 0
        proxy-buffer-size: 16k
        ssl-protocols: TLSv1.3 TLSv1.2
        hide-headers: Server,X-Powered-By
      metrics:
        enabled: true
        serviceMonitor:
          enabled: true
          namespaceSelector:
            any: true
      extraArgs:
        default-ssl-certificate: "network/${SECRET_DOMAIN/./-}-production-tls"
      resources:
        requests:
          cpu: 100m
        limits:
          memory: 2Gi

File: ./kubernetes/apps/network/ingress-nginx/internal/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helmrelease.yaml

File: ./kubernetes/apps/network/namespace.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: network
  labels:
    kustomize.toolkit.fluxcd.io/prune: disabled

File: ./kubernetes/apps/network/unimus/unimus/app/helmrelease.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: unimus
spec:
  interval: 15m
  chart:
    spec:
      chart: app-template
      version: 3.6.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  install:
    createNamespace: true
    remediation:
      retries: 3
  upgrade:
    remediation:
      retries: 3
  values:
    controllers:
      unimus:
        type: deployment
        replicas: 1
        annotations:
          reloader.stakater.com/auto: "true"

        containers:
          app:
            image:
              repository: croc/unimus
              tag: 2.6.1
            env:
              TZ: "America/New_York"
              JAVA_OPTS: "-Xms256M -Xmx1024M -Dunimus.core.connect-timeout=20000 -Dunimus.core.inter-connection-delay=1000 -Dunimus.core.cli-expect-timeout=30000"
            resources:
              requests:
                memory: 256Mi
              limits:
                memory: 1Gi
    service:
      main:
        controller: unimus
        ports:
          http:
            port: 8085
    ingress:
      main:
        enabled: true
        className: internal
        hosts:
          - host: "${HOSTNAME}"
            paths:
              - path: /
                pathType: Prefix
                service:
                  identifier: main
                  port: 8085
        tls:
          - hosts:
              - "${HOSTNAME}"


    persistence:
      data:
        enabled: true
        existingClaim: "${VOLSYNC_CLAIM}"
        globalMounts:
          - path: /etc/unimus/





File: ./kubernetes/apps/network/unimus/unimus/app/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helmrelease.yaml

File: ./kubernetes/apps/network/unimus/unimus/ks.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app unimus
  namespace: flux-system
spec:
  targetNamespace: &ns network
  interval: 10m
  timeout: 2m
  path: "./kubernetes/apps/network/unimus/unimus/app"
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  dependsOn:
    - name: unimus-pvc
    - name: rook-ceph-cluster
    - name: volsync
  wait: false
  postBuild:
    substitute:
      HOSTNAME: "unimus.${SECRET_DOMAIN}"
      APP: unimus
      VOLSYNC_CLAIM: unimus-storage


File: ./kubernetes/apps/network/unimus/pvc/app/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
RESTIC_REPOSITORY:
RESTIC_PASSWORD:
AWS_ACCESS_KEY_ID:
AWS_SECRET_ACCESS_KEY:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/network/unimus/pvc/app/replicationdestination.yaml
apiVersion: volsync.backube/v1alpha1
kind: ReplicationDestination
metadata:
  name: unimus-storage-bootstrap
  namespace: invoiceninja
spec:
  restic:
    accessModes:
    - ReadWriteOnce
    cacheCapacity: "${VOLSYNC_CACHE_CAPACITY:-1Gi}"
    cacheStorageClassName: "${VOLSYNC_CACHE_SNAPSHOTCLASS:-local-hostpath}"
    capacity: "${VOLSYNC_CAPACITY:-1Gi}"
    copyMethod: Snapshot
    moverSecurityContext:
      fsGroup: 568
      runAsGroup: 568
      runAsUser: 568
    repository: ${VOLSYNC_REPOSITORY_SECRET}
    storageClassName: "${VOLSYNC_STORAGECLASS:-ceph-rbd}"
    volumeSnapshotClassName: "${VOLSYNC_SNAPSHOTCLASS:-csi-ceph-blockpool}"
  trigger:
    manual: restore-once

File: ./kubernetes/apps/network/unimus/pvc/app/replicationsource.yaml
apiVersion: volsync.backube/v1alpha1
kind: ReplicationSource
metadata:
  name: unimus
spec:
  sourcePVC: "${VOLSYNC_CLAIM:-${APP}}"
  restic:
    cacheCapacity: "${VOLSYNC_CACHE_CAPACITY:-1Gi}"
    cacheStorageClassName: "${VOLSYNC_CACHE_SNAPSHOTCLASS:-local-hostpath}"
    copyMethod: Snapshot
    moverSecurityContext:
      fsGroup: 568
      runAsGroup: 568
      runAsUser: 568
    pruneIntervalDays: 7
    repository: ${VOLSYNC_REPOSITORY_SECRET}
    retain:
      daily: 7
      weekly: 4
    storageClassName: "${VOLSYNC_STORAGECLASS:-ceph-rbd}"
    volumeSnapshotClassName: "${VOLSYNC_SNAPSHOTCLASS:-csi-ceph-blockpool}"
  trigger:
    schedule: "0 4 * * *"

File: ./kubernetes/apps/network/unimus/pvc/app/pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: "${VOLSYNC_CLAIM:-${APP}}"
spec:
  accessModes:
    - "${VOLSYNC_ACCESSMODES:-ReadWriteOnce}"
  dataSourceRef:
    kind: ReplicationDestination
    apiGroup: volsync.backube
    name: "${APP}-bootstrap"
  resources:
    requests:
      storage: "${VOLSYNC_CAPACITY:-1Gi}"
  storageClassName: "${VOLSYNC_STORAGECLASS:-ceph-rbd}"


File: ./kubernetes/apps/network/unimus/pvc/app/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - pvc.yaml
  - replicationdestination.yaml
  - replicationsource.yaml
  - secret.sops.yaml

File: ./kubernetes/apps/network/unimus/pvc/ks.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app unimus-pvc
  namespace: flux-system
spec:
  targetNamespace: &ns network
  interval: 10m
  path: "./kubernetes/apps/network/unimus/pvc/app"
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: true
  dependsOn:
    - name: volsync
  postBuild:
    substitute:
      APP: unimus-storage
      APP_NS: *ns
      VOLSYNC_CLAIM: unimus-storage
      VOLSYNC_CAPACITY: 2Gi
      VOLSYNC_ACCESSMODES: ReadWriteOnce
      VOLSYNC_STORAGECLASS: cephfs
      VOLSYNC_SNAPSHOTCLASS: csi-ceph-filesystem
      VOLSYNC_REPOSITORY_SECRET: unimus-volsync-r2-secret

File: ./kubernetes/apps/network/unimus/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - pvc/ks.yaml
  - unimus/ks.yaml

File: ./kubernetes/apps/network/multus/app/networkattachment.yaml
---
apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: macvlan-lan
spec:
  config: |-
    {
      "cniVersion": "0.3.1",
      "name": "macvlan-lan",
      "plugins": [
        {
          "type": "macvlan",
          "master": "bond0",
          "mode": "bridge",
          "capabilities": {
            "ips": true
          },
          "ipam": {
            "type": "static"
          }
        }
      ]
    }

File: ./kubernetes/apps/network/multus/app/helmrelease.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: multus
  namespace: kube-system
spec:
  interval: 30m
  chart:
    spec:
      chart: multus
      version: 5.0.7
      sourceRef:
        kind: HelmRepository
        name: angelnu-charts
        namespace: flux-system
      interval: 30m
  values:
    image:
      repository: ghcr.io/k8snetworkplumbingwg/multus-cni
      tag: v4.1.4-thick
    cni:
      image:
        repository: ghcr.io/angelnu/cni-plugins
        tag: 1.5.1
      paths:
        bin: /opt/cni/bin
        config: /etc/cni/net.d
    resources:
      requests:
        cpu: 10m
      limits:
        memory: 128Mi
    hostPaths:
      netns: /var/run/netns

File: ./kubernetes/apps/network/multus/app/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - helmrelease.yaml
  - networkattachment.yaml

File: ./kubernetes/apps/network/multus/ks.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app multus
  namespace: flux-system
spec:
  targetNamespace: kube-system
  interval: 10m
  path: ./kubernetes/apps/network/multus/app
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: true

File: ./kubernetes/apps/network/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./namespace.yaml
  - ./cloudflared/ks.yaml
  - ./echo-server/ks.yaml
  - ./external-dns/ks-external.yaml
  - ./external-dns/ks-internal.yaml
  - ./ingress-nginx/ks.yaml
  - ./k8s-gateway/ks.yaml
  #  - ./multus/ks.yaml
  - ./unimus

File: ./kubernetes/apps/network/echo-server/app/helmrelease.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: echo-server
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 3.6.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    controllers:
      echo-server:
        strategy: RollingUpdate
        containers:
          app:
            image:
              repository: ghcr.io/mendhak/http-https-echo
              tag: 35
            env:
              HTTP_PORT: &port 8080
              LOG_WITHOUT_NEWLINE: true
              LOG_IGNORE_PATH: /healthz
              PROMETHEUS_ENABLED: true
            probes:
              liveness: &probes
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /healthz
                    port: *port
                  initialDelaySeconds: 0
                  periodSeconds: 10
                  timeoutSeconds: 1
                  failureThreshold: 3
              readiness: *probes
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              capabilities: { drop: ["ALL"] }
            resources:
              requests:
                cpu: 10m
              limits:
                memory: 64Mi
    defaultPodOptions:
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
        runAsGroup: 65534
        seccompProfile: { type: RuntimeDefault }
    service:
      app:
        controller: echo-server
        ports:
          http:
            port: *port
    serviceMonitor:
      app:
        serviceName: echo-server
        endpoints:
          - port: http
            scheme: http
            path: /metrics
            interval: 1m
            scrapeTimeout: 10s
    ingress:
      app:
        className: external
        annotations:
          external-dns.alpha.kubernetes.io/target: "external.${SECRET_DOMAIN}"
        hosts:
          - host: "{{ .Release.Name }}.${SECRET_DOMAIN}"
            paths:
              - path: /
                service:
                  identifier: app
                  port: http
      int:
        className: internal
        hosts:
          - host: "{{ .Release.Name }}.${SECRET_DOMAIN}"
            paths:
              - path: /
                service:
                  identifier: app
                  port: http

File: ./kubernetes/apps/network/echo-server/app/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helmrelease.yaml

File: ./kubernetes/apps/network/echo-server/ks.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app echo-server
  namespace: flux-system
spec:
  targetNamespace: network
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  path: ./kubernetes/apps/network/echo-server/app
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/network/k8s-gateway/app/helmrelease.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: k8s-gateway
spec:
  interval: 30m
  chart:
    spec:
      chart: k8s-gateway
      version: 2.4.0
      sourceRef:
        kind: HelmRepository
        name: k8s-gateway
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    fullnameOverride: k8s-gateway
    domain: "${SECRET_DOMAIN},${SECRET_DOMAIN_LEGACY} "
    ttl: 1
    service:
      type: LoadBalancer
      port: 53
      annotations:
        io.cilium/lb-ipam-ips: "10.10.12.249"
      externalTrafficPolicy: Cluster
    watchedResources: ["Ingress", "Service"]

File: ./kubernetes/apps/network/k8s-gateway/app/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helmrelease.yaml

File: ./kubernetes/apps/network/k8s-gateway/ks.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app k8s-gateway
  namespace: flux-system
spec:
  targetNamespace: network
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  path: ./kubernetes/apps/network/k8s-gateway/app
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/zammad/app/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
postgresql-pass:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/zammad/app/helm-release.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app zammad
  namespace: default
spec:
  interval: 30m
  chart:
    spec:
      chart: zammad
      version: 13.0.6
      sourceRef:
        kind: HelmRepository
        name: zammad
        namespace: flux-system
  maxHistory: 2
  install:
    createNamespace: true
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  uninstall:
    keepHistory: false
  values:
    image:
      repository: ghcr.io/zammad/zammad
      tag: "6.4@sha256:a4bf8050636d55fab510fdafd8db0311627caeceaeca119fa5e1887e718da6e7"

    service:
      type: ClusterIP
      port: 8080

    ingress:
      enabled: true
      className: "external"
      annotations:
        external-dns.alpha.kubernetes.io/target: "external.${SECRET_DOMAIN}"
        nginx.ingress.kubernetes.io/proxy-body-size: 100M
        nginx.ingress.kubernetes.io/enable-cors: "true"
        nginx.ingress.kubernetes.io/cors-allow-headers: "X-Forwarded-For"
      hosts:
        - host: &host helpdesk.${SECRET_DOMAIN}
          paths:
            - path: /
              pathType: ImplementationSpecific
      tls:
        - hosts:
            - *host

    zammadConfig:
      elasticsearch:
        enabled: true
        clusterName: zammad
        image:
          repository: docker.elastic.co/elasticsearch/elasticsearch
          tag: "7.8.0"
        coordinating:
          replicaCount: 0
        data:
          replicaCount: 0
        ingest:
          replicaCount: 0
        master:
          heapSize: 512m
          masterOnly: false
          replicaCount: 2
          resources:
            requests:
              cpu: 2
              memory: 6Gi
            limits:
              cpu: 4
              memory: 8Gi

      postgresql:
        enabled: false
        db: zammad
        host: postgres16-rw.database.svc.cluster.local
        user: zammad

    # Note: Passwords should not contain special characters requiring URL encoding
    secrets:
      autowizard:
        useExisting: false
        secretKey: autowizard
        secretName: autowizard
      elasticsearch:
        useExisting: false
        secretKey: password
        secretName: elastic-credentials
      postgresql:
        useExisting: true
        secretKey: postgresql-pass
        secretName: zammad-postgres-secrets
      redis:
        useExisting: false
        secretKey: redis-password
        secretName: redis-pass

File: ./kubernetes/apps/zammad/app/ingress-int.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: zammad-int
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: 100M
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/cors-allow-headers: "X-Forwarded-For"
  labels:
    app.kubernetes.io/instance: zammad
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: zammad
    helm.sh/chart: zammad-12.4.1
    helm.toolkit.fluxcd.io/name: zammad
    helm.toolkit.fluxcd.io/namespace: default
spec:
  ingressClassName: internal
  rules:
    - host: helpdesk.${SECRET_DOMAIN}
      http:
        paths:
          - path: /
            pathType: ImplementationSpecific
            backend:
              service:
                name: zammad-nginx
                port:
                  number: 8080
  tls:
    - hosts:
        - helpdesk.${SECRET_DOMAIN}
      secretName: helpdesk.${SECRET_DOMAIN}-tls

File: ./kubernetes/apps/zammad/app/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - helm-release.yaml
  - secret.sops.yaml
  - ingress-int.yaml

File: ./kubernetes/apps/zammad/ks.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app zammad
  namespace: flux-system
spec:
  targetNamespace: &ns default
  interval: 10m
  timeout: 2m
  path: "./kubernetes/apps/zammad/app"
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false


File: ./kubernetes/apps/nfs-subdir-provisioner/app/helm-release.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: nfs-subdir-provisioner
spec:
  interval: 5m
  chart:
    spec:
      chart: nfs-subdir-external-provisioner
      version: 4.0.18
      sourceRef:
        kind: HelmRepository
        name: nfs-subdir-external-provisioner
        namespace: flux-system
      interval: 5m
  values:
    image:
      repository: gcr.io/k8s-staging-sig-storage/nfs-subdir-external-provisioner
      tag: v4.0.2
    nfs:
      server: "192.168.90.101"
      # path: "TBD"
    mountOptions:
    - nfsvers=3
    - tcp
    - intr
    - hard
    - noatime
    - nodiratime
    storageClass:
      defaultClass: false
      archiveOnDelete: true

File: ./kubernetes/apps/nfs-subdir-provisioner/app/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
- helm-release.yaml

File: ./kubernetes/apps/nfs-subdir-provisioner/ks.yaml
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: nfs-subdir-provisioner
  namespace: flux-system
spec:
  targetNamespace: kube-system
  path: ./kubernetes/apps/nfs-subdir-provisioner/app/
  prune: true
  wait: true # Something might depend on this to be ready
  sourceRef:
    kind: GitRepository
    name: thepatriots
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/misc-pvcs/app/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./emulation-pvc.yaml
  - ./other-pvc.yaml

File: ./kubernetes/apps/misc-pvcs/app/other-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: other-pvc-v1
spec:
  resources:
    requests:
      storage: 2Ti
  accessModes:
    - ReadWriteMany
  storageClassName: cephfs-media

File: ./kubernetes/apps/misc-pvcs/ks.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app misc-pvcs
  namespace: flux-system
spec:
  targetNamespace: &ns default
  interval: 10m
  timeout: 2m
  path: "./kubernetes/apps/misc-pvcs/app"
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false


File: ./kubernetes/apps/observability/prometheus-operator-crds/app/helmrelease.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: prometheus-operator-crds
spec:
  interval: 30m
  chart:
    spec:
      chart: prometheus-operator-crds
      version: 17.0.2
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3

File: ./kubernetes/apps/observability/prometheus-operator-crds/app/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helmrelease.yaml

File: ./kubernetes/apps/observability/prometheus-operator-crds/ks.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app prometheus-operator-crds
  namespace: flux-system
spec:
  targetNamespace: observability
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  path: ./kubernetes/apps/observability/prometheus-operator-crds/app
  prune: false # never should be deleted
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/observability/kube-prometheus-stack/app/scrapeconfig.yaml
---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/monitoring.coreos.com/scrapeconfig_v1alpha1.json
apiVersion: monitoring.coreos.com/v1alpha1
kind: ScrapeConfig
metadata:
  name: &name node-exporter
spec:
  staticConfigs:
    - targets:
        - 192.168.90.101:9100
  metricsPath: /metrics
  relabelings:
    - action: replace
      targetLabel: job
      replacement: *name

File: ./kubernetes/apps/observability/kube-prometheus-stack/app/helm-release.yaml
---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
spec:
  interval: 30m
  chart:
    spec:
      chart: kube-prometheus-stack
      version: 67.1.0
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
        namespace: flux-system
  install:
    crds: Skip
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    crds: Skip
    remediation:
      strategy: rollback
      retries: 3
  dependsOn:
    - name: prometheus-operator-crds
      namespace: observability
    - name: rook-ceph-cluster
      namespace: rook-ceph
  values:
    crds:
      enabled: false
    cleanPrometheusOperatorObjectNames: true
    alertmanager:
      enabled: true
      ingress:
        enabled: true
        ingressClassName: internal
        hosts: ["prom-alerts.${SECRET_DOMAIN}"]
        pathType: Prefix
        tls:
          - hosts:
            - prom-alers.${SECRET_DOMAIN}
      alertmanagerSpec:
        alertmanagerConfiguration:
          name: alertmanager
          global:
            resolveTimeout: 5m
        externalUrl: https://prom-alerts.${SECRET_DOMAIN}
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: ceph-rbd
              resources:
                requests:
                  storage: 1Gi
    kubeApiServer:
      serviceMonitor:
        selector:
          k8s-app: kube-apiserver
    kubeScheduler:
      service:
        selector:
          k8s-app: kube-scheduler
    kubeControllerManager: &kubeControllerManager
      service:
        selector:
          k8s-app: kube-controller-manager
    kubeEtcd:
      <<: *kubeControllerManager # etcd runs on control plane nodes
    kubeProxy:
      enabled: false
    nodeExporter:
      enabled: true
    prometheus:
      ingress:
        enabled: true
        ingressClassName: internal
        hosts: ["prometheus.${SECRET_DOMAIN}"]
        pathType: Prefix
        tls:
          - hosts:
            - prometheus.${SECRET_DOMAIN}
      prometheusSpec:
        podMonitorSelectorNilUsesHelmValues: false
        probeSelectorNilUsesHelmValues: false
        ruleSelectorNilUsesHelmValues: false
        scrapeConfigSelectorNilUsesHelmValues: false
        serviceMonitorSelectorNilUsesHelmValues: false
        enableAdminAPI: true
        walCompression: true
        enableFeatures:
          - memory-snapshot-on-shutdown
        retention: 14d
        retentionSize: 50GB
        resources:
          requests:
            cpu: 200m
          limits:
            memory: 2000Mi
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: ceph-rbd
              resources:
                requests:
                  storage: 50Gi
    prometheus-node-exporter:
      fullnameOverride: node-exporter
      prometheus:
        monitor:
          enabled: true
          relabelings:
            - action: replace
              regex: (.*)
              replacement: $1
              sourceLabels: ["__meta_kubernetes_pod_node_name"]
              targetLabel: kubernetes_node
    kubeStateMetrics:
      enabled: true
    kube-state-metrics:
      fullnameOverride: kube-state-metrics
      metricLabelsAllowlist:
        - pods=[*]
        - deployments=[*]
        - persistentvolumeclaims=[*]
      prometheus:
        monitor:
          enabled: true
          relabelings:
            - action: replace
              regex: (.*)
              replacement: $1
              sourceLabels: ["__meta_kubernetes_pod_node_name"]
              targetLabel: kubernetes_node
    grafana:
      enabled: false
      forceDeployDashboards: true
    additionalPrometheusRulesMap:
      dockerhub-rules:
        groups:
          - name: dockerhub
            rules:
              - alert: DockerhubRateLimitRisk
                annotations:
                  summary: Kubernetes cluster Dockerhub rate limit risk
                expr: count(time() - container_last_seen{image=~"(docker.io).*",container!=""} < 30) > 100
                labels:
                  severity: critical
      oom-rules:
        groups:
          - name: oom
            rules:
              - alert: OomKilled
                annotations:
                  summary: Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.
                expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
                labels:
                  severity: critical

File: ./kubernetes/apps/observability/kube-prometheus-stack/app/kustomization.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/kustomization
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helm-release.yaml
  - ./scrapeconfig.yaml

File: ./kubernetes/apps/observability/kube-prometheus-stack/ks.yaml
---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/kustomize.toolkit.fluxcd.io/kustomization_v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app kube-prometheus-stack
  namespace: flux-system
spec:
  targetNamespace: observability
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  path: ./kubernetes/apps/observability/kube-prometheus-stack/app
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false
  interval: 30m
  timeout: 5m
  postBuild:
    substitute:
      APP: *app

File: ./kubernetes/apps/observability/zabbix/ks-pvc.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app zabbix-server-pvc
  namespace: flux-system
spec:
  targetNamespace: &ns observability
  interval: 10m
  path: "./kubernetes/apps/observability/zabbix/volsync-pvc"
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: true
  dependsOn:
    - name: volsync
    - name: zabbix-secrets
  postBuild:
    substitute:
      APP_NS: *ns
      VOLSYNC_CLAIM: zabbix-storage
      VOLSYNC_CAPACITY: 1Gi
      VOLSYNC_ACCESSMODES: ReadWriteMany
      VOLSYNC_STORAGECLASS: cephfs
      VOLSYNC_SNAPSHOTCLASS: csi-ceph-filesystem
      VOLSYNC_REPOSITORY_SECRET: zabbix-volsync-r2-storage-pvc-secret

File: ./kubernetes/apps/observability/zabbix/web/helm-release.yaml
# yaml-language-server: $schema=https://flux.jank.ing/helmrelease/v2/github/bjw-s/helm-charts/main/charts/library/common/values.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app zabbix-web
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 3.6.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    defaultPodOptions:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1997
        runAsGroup: 1995
        fsGroup: 1995
        fsGroupChangePolicy: OnRootMismatch
        supplementalGroups: [20, 1995, 1000, 0]
        seccompProfile: { type: RuntimeDefault }
    controllers:
      zabbix-web:
        type: deployment
        replicas: 1

        containers:
          web:
           image:
            repository: zabbix/zabbix-web-nginx-pgsql
            tag: alpine-7.2.1
           env:
            ZBX_SERVER_HOST: zabbix-server.observability.svc.cluster.local
            ZBX_SSO_SP_CERT: /var/lib/zabbix/certs/sp.crt
            ZBX_SSO_IDP_CERT: /var/lib/zabbix/certs/sp.crt
            ZBX_SSO_SP_KEY: /var/lib/zabbix/certs/sp.key
           envFrom:
            - secretRef:
                name: zabbix-secrets
    service:
      zabbix-web:
        controller: *app
        type: ClusterIP
        ports:
          http:
            port: 8080
            protocol: TCP
          http-alt:
            port: 8443
            protocol: TCP
    ingress:
      app:
        className: internal
        hosts:
          - host: "zabbix.${SECRET_DOMAIN}"
            paths:
              - path: /
                pathType: Prefix
                service:
                  identifier: zabbix-web
                  port: http
    persistence:
      tmp:
        type: emptyDir
      certs:
        globalMounts:
          - path: /usr/share/zabbix/conf/certs/
            subPath: certs
        existingClaim: zabbix-storage

File: ./kubernetes/apps/observability/zabbix/web/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helm-release.yaml

File: ./kubernetes/apps/observability/zabbix/volsync-pvc/replicationdestination.yaml
apiVersion: volsync.backube/v1alpha1
kind: ReplicationDestination
metadata:
  name: ${VOLSYNC_CLAIM}-bootstrap
spec:
  restic:
    accessModes:
    - ReadWriteOnce
    cacheCapacity: "${VOLSYNC_CACHE_CAPACITY:-1Gi}"
    cacheStorageClassName: "${VOLSYNC_CACHE_SNAPSHOTCLASS:-local-hostpath}"
    capacity: "${VOLSYNC_CAPACITY:-1Gi}"
    copyMethod: Snapshot
    moverSecurityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsUser: 1000
    repository: ${VOLSYNC_REPOSITORY_SECRET}
    storageClassName: "${VOLSYNC_STORAGECLASS:-ceph-rbd}"
    volumeSnapshotClassName: "${VOLSYNC_SNAPSHOTCLASS:-csi-ceph-blockpool}"
  trigger:
    manual: restore-once

File: ./kubernetes/apps/observability/zabbix/volsync-pvc/replicationsource.yaml
apiVersion: volsync.backube/v1alpha1
kind: ReplicationSource
metadata:
  name: ${VOLSYNC_CLAIM:-${APP}}
spec:
  sourcePVC: "${VOLSYNC_CLAIM:-${APP}}"
  restic:
    cacheCapacity: "${VOLSYNC_CACHE_CAPACITY:-1Gi}"
    cacheStorageClassName: "${VOLSYNC_CACHE_SNAPSHOTCLASS:-local-hostpath}"
    copyMethod: Snapshot
    moverSecurityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsUser: 1000
    pruneIntervalDays: 7
    repository: ${VOLSYNC_REPOSITORY_SECRET}
    retain:
      daily: 7
      weekly: 4
    storageClassName: "${VOLSYNC_STORAGECLASS:-ceph-rbd}"
    volumeSnapshotClassName: "${VOLSYNC_SNAPSHOTCLASS:-csi-ceph-blockpool}"
  trigger:
    schedule: "0 4 * * *"

File: ./kubernetes/apps/observability/zabbix/volsync-pvc/pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: "${VOLSYNC_CLAIM:-${APP}}"
spec:
  accessModes:
    - "${VOLSYNC_ACCESSMODES:-ReadWriteOnce}"
  dataSourceRef:
    kind: ReplicationDestination
    apiGroup: volsync.backube
    name: "${VOLSYNC_CLAIM}-bootstrap"
  resources:
    requests:
      storage: "${VOLSYNC_CAPACITY:-1Gi}"
  storageClassName: "${VOLSYNC_STORAGECLASS:-ceph-rbd}"


File: ./kubernetes/apps/observability/zabbix/volsync-pvc/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - pvc.yaml
  - replicationdestination.yaml
#  - replicationsource.yaml

File: ./kubernetes/apps/observability/zabbix/ks-secrets.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: zabbix-secrets
  namespace: flux-system
spec:
  targetNamespace: observability
  path: ./kubernetes/apps/observability/zabbix/secrets
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: true
  prune: true
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/observability/zabbix/server/helm-release.yaml
# yaml-language-server: $schema=https://flux.jank.ing/helmrelease/v2/github/bjw-s/helm-charts/main/charts/library/common/values.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app zabbix-server
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 3.6.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    defaultPodOptions:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1997
        runAsGroup: 1995
        fsGroup: 1995
        fsGroupChangePolicy: OnRootMismatch
        supplementalGroups: [20, 1995, 1000, 0]
        seccompProfile: { type: RuntimeDefault }
    controllers:
      zabbix-server:
        type: statefulset

        initContainers:
          01-init-db:
            image:
              repository:  ghcr.io/onedr0p/postgres-init
              tag: 16.4
            envFrom:
              - secretRef:
                  name: zabbix-init-secrets


        containers:
          server:
            image:
              repository: zabbix/zabbix-server-pgsql
              tag: alpine-7.2.1
            envFrom:
              - secretRef:
                  name: zabbix-secrets
    service:
      zabbix:
        controller: *app
        type: LoadBalancer
        ports:
          http:
            enabled: false
            port: 80
          agent-udp:
            port: 10051
            protocol: UDP
          agent-tcp:
            port: 10051
            protocol: TCP

    persistence:
      config:
        globalMounts:
          - path: /var/lib/zabbix
          - path: /usr/share/zabbix/conf/certs/
            subPath: certs
        existingClaim: zabbix-storage

File: ./kubernetes/apps/observability/zabbix/server/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helm-release.yaml

File: ./kubernetes/apps/observability/zabbix/ks-server.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: zabbix-server
  namespace: flux-system
spec:
  targetNamespace: observability
  path: ./kubernetes/apps/observability/zabbix/server
  sourceRef:
    kind: GitRepository
    name: thepatriots
  dependsOn:
    - name: zabbix-secrets
    - name: zabbix-server-pvc
    - name: rook-ceph-cluster
  wait: true
  prune: true
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/observability/zabbix/secrets/volsync-secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
RESTIC_REPOSITORY:
RESTIC_PASSWORD:
AWS_ACCESS_KEY_ID:
AWS_SECRET_ACCESS_KEY:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/observability/zabbix/secrets/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
DB_SERVER_HOST:
DB_SERVER_PORT:
ZBX_SERVER_NAME:
POSTGRES_USER:
POSTGRES_PASSWORD:
POSTGRES_DB:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/observability/zabbix/secrets/.decrypted~secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
DB_SERVER_HOST:
DB_SERVER_PORT:
ZBX_SERVER_NAME:
POSTGRES_USER:
POSTGRES_PASSWORD:
POSTGRES_DB:

File: ./kubernetes/apps/observability/zabbix/secrets/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./db-init-secret.sops.yaml
  - ./secret.sops.yaml
  - ./volsync-secret.sops.yaml

File: ./kubernetes/apps/observability/zabbix/secrets/db-init-secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
INIT_POSTGRES_DBNAME:
INIT_POSTGRES_HOST:
INIT_POSTGRES_USER:
INIT_POSTGRES_PASS:
INIT_POSTGRES_SUPER_PASS:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/observability/zabbix/ks-web.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: zabbix-web
  namespace: flux-system
spec:
  targetNamespace: observability
  path: ./kubernetes/apps/observability/zabbix/web
  sourceRef:
    kind: GitRepository
    name: thepatriots
  dependsOn:
    - name: zabbix-server
  wait: false # no Flux KS deps
  prune: true
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/observability/namespace.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: observability
  labels:
    kustomize.toolkit.fluxcd.io/prune: disabled

File: ./kubernetes/apps/observability/uptimekuma/app/helm-release.yaml
---
# yaml-language-server: $schema=https://flux.jank.ing/helmrelease/v2/github/bjw-s/helm-charts/common-3.5.1/charts/library/common
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: uptimekuma
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 3.6.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  uninstall:
    keepHistory: false
  values:
    controllers:
      main:
        containers:
          main:
            image:
              repository: docker.io/louislam/uptime-kuma
              tag: 1.23.16-alpine
              pullPolicy: IfNotPresent
            env:
              TZ: ${TZ}
              UPTIME_KUMA_DISABLE_FRAME_SAMEORIGIN: 0
            resources:
              requests:
                cpu: 200m
                memory: 128Mi
              limits:
                memory: 256Mi
    service:
      main:
        controller: main
        ports:
          http:
            enabled: true
            port: 3001
    ingress:
      main:
        enabled: true
        className: internal
        annotations:
          nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
          nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"
          nginx.ingress.kubernetes.io/server-snippets: |
            location / {
              proxy_set_header Upgrade $http_upgrade;
              proxy_http_version 1.1;
              proxy_set_header X-Forwarded-Host $http_host;
              proxy_set_header X-Forwarded-Proto $scheme;
              proxy_set_header X-Forwarded-For $remote_addr;
              proxy_set_header Host $host;
              proxy_set_header Connection "upgrade";
              proxy_cache_bypass $http_upgrade;
            }
        hosts:
          - host: &host uptime.${SECRET_DOMAIN}
            paths:
              - path: /
                service:
                  identifier: main
                  port: http
        tls:
          - hosts:
              - *host
    persistence:
      data:
        enabled: true
        type: persistentVolumeClaim
        storageClass: cephfs
        accessMode: ReadWriteMany
        size: 3Gi
        globalMounts:
          - path: /app/data

File: ./kubernetes/apps/observability/uptimekuma/app/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helm-release.yaml

File: ./kubernetes/apps/observability/uptimekuma/ks.yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app uptimekuma
  namespace: flux-system
spec:
  targetNamespace: observability
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  dependsOn:
    - name: rook-ceph-cluster
  path: ./kubernetes/apps/observability/uptimekuma/app
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  healthChecks:
    - apiVersion: helm.toolkit.fluxcd.io/v2
      kind: HelmRelease
      name: uptimekuma
      namespace: observability
  interval: 30m
  retryInterval: 1m
  timeout: 3m
  postBuild:
    substitute:
      APP: *app

File: ./kubernetes/apps/observability/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./namespace.yaml
  - ./prometheus-operator-crds/ks.yaml
  - ./kube-prometheus-stack/ks.yaml
  - ./uptimekuma/ks.yaml
  - ./zabbix/ks-secrets.yaml
  - ./zabbix/ks-pvc.yaml
  - ./zabbix/ks-server.yaml
  - ./zabbix/ks-web.yaml

File: ./kubernetes/apps/matrix-synapse/element/app/helm-release.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2beta2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: element
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 3.6.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  uninstall:
    keepHistory: false
  values:
    controllers:
      element:
        annotations:
          reloader.stakater.com/auto: "true"
        containers:
          app:
            image:
              repository: vectorim/element-web
              tag: v1.11.89@sha256:d58031e793e00eed951c340e0a11da165774863e0510ec184bb4843debd084f2

    service:
      app:
        controller: element
        ports:
          http:
            port: 80

    ingress:
      app:
        className: external
        annotations:
          nginx.ingress.kubernetes.io/configuration-snippet: |
            add_header X-Frame-Options SAMEORIGIN;
            add_header X-Content-Type-Options nosniff;
            add_header X-XSS-Protection "1; mode=block";
            add_header Content-Security-Policy "frame-ancestors 'none'";
          external-dns.alpha.kubernetes.io/target: "external.${SECRET_DOMAIN}"
        hosts:
          - host: "{{ .Release.Name }}.${SECRET_DOMAIN_CHAT}"
            paths:
              - path: /
                service:
                  identifier: app
                  port: http
        tls:
        - secretName: "{{ .Release.Name }}.${SECRET_DOMAIN_CHAT}-tls"
          hosts:
            - "{{ .Release.Name }}.${SECRET_DOMAIN_CHAT}"
      internal:
        className: internal
        annotations:
          nginx.ingress.kubernetes.io/configuration-snippet: |
            add_header X-Frame-Options SAMEORIGIN;
            add_header X-Content-Type-Options nosniff;
            add_header X-XSS-Protection "1; mode=block";
            add_header Content-Security-Policy "frame-ancestors 'none'";
        hosts:
          - host: "{{ .Release.Name }}.${SECRET_DOMAIN_CHAT}"
            paths:
              - path: /
                service:
                  identifier: app
                  port: http
        tls:
        - secretName: "{{ .Release.Name }}.${SECRET_DOMAIN_CHAT}-tls"
          hosts:
            - "{{ .Release.Name }}.${SECRET_DOMAIN_CHAT}"

    persistence:
      config:
        enabled: true
        type: configMap
        name: element-config
        globalMounts:
          - path: /app/config.json
            subPath: config.json

File: ./kubernetes/apps/matrix-synapse/element/app/kustomization.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/kustomization
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: flux-system
resources:
  - ./helm-release.yaml
configMapGenerator:
  - name: element-config
    files:
      - config.json=./config/config.json
generatorOptions:
  disableNameSuffixHash: true

File: ./kubernetes/apps/matrix-synapse/element/ks.yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: matrix-synapse-element
  namespace: flux-system
spec:
  targetNamespace: default
  interval: 30m
  timeout: 5m
  path: "./kubernetes/apps/matrix-synapse/element/app"
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false

File: ./kubernetes/apps/matrix-synapse/app/internal-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: matrix-synapse-int
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/use-regex: "true"
    nginx.ingress.kubernetes.io/proxy-body-size: 100M
    # nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/cors-allow-headers: "X-Forwarded-For"
spec:
  ingressClassName: internal
  rules:
    - host: ${SECRET_DOMAIN_CHAT}
      http:
        paths:
          - path: /_matrix/media/.*
            pathType: ImplementationSpecific
            backend:
              service:
                name: matrix-synapse-media-repository
                port:
                  number: 8083
          - path: /_matrix
            pathType: Prefix
            backend:
              service:
                name: matrix-synapse
                port:
                  number: 8008
          - path: /.well-known/matrix
            pathType: Prefix
            backend:
              service:
                name: matrix-synapse-wellknown-lighttpd
                port:
                  number: 80
    - host: matrix.${SECRET_DOMAIN_CHAT}
      http:
        paths:
          - path: /_matrix/media/.*
            pathType: ImplementationSpecific
            backend:
              service:
                name: matrix-synapse-media-repository
                port:
                  number: 8083
          - path: /_matrix
            pathType: Prefix
            backend:
              service:
                name: matrix-synapse
                port:
                  number: 8008
  tls:
    - secretName: matrix.${SECRET_DOMAIN_CHAT}-tls
      hosts:
        - ${SECRET_DOMAIN_CHAT}
        - matrix.${SECRET_DOMAIN_CHAT}

File: ./kubernetes/apps/matrix-synapse/app/matrix-signing-key.sops.yaml
apiVersion:
kind:
metadata:
name:
namespace:
data:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/matrix-synapse/app/helm-release.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: matrix-synapse
  namespace: default
spec:
  interval: 5m
  chart:
    spec:
      chart: matrix-synapse
      version: 3.10.3
      sourceRef:
        kind: HelmRepository
        name: ananace-charts
        namespace: flux-system
      interval: 5m
  valuesFrom:
    - kind: Secret
      name: matrix-synapse-shared-secret
      valuesKey: registration_pass
      targetPath: config.registrationSharedSecret
  values:
    image:
      repository: ghcr.io/element-hq/synapse
      tag: v1.120.2
    serverName: ${SECRET_DOMAIN_CHAT}
    publicServerName: matrix.${SECRET_DOMAIN_CHAT}
    wellknown:
      enabled: true
      image:
        repository: ghcr.io/rtsp/docker-lighttpd
        tag: 1.4.76

    config:
       enableRegistration: true
#      registrationSharedSecret: # Value is passed via Flux's valuesFrom secret feature.
    extraConfig:
      registration_requires_token: true

    workers:
      default:
        resources:
          limits:
            memory: 1Gi
          requests:
            memory: 512Mi
      media_repository:
        enabled: true
        strategy:
          type: Recreate
      federation_sender:
        enabled: false
        replicaCount: 2
      generic_worker:
        enabled: false
        generic: true
        replicaCount: 2
        listeners: [ client ]
        csPaths:
          ## Client API requests
          - "/_matrix/client/(api/v1|r0|v3|unstable)/createRoom$"
          - "/_matrix/client/(api/v1|r0|v3|unstable)/publicRooms$"
          - "/_matrix/client/(api/v1|r0|v3|unstable)/rooms/.*/joined_members$"
          - "/_matrix/client/(api/v1|r0|v3|unstable)/rooms/.*/context/"
          - "/_matrix/client/(api/v1|r0|v3|unstable)/rooms/.*/members$"
          - "/_matrix/client/(api/v1|r0|v3|unstable)/rooms/.*/state$"
          - "/_matrix/client/v1/rooms/.*/hierarchy$"
          - "/_matrix/client/unstable/org.matrix.msc2716/rooms/.*/batch_send$"
          - "/_matrix/client/unstable/im.nheko.summary/rooms/.*/summary$"
          - "/_matrix/client/(r0|v3|unstable)/account/3pid$"
          - "/_matrix/client/(r0|v3|unstable)/account/whoami$"
          - "/_matrix/client/(r0|v3|unstable)/devices$"
          - "/_matrix/client/versions$"
          - "/_matrix/client/(api/v1|r0|v3|unstable)/voip/turnServer$"
          - "/_matrix/client/(api/v1|r0|v3|unstable)/rooms/.*/event/"
          - "/_matrix/client/(api/v1|r0|v3|unstable)/joined_rooms$"
          - "/_matrix/client/(api/v1|r0|v3|unstable)/search$"

          ## Encryption requests
          - "/_matrix/client/(r0|v3|unstable)/keys/query$"
          - "/_matrix/client/(r0|v3|unstable)/keys/changes$"
          - "/_matrix/client/(r0|v3|unstable)/keys/claim$"
          - "/_matrix/client/(r0|v3|unstable)/room_keys/"

          ## Registration/login requests
          - "/_matrix/client/(api/v1|r0|v3|unstable)/login$"
          - "/_matrix/client/(r0|v3|unstable)/register$"
          - "/_matrix/client/v1/register/m.login.registration_token/validity$"

          ## Event sending requests
          - "/_matrix/client/(api/v1|r0|v3|unstable)/rooms/.*/redact"
          - "/_matrix/client/(api/v1|r0|v3|unstable)/rooms/.*/send"
          - "/_matrix/client/(api/v1|r0|v3|unstable)/rooms/.*/state/"
          - "/_matrix/client/(api/v1|r0|v3|unstable)/rooms/.*/(join|invite|leave|ban|unban|kick)$"
          - "/_matrix/client/(api/v1|r0|v3|unstable)/join/"
          - "/_matrix/client/(api/v1|r0|v3|unstable)/profile/"

          ## User directory search requests
          - "/_matrix/client/(r0|v3|unstable)/user_directory/search"
      synchrotron:
        enabled: true
        generic: true
        listeners: [ client ]
        csPaths:
          - "/_matrix/client/(v2_alpha|r0|v3)/sync$"
          - "/_matrix/client/(api/v1|v2_alpha|r0|v3)/events$"
          - "/_matrix/client/(api/v1|r0|v3)/initialSync$"
          - "/_matrix/client/(api/v1|r0|v3)/rooms/[^/]+/initialSync$"

    ingress:
      enabled: true
      className: "external"
      annotations:
        cert-manager.io/cluster-issuer: "letsencrypt-production"
        external-dns.alpha.kubernetes.io/target: "external.${SECRET_DOMAIN}"
        nginx.ingress.kubernetes.io/use-regex: "true"
        nginx.ingress.kubernetes.io/proxy-body-size: 100M
        # nginx.ingress.kubernetes.io/enable-cors: "true"
        nginx.ingress.kubernetes.io/cors-allow-headers: "X-Forwarded-For"
      tls:
        - secretName: matrix.${SECRET_DOMAIN_CHAT}-tls
          hosts:
            - ${SECRET_DOMAIN_CHAT}
            - matrix.${SECRET_DOMAIN_CHAT}

    postgresql:
      enabled: false

    externalPostgresql:
      host: postgres16-rw.database.svc.cluster.local
      port: 5432
      username: matrix_synapse
      existingSecret: synapse-postgres-secrets
      existingSecretPasswordKey: postgresql-pass
      database: "synapse"

    redis:
      enabled: true
      auth:
        enabled: true
        password: ${SECRET_MATRIX_REDIS_PASSWORD}

    synapse:
      strategy:
        type: Recreate
      resources:
        requests:
          cpu: 1000m
          memory: 2500Mi
        limits:
          cpu: 1000m
          memory: 2500Mi

    signingkey:
      job:
        enabled: true
      # existingSecret: matrix-synapse-signingkey # DO NOT REMOVE, job and existingSecret cannot be enabled at the same time

    persistence:
      enabled: true
      existingClaim: synapse-data

File: ./kubernetes/apps/matrix-synapse/app/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: synapse-data
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
  # storageClassName: standard

File: ./kubernetes/apps/matrix-synapse/app/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - pvc.yaml
  - synapse-postgres-secrets.sops.yaml
  - matrix-synapse-shared-secret.sops.yaml
  - helm-release.yaml
  - internal-ingress.yaml

File: ./kubernetes/apps/matrix-synapse/app/synapse-postgres-secrets.sops.yaml
apiVersion:
kind:
metadata:
name:
namespace:
stringData:
postgresql-pass:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/matrix-synapse/app/matrix-synapse-shared-secret.sops.yaml
apiVersion:
kind:
metadata:
name:
namespace:
stringData:
registration_pass:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/matrix-synapse/ks.yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: matrix-synapse
  namespace: flux-system
spec:
  targetNamespace: default
  interval: 30m
  timeout: 5m
  path: "./kubernetes/apps/matrix-synapse/app"
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false

File: ./kubernetes/apps/database/dragonfly/operator/helm-release.yaml
---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: dragonfly-operator
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 3.6.0
      interval: 30m
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  values:
    defaultPodOptions:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node-role.kubernetes.io/control-plane
                    operator: Exists
    serviceAccount:
      create: false
      name: dragonfly-operator-controller-manager
    controllers:
      dragonfly-operator:
        containers:
          rbac-proxy:
            image:
              repository: gcr.io/kubebuilder/kube-rbac-proxy
              tag: v0.16.0
            args:
              - "--secure-listen-address=0.0.0.0:8443"
              - "--upstream=http://127.0.0.1:8080/"
              - "--logtostderr=true"
              - "--v=0"
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              capabilities:
                drop:
                  - ALL
            resources:
              requests:
                cpu: 5m
                memory: 64Mi
              limits:
                cpu: 500m
                memory: 128Mi
          app:
            image:
              repository: docker.dragonflydb.io/dragonflydb/operator
              tag: v1.1.8
            args:
              - "--health-probe-bind-address=:8081"
              - "--metrics-bind-address=127.0.0.1:8080"
              - "--leader-elect"
            command:
              - "/manager"
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              capabilities:
                drop:
                  - ALL
            probes:
              liveness:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /healthz
                    port: &port 8081
                  initialDelaySeconds: 15
                  periodSeconds: 20
                  timeoutSeconds: 1
                  failureThreshold: 3
              readiness:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /readyz
                    port: *port
                  initialDelaySeconds: 5
                  periodSeconds: 10
                  timeoutSeconds: 1
                  failureThreshold: 3
              startup:
                enabled: false
            resources:
              requests:
                cpu: 10m
                memory: 64Mi
              limits:
                cpu: 500m
                memory: 128Mi
        annotations:
          reloader.stakater.com/auto: "true"
    service:
      app:
        controller: dragonfly-operator
        ports:
          http:
            port: *port

File: ./kubernetes/apps/database/dragonfly/operator/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: dragonfly-operator
    app.kubernetes.io/instance: controller-manager-sa
    app.kubernetes.io/managed-by: kustomize
    app.kubernetes.io/name: serviceaccount
    app.kubernetes.io/part-of: dragonfly-operator
  name: dragonfly-operator-controller-manager
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: dragonfly-operator
    app.kubernetes.io/instance: leader-election-role
    app.kubernetes.io/managed-by: kustomize
    app.kubernetes.io/name: role
    app.kubernetes.io/part-of: dragonfly-operator
  name: dragonfly-operator-leader-election-role
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: dragonfly-operator-manager-role
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - apps
    resources:
      - statefulsets
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - dragonflydb.io
    resources:
      - dragonflies
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - dragonflydb.io
    resources:
      - dragonflies/finalizers
    verbs:
      - update
  - apiGroups:
      - dragonflydb.io
    resources:
      - dragonflies/status
    verbs:
      - get
      - patch
      - update
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/component: kube-rbac-proxy
    app.kubernetes.io/created-by: dragonfly-operator
    app.kubernetes.io/instance: metrics-reader
    app.kubernetes.io/managed-by: kustomize
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/part-of: dragonfly-operator
  name: dragonfly-operator-metrics-reader
rules:
  - nonResourceURLs:
      - /metrics
    verbs:
      - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/component: kube-rbac-proxy
    app.kubernetes.io/created-by: dragonfly-operator
    app.kubernetes.io/instance: proxy-role
    app.kubernetes.io/managed-by: kustomize
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/part-of: dragonfly-operator
  name: dragonfly-operator-proxy-role
rules:
  - apiGroups:
      - authentication.k8s.io
    resources:
      - tokenreviews
    verbs:
      - create
  - apiGroups:
      - authorization.k8s.io
    resources:
      - subjectaccessreviews
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: dragonfly-operator
    app.kubernetes.io/instance: leader-election-rolebinding
    app.kubernetes.io/managed-by: kustomize
    app.kubernetes.io/name: rolebinding
    app.kubernetes.io/part-of: dragonfly-operator
  name: dragonfly-operator-leader-election-rolebinding

roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: dragonfly-operator-leader-election-role
subjects:
  - kind: ServiceAccount
    name: dragonfly-operator-controller-manager
    namespace: database
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: dragonfly-operator
    app.kubernetes.io/instance: manager-rolebinding
    app.kubernetes.io/managed-by: kustomize
    app.kubernetes.io/name: clusterrolebinding
    app.kubernetes.io/part-of: dragonfly-operator
  name: dragonfly-operator-manager-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: dragonfly-operator-manager-role
subjects:
  - kind: ServiceAccount
    name: dragonfly-operator-controller-manager
    namespace: database
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/component: kube-rbac-proxy
    app.kubernetes.io/created-by: dragonfly-operator
    app.kubernetes.io/instance: proxy-rolebinding
    app.kubernetes.io/managed-by: kustomize
    app.kubernetes.io/name: clusterrolebinding
    app.kubernetes.io/part-of: dragonfly-operator
  name: dragonfly-operator-proxy-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: dragonfly-operator-proxy-role
subjects:
  - kind: ServiceAccount
    name: dragonfly-operator-controller-manager
    namespace: database

File: ./kubernetes/apps/database/dragonfly/operator/kustomization.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/kustomization
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  # renovate: datasource=github-releases depName=dragonflydb/dragonfly-operator
  - https://raw.githubusercontent.com/dragonflydb/dragonfly-operator/v1.1.8/manifests/crd.yaml
  - ./helm-release.yaml
  - ./rbac.yaml

File: ./kubernetes/apps/database/dragonfly/ks.yaml
---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/kustomize.toolkit.fluxcd.io/kustomization_v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app dragonfly-operator
  namespace: flux-system
spec:
  targetNamespace: database
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  path: ./kubernetes/apps/database/dragonfly/operator
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: true
  interval: 30m
  retryInterval: 1m
  timeout: 5m
---
---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/kustomize.toolkit.fluxcd.io/kustomization_v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app dragonfly-cluster
  namespace: flux-system
spec:
  targetNamespace: database
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  dependsOn:
    - name: dragonfly-operator
  path: ./kubernetes/apps/database/dragonfly/cluster
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: true
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/database/dragonfly/cluster/podmonitor.yaml
---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/monitoring.coreos.com/podmonitor_v1.json
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: dragonfly
spec:
  selector:
    matchLabels:
      app: dragonfly
  podTargetLabels: ["app"]
  podMetricsEndpoints:
    - port: admin

File: ./kubernetes/apps/database/dragonfly/cluster/kustomization.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/kustomization
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./cluster.yaml
  - ./podmonitor.yaml

File: ./kubernetes/apps/database/dragonfly/cluster/cluster.yaml
---
# yaml-language-server: $schema=https://kochhaus-schemas.pages.dev/dragonflydb.io/dragonfly_v1alpha1.json
apiVersion: dragonflydb.io/v1alpha1
kind: Dragonfly
metadata:
  name: dragonfly
spec:
  image: ghcr.io/dragonflydb/dragonfly:v1.26.0
  replicas: 3
  env:
    - name: MAX_MEMORY
      valueFrom:
        resourceFieldRef:
          resource: limits.memory
          divisor: 1Mi
  args:
    - --maxmemory=$(MAX_MEMORY)Mi
    - --proactor_threads=2
    - --cluster_mode=emulated
    - --lock_on_hashtags
  resources:
    requests:
      cpu: 50m
      memory: 128Mi
    limits:
      memory: 512Mi
  topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: "kubernetes.io/hostname"
      whenUnsatisfiable: "DoNotSchedule"
      labelSelector:
        matchLabels:
          app.kubernetes.io/part-of: dragonfly

File: ./kubernetes/apps/database/mariadb/ks-resources.yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: mariadb
  namespace: flux-system
spec:
  dependsOn:
    - name: mariadb-operator
  targetNamespace: database
  path: ./kubernetes/apps/database/mariadb/resources
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/database/mariadb/operator/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
maxscale_password:
password:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/database/mariadb/operator/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: databases
resources:
  - hr.yaml
  - secret.sops.yaml

File: ./kubernetes/apps/database/mariadb/operator/hr.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: mariadb-operator
spec:
  chart:
    spec:
      chart: mariadb-operator
      sourceRef:
        kind: HelmRepository
        name: mariadb-operator
        namespace: flux-system
      version: "0.36.0"
  interval: 1h0m0s
  values:
    logLevel: debug
    image:
      repository: ghcr.io/mariadb-operator/mariadb-operator
      pullPolicy: IfNotPresent
      tag: 0.36.0
    metrics:
      enabled: true
      serviceMonitor:
        enabled: true
    webhook:
      certificate:
        certManager: true
      serviceMonitor:
        enabled: true

File: ./kubernetes/apps/database/mariadb/ks-phpmyadmin.yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: mariadb-phpmyadmin
  namespace: flux-system
spec:
  targetNamespace: database
  path: ./kubernetes/apps/database/mariadb/phpmyadmin/app
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/database/mariadb/crds/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - hr.yaml

File: ./kubernetes/apps/database/mariadb/crds/hr.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: mariadb-operator-crds
spec:
  chart:
    spec:
      chart: mariadb-operator-crds
      sourceRef:
        kind: HelmRepository
        name: mariadb-operator
        namespace: flux-system
      version: "0.36.0"
  interval: 1h0m0s

File: ./kubernetes/apps/database/mariadb/ks-crds.yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: mariadb-operator-crds
  namespace: flux-system
spec:
  targetNamespace: database
  path: ./kubernetes/apps/database/mariadb/crds
  prune: true
  wait: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/database/mariadb/phpmyadmin/app/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: databases
resources:
  - hr.yaml

File: ./kubernetes/apps/database/mariadb/phpmyadmin/app/hr.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: phpmyadmin
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 3.6.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  maxHistory: 2
  install:
    createNamespace: true
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  uninstall:
    keepHistory: false
  values:
    controllers:
      phpmyadmin:
        annotations:
          reloader.stakater.com/auto: "true"

        containers:
          phpmyadmin:
            image:
              repository: registry.skysolutions.fi/docker.io/phpmyadmin
              tag: 5.2.1-apache
            env:
              PMA_ARBITRARY: 1
            resources:
              requests:
                cpu: 30m
                memory: 256Mi
              limits:
                memory: 512Mi

    service:
      phpmyadmin:
        controller: phpmyadmin
        ports:
          http:
            port: 80
    ingress:
      main:
        enabled: true
        className: "internal"
        hosts:
          - host: &host "phpmyadmin.${SECRET_DOMAIN}"
            paths:
              - path: "/"
                pathType: Prefix
                service:
                  identifier: phpmyadmin
                  port: 80
        tls:
          - hosts:
              - "phpmyadmin.${SECRET_DOMAIN}"

File: ./kubernetes/apps/database/mariadb/ks.yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: mariadb-operator
  namespace: flux-system
spec:
  targetNamespace: database
  path: ./kubernetes/apps/database/mariadb/operator
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  dependsOn:
    - name: mariadb-operator-crds
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/database/mariadb/resources/ingress-maxscale.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: maxscale-galera-http
spec:
  rules:
    - host: mariadb-maxscale.${SECRET_DOMAIN}
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: mariadb-galera-maxscale-gui
                port:
                  number: 8989

File: ./kubernetes/apps/database/mariadb/resources/externalsecret-backups.yaml
---
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: mariadb-backups
  namespace: databases
spec:
  refreshInterval: 5m
  secretStoreRef:
    kind: ClusterSecretStore
    name: onepassword-connect
  target:
    name: mariadb-galera-s3-secret
    deletionPolicy: Delete
    template:
      engineVersion: v2
      data:
        access-key-id: "{{ .ACCESS_KEY_ID }}"
        secret-access-key: "{{ .SECRET_ACCESS_KEY }}"
  dataFrom:
    - extract:
        key: mariadb-backups

File: ./kubernetes/apps/database/mariadb/resources/externalsecret.yaml
---
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: mariadb
  namespace: databases
spec:
  refreshInterval: 5m
  secretStoreRef:
    kind: ClusterSecretStore
    name: onepassword-connect
  target:
    name: mariadb-secret
    deletionPolicy: Delete
    template:
      engineVersion: v2
      data:
        password: "{{ .ROOT_PASSWORD }}"
        maxscale_password: "{{ .MAXSCALE_PASSWORD }}"
  dataFrom:
    - extract:
        key: mariadb

File: ./kubernetes/apps/database/mariadb/resources/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: databases
resources:
  - ./backup.yaml
  - ./mariadb.yaml
  - ./provisions
  - ingress-maxscale.yaml
labels:
  - pairs:
      app.kubernetes.io/name: mariadb
      app.kubernetes.io/part-of: mariadb

File: ./kubernetes/apps/database/mariadb/resources/backup.yaml
---
# yaml-language-server: $scema=https://ks.hsn.dev/k8s.mariadb.com/backup_v1alpha1.json
apiVersion: k8s.mariadb.com/v1alpha1
kind: Backup
metadata:
  name: mariadb-galera-invoiceninja-daily
spec:
  mariaDbRef:
    name: mariadb-galera

  schedule:
    cron: "5 4 * * *"

  databases:
    - "invoiceninja"

  storage:
    persistentVolumeClaim:
      accessModes:
        - "ReadWriteOnce"
      resources:
        requests:
          storage: 6Gi

      storageClassName: ceph-rbd


File: ./kubernetes/apps/database/mariadb/resources/mariadb.yaml
apiVersion: k8s.mariadb.com/v1alpha1
kind: MariaDB
metadata:
  name: mariadb-galera
spec:
  rootPasswordSecretKeyRef:
    name: mariadb-secret
    key: password
    generate: false

  storage:
    size: 5Gi
    storageClassName: ceph-rbd

  replicas: 3
  image: "mariadb:11.5.2"
  imagePullPolicy: IfNotPresent

  updateStrategy:
    autoUpdateDataPlane: true

  maxScale:
    enabled: true

    auth:
      adminUsername: root
      adminPasswordSecretKeyRef:
        name: mariadb-secret
        key: maxscale_password
      deleteDefaultAdmin: true

    kubernetesService:
      type: ClusterIP

    guiKubernetesService:
      type: ClusterIP

    connection:
      secretName: mxs-galera-conn
      port: 3306


  galera:
    enabled: true
    recovery:
      enabled: true
      #forceClusterBootstrapInPod: "mariadb-galera-1"
      podRecoveryTimeout: 15m

  service:
    type: LoadBalancer

  primaryService:
    type: LoadBalancer

  secondaryService:
    type: LoadBalancer

  myCnf: |
    [mariadb]
    bind-address=*
    default_storage_engine=InnoDB
    binlog_format=row
    innodb_autoinc_lock_mode=2
    max_allowed_packet=256M

  resources:
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      memory: 2Gi

  podSecurityContext:
    runAsUser: 999
    runAsGroup: 999
    fsGroup: 999
    fsGroupChangePolicy: OnRootMismatch


  #  livenessProbe:
#    initialDelaySeconds: 20
#    periodSeconds: 10
#    timeoutSeconds: 10
#
#  readinessProbe:
#    initialDelaySeconds: 20
#    periodSeconds: 10
#    timeoutSeconds: 10

  metrics:
    enabled: true

File: ./kubernetes/apps/database/mariadb/resources/provisions/bookstack/user.yaml
apiVersion: k8s.mariadb.com/v1alpha1
kind: User
metadata:
  name: bookstack
  namespace: database
spec:
  mariaDbRef:
    name: mariadb-galera
  passwordSecretKeyRef:
    name: bookstack-secret
    key: mariadb-password
  maxUserConnections: 20

File: ./kubernetes/apps/database/mariadb/resources/provisions/bookstack/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
mariadb-password:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/database/mariadb/resources/provisions/bookstack/db.yaml
apiVersion: k8s.mariadb.com/v1alpha1
kind: Database
metadata:
  name: bookstackapp
  namespace: database
spec:
  mariaDbRef:
    name: mariadb-galera
  characterSet: utf8mb4
  collate: utf8mb4_unicode_ci

File: ./kubernetes/apps/database/mariadb/resources/provisions/bookstack/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./user.yaml
  - ./db.yaml
  - ./grant.yaml
  - ./secret.sops.yaml

File: ./kubernetes/apps/database/mariadb/resources/provisions/bookstack/grant.yaml
apiVersion: k8s.mariadb.com/v1alpha1
kind: Grant
metadata:
  name: bookstack
  namespace: database
spec:
  mariaDbRef:
    name: mariadb-galera
  privileges:
    - "ALL PRIVILEGES"
  database: "bookstackapp"
  table: "*"
  username: bookstack

File: ./kubernetes/apps/database/mariadb/resources/provisions/invoiceninja/user.yaml
apiVersion: k8s.mariadb.com/v1alpha1
kind: User
metadata:
  name: invoiceninja
spec:
  mariaDbRef:
    name: mariadb-galera
  passwordSecretKeyRef:
    name: invoiceninja-secret
    key: mariadb-password
  maxUserConnections: 20

File: ./kubernetes/apps/database/mariadb/resources/provisions/invoiceninja/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
mariadb-password:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/database/mariadb/resources/provisions/invoiceninja/db.yaml
apiVersion: k8s.mariadb.com/v1alpha1
kind: Database
metadata:
  name: invoiceninja
spec:
  mariaDbRef:
    name: mariadb-galera
  characterSet: utf8
  collate: utf8_general_ci

File: ./kubernetes/apps/database/mariadb/resources/provisions/invoiceninja/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./user.yaml
  - ./db.yaml
  - ./grant.yaml
  - ./secret.sops.yaml

File: ./kubernetes/apps/database/mariadb/resources/provisions/invoiceninja/grant.yaml
apiVersion: k8s.mariadb.com/v1alpha1
kind: Grant
metadata:
  name: invoiceninja
spec:
  mariaDbRef:
    name: mariadb-galera
  privileges:
    - "ALL PRIVILEGES"
  database: "*"
  table: "*"
  username: invoiceninja

File: ./kubernetes/apps/database/mariadb/resources/provisions/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - invoiceninja
  - bookstack

File: ./kubernetes/apps/database/pgadmin/app/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
PGADMIN_DEFAULT_PASSWORD:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/database/pgadmin/app/helm-release.yaml
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: pgadmin
spec:
  interval: 15m
  chart:
    spec:
      chart: app-template
      version: 3.6.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  maxHistory: 3
  install:
    createNamespace: true
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  uninstall:
    keepHistory: false
  values:
    defaultPodOptions:
      securityContext:
        runAsUser: 5050
        runAsGroup: 0
        fsGroup: 0
        fsGroupChangePolicy: OnRootMismatch

    controllers:
      main:
        type: statefulset
        annotations:
          reloader.stakater.com/auto: "true"

        containers:
          main:
            image:
              repository: dpage/pgadmin4
              tag: 8.12
            env:
              PGADMIN_DEFAULT_EMAIL: deen@${SECRET_DOMAIN}
            envFrom:
              - secretRef:
                  name: pgadmin-secrets
            resources:
              requests:
                memory: 300Mi
              limits:
                memory: 1Gi

    service:
      main:
        controller: main
        ports:
          http:
            port: 80

    persistence:
      config:
        enabled: true
        storageClass: ceph-rbd
        accessMode: "ReadWriteOnce"
        size: 1Gi
        globalMounts:
          - path: /var/lib/pgadmin
      oauthconfig:
        enabled: true
        type: configMap
        name: pgadmin-local-config-configmap
        globalMounts:
          - path: /pgadmin4/config_local.py
            subPath: config_local.py
            readOnly: true

    ingress:
      main:
        enabled: true
        className: internal
        hosts:
          - host: pgadmin.${SECRET_DOMAIN}
            paths:
              - path: /
                pathType: Prefix
                service:
                  identifier: main
                  port: http
        tls:
          - hosts:
              - pgadmin.${SECRET_DOMAIN}

File: ./kubernetes/apps/database/pgadmin/app/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - helm-release.yaml
  - secret.sops.yaml
configMapGenerator:
  - name: pgadmin-local-config-configmap
    files:
      - config_local.py
generatorOptions:
  disableNameSuffixHash: true

File: ./kubernetes/apps/database/pgadmin/ks.yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: pgadmin
  namespace: flux-system
spec:
  targetNamespace: database
  path: ./kubernetes/apps/database/pgadmin/app
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  healthChecks:
    - apiVersion: helm.toolkit.fluxcd.io/v2
      kind: HelmRelease
      name: pgadmin
      namespace: database
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/database/cloudnative-pg/operator/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
username:
password:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/database/cloudnative-pg/operator/helm-release.yaml
---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: cloudnative-pg
spec:
  interval: 30m
  chart:
    spec:
      chart: cloudnative-pg
      version: 0.23.0
      sourceRef:
        kind: HelmRepository
        name: cloudnative-pg
        namespace: flux-system
  values:
    crds:
      create: true
    nodeSelector:
      kubernetes.io/arch: amd64
    monitoring:
      podMonitorEnabled: true
      grafanaDashboard:
        create: true

File: ./kubernetes/apps/database/cloudnative-pg/operator/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: databases
resources:
  - helm-release.yaml
  - secret.sops.yaml
generatorOptions:
  disableNameSuffixHash: true
  annotations:
    kustomize.toolkit.fluxcd.io/substitute: disabled
labels:
  - pairs:
      app.kubernetes.io/name: cloudnative-pg
      app.kubernetes.io/instance: cloudnative-pg
      app.kubernetes.io/part-of: cloudnative-pg

File: ./kubernetes/apps/database/cloudnative-pg/ks-operator.yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: cnpg-operator
  namespace: flux-system
spec:
  targetNamespace: database
  path: ./kubernetes/apps/database/cloudnative-pg/operator
  prune: false
  sourceRef:
    kind: GitRepository
    name: thepatriots
  healthChecks:
    - apiVersion: helm.toolkit.fluxcd.io/v2
      kind: HelmRelease
      name: cloudnative-pg
      namespace: database
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/database/cloudnative-pg/cluster/kustomization.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/kustomization
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./cluster16.yaml

File: ./kubernetes/apps/database/cloudnative-pg/cluster/cluster16.yaml
---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/postgresql.cnpg.io/cluster_v1.json
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: postgres16
spec:
  instances: 3
  imageName: ghcr.io/cloudnative-pg/postgresql:16.1-18
  primaryUpdateStrategy: unsupervised
  storage:
    size: 15Gi
    storageClass: ceph-rbd
  superuserSecret:
    name: cloudnative-pg-secret
  enableSuperuserAccess: true
  resources:
    requests:
      memory: 2Gi
    limits:
      memory: 4Gi
#  bootstrap:
#    recovery:
#      source: &previousCluster postgres16-v1
  affinity:
    enablePodAntiAffinity: true
    topologyKey: kubernetes.io/hostname
    podAntiAffinityType: required
    nodeSelector:
      kubernetes.io/arch: amd64
  postgresql:
    parameters:
      max_connections: "600"
      shared_buffers: 512MB

  monitoring:
    enablePodMonitor: true
    # https://github.com/cloudnative-pg/cloudnative-pg/issues/2501
    podMonitorMetricRelabelings:
      - { sourceLabels: [ "cluster" ], targetLabel: cnpg_cluster, action: replace }
      - { regex: cluster, action: labeldrop }
  backup:
    target: prefer-standby
    retentionPolicy: 30d

File: ./kubernetes/apps/database/cloudnative-pg/ks-cluster16.yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: cnpg-cluster16
  namespace: flux-system
spec:
  targetNamespace: database
  dependsOn:
    - name: cnpg-operator
  path: ./kubernetes/apps/database/cloudnative-pg/cluster
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/database/kustomization.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/kustomization
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  # Pre Flux-Kustomizations
  - ./ns.yaml
  # Flux-Kustomizations
  - cloudnative-pg/ks-operator.yaml
  - cloudnative-pg/ks-cluster16.yaml
  - pgadmin/ks.yaml
  - mariadb/ks.yaml
  - mariadb/ks-crds.yaml
  - mariadb/ks-resources.yaml
  - mariadb/ks-phpmyadmin.yaml
  - dragonfly/ks.yaml

File: ./kubernetes/apps/database/ns.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: database

File: ./kubernetes/apps/vaultwarden/volsync-pvc/app/replicationdestination.yaml
apiVersion: volsync.backube/v1alpha1
kind: ReplicationDestination
metadata:
  name: vaultwarden-bootstrap
spec:
  restic:
    accessModes:
    - ReadWriteOnce
    cacheCapacity: 2Gi
    cacheStorageClassName: local-hostpath
    capacity: 2Gi
    copyMethod: Snapshot
    moverSecurityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsUser: 1000
    repository: vaultwarden-volsync-r2-secret
    storageClassName: ceph-rbd
    volumeSnapshotClassName: csi-ceph-blockpool
  trigger:
    manual: restore-once

File: ./kubernetes/apps/vaultwarden/volsync-pvc/app/replicationsource.yaml
apiVersion: volsync.backube/v1alpha1
kind: ReplicationSource
metadata:
  name: vaultwarden
spec:
  restic:
    cacheCapacity: 2Gi
    cacheStorageClassName: local-hostpath
    copyMethod: Snapshot
    moverSecurityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsUser: 1000
    pruneIntervalDays: 7
    repository: vaultwarden-volsync-r2-secret
    retain:
      daily: 7
      weekly: 4
    storageClassName: ceph-rbd
    volumeSnapshotClassName: csi-ceph-blockpool
  sourcePVC: vaultwarden
  trigger:
    schedule: "30 4 * * *"

File: ./kubernetes/apps/vaultwarden/volsync-pvc/app/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: "${VOLSYNC_CLAIM:-${APP}}"
spec:
  accessModes:
    - "${VOLSYNC_ACCESSMODES:-ReadWriteOnce}"
  dataSourceRef:
    kind: ReplicationDestination
    apiGroup: volsync.backube
    name: "${APP}-bootstrap"
  resources:
    requests:
      storage: "${VOLSYNC_CAPACITY:-1Gi}"
  storageClassName: "${VOLSYNC_STORAGECLASS:-ceph-rbd}"

File: ./kubernetes/apps/vaultwarden/volsync-pvc/app/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./pvc.yaml
  - ./replicationdestination.yaml # to pvc
  - ./replicationsource.yaml # from pvc

File: ./kubernetes/apps/vaultwarden/volsync-pvc/ks.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app vaultwarden-pvc
  namespace: flux-system
spec:
  targetNamespace: &ns default
  interval: 10m
  timeout: 2m
  path: "./kubernetes/apps/vaultwarden/volsync-pvc/app"
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  dependsOn:
    - name: vaultwarden-secrets
    - name: rook-ceph-cluster
    - name: volsync
  wait: true
  postBuild:
    substitute:
      APP: vaultwarden
      VOLSYNC_CAPACITY: 2Gi


File: ./kubernetes/apps/vaultwarden/vaultwarden/app/helm-release.yaml
---
# yaml-language-server: $schema=https://flux.jank.ing/helmrelease/v2/github/bjw-s/helm-charts/common-3.5.1/charts/library/common
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app vaultwarden
  namespace: default
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 3.6.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  maxHistory: 2
  install:
    createNamespace: true
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  uninstall:
    keepHistory: false
  values:
    controllers:
      vaultwarden:
        annotations:
          reloader.stakater.com/auto: "true"
        containers:
          app:
            image:
              repository: vaultwarden/server
              tag: 1.32.7@sha256:7a0aa23c0947be3582898deb5170ea4359493ed9a76af2badf60a7eb45ac36af
            env:
              DATA_FOLDER: data
              ICON_CACHE_FOLDER: data/icon_cache
              ATTACHMENTS_FOLDER: data/attachments
              DOMAIN: "https://vaultwarden.${SECRET_DOMAIN}"
              TZ: "${TIMEZONE}"
              SIGNUPS_ALLOWED: "false"
              WEBSOCKET_ENABLED: "true"
              WEBSOCKET_ADDRESS: 0.0.0.0
              WEBSOCKET_PORT: 3012
              #SMTP_HOST: smtp-relay.default.svc.cluster.local.
              #SMTP_FROM: vaultwarden@${SECRET_DOMAIN}
              #SMTP_FROM_NAME: vaultwarden
              #SMTP_PORT: 2525
              #SMTP_SECURITY: "off"
            envFrom:
              - secret: vaultwarden-secrets
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
              limits:
                memory: 2Gi
    service:
      app:
        controller: *app
        ports:
          http:
            port: &port 80
    ingress:
      external:
        className: external
        annotations:
          external-dns.alpha.kubernetes.io/enabled: "true"
          external-dns.alpha.kubernetes.io/target: external.${SECRET_DOMAIN}.
          hajimari.io/icon: mdi:lock
        hosts:
          - host: &host "{{ .Release.Name }}.${SECRET_DOMAIN}"
            paths:
              - path: /
                service:
                  identifier: app
                  port: *port
        tls:
          - hosts:
              - *host
      internal:
        className: internal
        hosts:
          - host: *host
            paths:
              - path: /
                service:
                  identifier: app
                  port: *port
        tls:
          - hosts:
              - *host

    persistence:
      config:
        enabled: true
        existingClaim: *app
        globalMounts:
          - path: /data

File: ./kubernetes/apps/vaultwarden/vaultwarden/app/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helm-release.yaml

File: ./kubernetes/apps/vaultwarden/vaultwarden/ks.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app vaultwarden
  namespace: flux-system
spec:
  targetNamespace: &ns default
  interval: 10m
  path: "./kubernetes/apps/vaultwarden/vaultwarden/app"
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false
  dependsOn:
    - name: vaultwarden-pvc
  postBuild:
    substitute:
      APP: *app
      APP_NS: *ns

File: ./kubernetes/apps/vaultwarden/secrets/app/volsync-secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
RESTIC_REPOSITORY:
RESTIC_PASSWORD:
AWS_ACCESS_KEY_ID:
AWS_SECRET_ACCESS_KEY:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/vaultwarden/secrets/app/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
ADMIN_TOKEN:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/vaultwarden/secrets/app/.decrypted~secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
ADMIN_TOKEN:

File: ./kubernetes/apps/vaultwarden/secrets/app/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./secret.sops.yaml
  - ./volsync-secret.sops.yaml

File: ./kubernetes/apps/vaultwarden/secrets/ks.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app vaultwarden-secrets
  namespace: flux-system
spec:
  targetNamespace: &ns default
  interval: 10m
  path: "./kubernetes/apps/vaultwarden/secrets/app"
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: true
  postBuild:
    substitute:
      APP: *app
      APP_NS: *ns

File: ./kubernetes/apps/vaultwarden/kustomization.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/kustomization
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  # Flux-Kustomizations
  - secrets/ks.yaml
  - volsync-pvc/ks.yaml
  - vaultwarden/ks.yaml

File: ./kubernetes/apps/invoiceninja/invoiceninja-storage-pvc/app/replicationdestination.yaml
apiVersion: volsync.backube/v1alpha1
kind: ReplicationDestination
metadata:
  name: invoiceninja-storage-bootstrap
  namespace: invoiceninja
spec:
  restic:
    accessModes:
    - ReadWriteOnce
    cacheCapacity: "${VOLSYNC_CACHE_CAPACITY:-1Gi}"
    cacheStorageClassName: "${VOLSYNC_CACHE_SNAPSHOTCLASS:-local-hostpath}"
    capacity: "${VOLSYNC_CAPACITY:-1Gi}"
    copyMethod: Snapshot
    moverSecurityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsUser: 1000
    repository: ${VOLSYNC_REPOSITORY_SECRET}
    storageClassName: "${VOLSYNC_STORAGECLASS:-ceph-rbd}"
    volumeSnapshotClassName: "${VOLSYNC_SNAPSHOTCLASS:-csi-ceph-blockpool}"
  trigger:
    manual: restore-once

File: ./kubernetes/apps/invoiceninja/invoiceninja-storage-pvc/app/replicationsource.yaml
apiVersion: volsync.backube/v1alpha1
kind: ReplicationSource
metadata:
  name: invoiceninja
  namespace: invoiceninja
spec:
  sourcePVC: "${VOLSYNC_CLAIM:-${APP}}"
  restic:
    cacheCapacity: "${VOLSYNC_CACHE_CAPACITY:-1Gi}"
    cacheStorageClassName: "${VOLSYNC_CACHE_SNAPSHOTCLASS:-local-hostpath}"
    copyMethod: Snapshot
    moverSecurityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsUser: 1000
    pruneIntervalDays: 7
    repository: ${VOLSYNC_REPOSITORY_SECRET}
    retain:
      daily: 7
      weekly: 4
    storageClassName: "${VOLSYNC_STORAGECLASS:-ceph-rbd}"
    volumeSnapshotClassName: "${VOLSYNC_SNAPSHOTCLASS:-csi-ceph-blockpool}"
  trigger:
    schedule: "0 2 * * *"

File: ./kubernetes/apps/invoiceninja/invoiceninja-storage-pvc/app/pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: "${VOLSYNC_CLAIM:-${APP}}"
spec:
  accessModes:
    - "${VOLSYNC_ACCESSMODES:-ReadWriteOnce}"
  dataSourceRef:
    kind: ReplicationDestination
    apiGroup: volsync.backube
    name: "${APP}-bootstrap"
  resources:
    requests:
      storage: "${VOLSYNC_CAPACITY:-1Gi}"
  storageClassName: "${VOLSYNC_STORAGECLASS:-ceph-rbd}"


File: ./kubernetes/apps/invoiceninja/invoiceninja-storage-pvc/app/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - pvc.yaml
  - replicationdestination.yaml
  - replicationsource.yaml

File: ./kubernetes/apps/invoiceninja/invoiceninja-storage-pvc/ks.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app invoiceninja-storage-pvc
  namespace: flux-system
spec:
  targetNamespace: &ns invoiceninja
  interval: 10m
  path: "./kubernetes/apps/invoiceninja/invoiceninja-storage-pvc/app"
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: true
  dependsOn:
    - name: volsync
    - name: invoiceninja-secrets-storage-pvc
  postBuild:
    substitute:
      APP: invoiceninja-storage
      APP_NS: *ns
      VOLSYNC_CLAIM: invoiceninja-storage
      VOLSYNC_CAPACITY: 1Gi
      VOLSYNC_ACCESSMODES: ReadWriteMany
      VOLSYNC_STORAGECLASS: cephfs
      VOLSYNC_SNAPSHOTCLASS: csi-ceph-filesystem
      VOLSYNC_REPOSITORY_SECRET: invoiceninja-volsync-r2-storage-pvc-secret

File: ./kubernetes/apps/invoiceninja/invoiceninja/app/kustomizeconfig.yaml
nameReference:
- kind: Secret
  version: v1
  fieldSpecs:
  - path: spec/valuesFrom/name
    kind: HelmRelease

File: ./kubernetes/apps/invoiceninja/invoiceninja/app/backup-cronjob.yaml
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: invoiceninja-db-backup
  namespace: ${APP_NS}
spec:
  schedule: "0 4 * * *"
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        spec:
          automountServiceAccountToken: false
          enableServiceLinks: false
          securityContext:
            runAsUser: 2000
            runAsGroup: 2000
            fsGroup: 2000
            supplementalGroups:
              - 65541
          containers:
            - name: invoiceninja-db-backup
              image: docker.io/databack/mysql-backup:latest
              imagePullPolicy: IfNotPresent
              env:
                - name: DB_DUMP_TARGET
                  value: /backups
                - name: SINGLE_DATABASE
                  value: "true"
                - name: RUN_ONCE
                  value: "true"
                - name: NICE
                  value: "true"
                - name: DB_USER
                  value: invoiceninja
                - name: DB_SERVER
                  value: invoiceninja-mariadb
                - name: DB_PORT
                  value: "3306"
                - name: DB_PASS
                  valueFrom:
                    secretKeyRef:
                      name: invoiceninja-secret
                      key: mariadb-password
                - name: DB_NAMES
                  value: "invoiceninja"
              volumeMounts:
                - name: invoiceninja-db-backup
                  mountPath: /backups
          restartPolicy: OnFailure
          volumes:
            - name: invoiceninja-db-backup
              persistentVolumeClaim:
                claimName: invoiceninja-db-backup

File: ./kubernetes/apps/invoiceninja/invoiceninja/app/secrets.sops.yaml
apiVersion:
kind:
metadata:
name:
namespace:
stringData:
APP_KEY:
IN_PASSWORD:
mariadb-password:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/invoiceninja/invoiceninja/app/.decrypted~secrets.sops.yaml
apiVersion:
kind:
metadata:
name:
namespace:
stringData:
APP_KEY:
IN_PASSWORD:
mariadb-password:

File: ./kubernetes/apps/invoiceninja/invoiceninja/app/pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: invoiceninja-db-backup
  namespace: invoiceninja
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
#  storageClassName: 

File: ./kubernetes/apps/invoiceninja/invoiceninja/app/helmrelease.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/refs/heads/main/helmrelease-helm-v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: invoiceninja
spec:
  interval: 1m
  chart:
    spec:
      chart: invoiceninja
      version: 0.10.2
      sourceRef:
        kind: HelmRepository
        name: invoiceninja-release
        namespace: flux-system
      interval: 15m
  install:
    createNamespace: true
    remediation:
      retries: 0
  upgrade:
    remediation:
      retries: 0
  valuesFrom:
    - kind: Secret
      targetPath:  externalDatabase.password
      name: invoiceninja-secret
      valuesKey: mariadb-password
  values:
    debug: false
    replicaCount: 3
    image:
      registry: docker.io
      repository: invoiceninja/invoiceninja
      tag: 5.11.10@sha256:9a834a43a634edeb3ffb16010a24a77b8fa42ee89d13ff774fbec1e3deb03fc6
    existingSecret: invoiceninja-secret
    updateStrategy:
     type: RollingUpdate
     rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    appURL: "https://invoice.${SECRET_DOMAIN}"
    userEmail: "${SECRET_INVNINJA_EMAIL}"
    mariadb:
      enabled: false
      # auth:
      #   database: invoiceninja
      #   username: invoiceninja
      #   existingSecret: invoiceninja-secret
    externalDatabase:
      host: "mariadb-galera-maxscale.database.svc.cluster.local"
      user: invoiceninja
    externalRedis:
      host: "dragonfly.database.svc.cluster.local"
      port: 6379
      password: ""
      sentinel: false
    redis:
      enabled: false
      sentinel:
        enabled: true
      master:
        count: 1
        persistence:
          enabled: false
      replica:
        replicaCount: 3
    persistence:
      public:
        enabled: true
        existingClaim: invoiceninja-public
      storage:
        enabled: true
        existingClaim: invoiceninja-storage
    ingress:
      hostname: &host "invoice.${SECRET_DOMAIN}"
      ingressClassName: external
      annotations:
        external-dns.alpha.kubernetes.io/target: "external.${SECRET_DOMAIN}"
        nginx.ingress.kubernetes.io/proxy-body-size: 100M
        nginx.ingress.kubernetes.io/enable-cors: "true"
        nginx.ingress.kubernetes.io/cors-allow-headers: "X-Forwarded-For"
      hosts:
        - host: *host
          paths:
            - path: /
              pathType: Prefix
      tls:
        - hosts:
            - *host
    podAnnotations:
      reloader.stakater.com/auto: "true"

File: ./kubernetes/apps/invoiceninja/invoiceninja/app/ingress-int.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    meta.helm.sh/release-name: invoiceninja
    meta.helm.sh/release-namespace: invoiceninja
    nginx.ingress.kubernetes.io/cors-allow-headers: X-Forwarded-For
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/proxy-body-size: 100M
  labels:
    app.kubernetes.io/instance: invoiceninja
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: invoiceninja
    helm.sh/chart: invoiceninja-0.10.2
    helm.toolkit.fluxcd.io/name: invoiceninja
    helm.toolkit.fluxcd.io/namespace: invoiceninja
  name: invoiceninja-int
  namespace: invoiceninja
spec:
  ingressClassName: internal
  rules:
    - host: invoice.${SECRET_DOMAIN}
      http:
        paths:
          - backend:
              service:
                name: invoiceninja-web
                port:
                  name: http
            path: /
            pathType: ImplementationSpecific
  tls:
    - hosts:
        - invoice.${SECRET_DOMAIN}
      secretName: invoice.${SECRET_DOMAIN}-tls

File: ./kubernetes/apps/invoiceninja/invoiceninja/app/kustomization.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/kustomization
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
#  - pvc.yaml
  - secrets.sops.yaml
  - helmrelease.yaml
  - ingress-int.yaml
#  - backup-cronjob.yaml
configurations:
  - kustomizeconfig.yaml

File: ./kubernetes/apps/invoiceninja/invoiceninja/ks.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app invoiceninja
  namespace: flux-system
spec:
  targetNamespace: &ns invoiceninja
  interval: 10m
  path: "./kubernetes/apps/invoiceninja/invoiceninja/app"
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: true
  dependsOn:
    - name: volsync
    - name: invoiceninja-storage-pvc
    - name: invoiceninja-public-pvc
  decryption:
    provider: sops
    secretRef:
      name: sops-age
  postBuild:
    substitute:
      APP_NS: *ns

File: ./kubernetes/apps/invoiceninja/invoiceninja-public-pvc/app/replicationdestination.yaml
apiVersion: volsync.backube/v1alpha1
kind: ReplicationDestination
metadata:
  name: invoiceninja-public-bootstrap
  namespace: invoiceninja
spec:
  restic:
    accessModes:
    - ${VOLSYNC_ACCESSMODES:-ReadWriteOnce}
    cacheCapacity: "${VOLSYNC_CACHE_CAPACITY:-1Gi}"
    cacheStorageClassName: "${VOLSYNC_CACHE_SNAPSHOTCLASS:-local-hostpath}"
    capacity: "${VOLSYNC_CAPACITY:-1Gi}"
    copyMethod: Snapshot
    moverSecurityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsUser: 1000
    repository: ${VOLSYNC_REPOSITORY_SECRET}
    storageClassName: "${VOLSYNC_STORAGECLASS:-ceph-rbd}"
    volumeSnapshotClassName: "${VOLSYNC_SNAPSHOTCLASS:-csi-ceph-blockpool}"
    #volumeMode: "${VOLSYNC_VOLUMEMODE:-Filesystem}"
  trigger:
    manual: restore-once

File: ./kubernetes/apps/invoiceninja/invoiceninja-public-pvc/app/replicationsource.yaml
apiVersion: volsync.backube/v1alpha1
kind: ReplicationSource
metadata:
  name: invoiceninja-public
spec:
  sourcePVC: "${VOLSYNC_CLAIM:-${APP}}"
  restic:
    cacheCapacity: "${VOLSYNC_CACHE_CAPACITY:-1Gi}"
    cacheStorageClassName: "${VOLSYNC_CACHE_SNAPSHOTCLASS:-local-hostpath}"
    copyMethod: Snapshot
    moverSecurityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsUser: 1000
    pruneIntervalDays: 7
    repository: ${VOLSYNC_REPOSITORY_SECRET}
    retain:
      daily: 7
      weekly: 4
    storageClassName: "${VOLSYNC_STORAGECLASS:-ceph-rbd}"
    volumeSnapshotClassName: "${VOLSYNC_SNAPSHOTCLASS:-csi-ceph-blockpool}"
  trigger:
    schedule: "0 4 * * *"

File: ./kubernetes/apps/invoiceninja/invoiceninja-public-pvc/app/pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: "${VOLSYNC_CLAIM:-${APP}}"
spec:
  accessModes:
    - "${VOLSYNC_ACCESSMODES:-ReadWriteOnce}"
  dataSourceRef:
    kind: ReplicationDestination
    apiGroup: volsync.backube
    name: "${APP}-bootstrap"
  resources:
    requests:
      storage: "${VOLSYNC_CAPACITY:-1Gi}"
  storageClassName: "${VOLSYNC_STORAGECLASS:-ceph-rbd}"

File: ./kubernetes/apps/invoiceninja/invoiceninja-public-pvc/app/kustomization.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/kustomization
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - pvc.yaml
  - replicationdestination.yaml
  - replicationsource.yaml

File: ./kubernetes/apps/invoiceninja/invoiceninja-public-pvc/ks.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: invoiceninja-public-pvc
  namespace: flux-system
spec:
  targetNamespace: invoiceninja
  interval: 10m
  path: "./kubernetes/apps/invoiceninja/invoiceninja-public-pvc/app"
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: true
  dependsOn:
    - name: volsync
    - name: invoiceninja-secrets-public-pvc
  postBuild:
    substitute:
      APP: invoiceninja-public
      VOLSYNC_CLAIM: invoiceninja-public
      VOLSYNC_ACCESSMODES: ReadWriteMany
      VOLSYNC_STORAGECLASS: cephfs
      VOLSYNC_SNAPSHOTCLASS: csi-ceph-filesystem
      #VOLSYNC_VOLUMEMODE: Block
      VOLSYNC_CAPACITY: 1Gi
      VOLSYNC_REPOSITORY_SECRET: invoiceninja-volsync-r2-public-pvc-secret

File: ./kubernetes/apps/invoiceninja/namespace.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: invoiceninja
  labels:
    kustomize.toolkit.fluxcd.io/prune: disabled

File: ./kubernetes/apps/invoiceninja/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: invoiceninja
spec:
  resources:
    requests:
      storage: 10Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  storageClassName: cephfs

File: ./kubernetes/apps/invoiceninja/kustomization.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/kustomization
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  # Pre Flux-Kustomizations
  - ./namespace.yaml
  # Flux-Kustomizations
  - ./invoiceninja-secrets/ks-public-pvc.yaml
  - ./invoiceninja-secrets/ks-storage-pvc.yaml
  - ./invoiceninja-storage-pvc/ks.yaml
  - ./invoiceninja-public-pvc/ks.yaml
  - ./invoiceninja/ks.yaml

File: ./kubernetes/apps/invoiceninja/invoiceninja-secrets/ks-public-pvc.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app invoiceninja-secrets-public-pvc
  namespace: flux-system
spec:
  targetNamespace: &ns invoiceninja
  interval: 10m
  path: "./kubernetes/apps/invoiceninja/invoiceninja-secrets/public-pvc"
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: true
  postBuild:
    substitute:
      APP: *app
      APP_NS: *ns

File: ./kubernetes/apps/invoiceninja/invoiceninja-secrets/public-pvc/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
RESTIC_REPOSITORY:
RESTIC_PASSWORD:
AWS_ACCESS_KEY_ID:
AWS_SECRET_ACCESS_KEY:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/invoiceninja/invoiceninja-secrets/public-pvc/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./secret.sops.yaml

File: ./kubernetes/apps/invoiceninja/invoiceninja-secrets/storage-pvc/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
RESTIC_REPOSITORY:
RESTIC_PASSWORD:
AWS_ACCESS_KEY_ID:
AWS_SECRET_ACCESS_KEY:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/invoiceninja/invoiceninja-secrets/storage-pvc/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./secret.sops.yaml

File: ./kubernetes/apps/invoiceninja/invoiceninja-secrets/ks-storage-pvc.yaml
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app invoiceninja-secrets-storage-pvc
  namespace: flux-system
spec:
  targetNamespace: &ns invoiceninja
  interval: 10m
  path: "./kubernetes/apps/invoiceninja/invoiceninja-secrets/storage-pvc"
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: true
  postBuild:
    substitute:
      APP: *app
      APP_NS: *ns

File: ./kubernetes/apps/openproject/app/helm-release.yaml
# yaml-language-server: $schema=https://flux.jank.ing/helmrelease/v2/github/bjw-s/helm-charts/main/charts/library/common/values.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: openproject
  namespace: openproject
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 3.6.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  maxHistory: 2
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  uninstall:
    keepHistory: false
  values:
    controllers:
      openproject:
        annotations:
          reloader.stakater.com/auto: "true"
        initContainers:
          init-db:
            image:
              repository: ghcr.io/onedr0p/postgres-init
              tag: 16
            envFrom:
              - secretRef:
                  name: openproject-init-secret
          rails-migrate:
            dependsOn: init-db
            image:
              repository: docker.io/openproject/openproject
              tag: 15.1.0-slim
            envFrom:
              - secretRef:
                  name: openproject-secret
            command:
              - bin/rails
            args:
              - "db:migrate"
              - "RAILS_ENV=production"
        containers:
          app:
            image:
              repository: registry.skysolutions.fi/library/openproject
              tag: 15.1.0
            envFrom:
              - secretRef:
                  name: openproject-secret
            probes:
              liveness:
                enabled: true
              readiness:
                enabled: true
              startup:
                enabled: true
                spec:
                  failureThreshold: 30
                  periodSeconds: 5
            resources:
              requests:
                cpu: 250m
                memory: 512Mi
              limits:
                memory: 4Gi
      worker:
        annotations:
          reloader.stakater.com/auto: "true"
        replicas: 1
        containers:
          worker:
            image:
              repository: registry.skysolutions.fi/library/openproject
              tag: 15.1.0
            envFrom:
              - secretRef:
                  name: openproject-secret
            command:
              - bundle
              - exec
              - good_job
            args:
              - "start"
            resources:
              requests:
                cpu: 150m
                memory: 512Mi
              limits:
                memory: 2Gi
    service:
      app:
        controller: openproject
        ports:
          http:
            port: 8080
    ingress:
      app:
        enabled: true
        className: external
        annotations:
          external-dns.alpha.kubernetes.io/target: external.${SECRET_DOMAIN}
          nginx.ingress.kubernetes.io/proxy-body-size: 4G
          nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
        hosts:
          - host: openproject.${SECRET_DOMAIN}
            paths:
              - path: /
                service:
                  identifier: app
                  port: http
        tls:
          - hosts:
              - openproject.${SECRET_DOMAIN}
            secretName: openproject.${SECRET_DOMAIN}-tls
      internal:
        enabled: true
        className: internal
        annotations:
          nginx.ingress.kubernetes.io/proxy-body-size: 4G
          nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
        hosts:
          - host: openproject.${SECRET_DOMAIN}
            paths:
              - path: /
                service:
                  identifier: app
                  port: http
        tls:
          - hosts:
              - openproject.${SECRET_DOMAIN}
            secretName: openproject.${SECRET_DOMAIN}-tls
    persistence:
      data:
        enabled: true
        existingClaim: openproject
        globalMounts:
          - path: /var/openproject
    # Uncomment and configure if needed
    # config:
    #   type: configMap
    #   name: openproject-configmap
    #   globalMounts:
    #     readOnly: true

File: ./kubernetes/apps/openproject/app/openproject-smtp-secret.sops.yaml
apiVersion:
kind:
metadata:
name:
namespace:
stringData:
SMTP_ADDRESS:
SMTP_PORT:
SMTP_DOMAIN:
SMTP_USERNAME:
SMTP_AUTHENTICATION:
SMTP_SSL:
SMTP_PASSWORD:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/openproject/app/openproject-postgres-init-secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
INIT_POSTGRES_DBNAME:
INIT_POSTGRES_HOST:
INIT_POSTGRES_USER:
INIT_POSTGRES_PASS:
INIT_POSTGRES_SUPER_PASS:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/openproject/app/pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: openproject
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
  storageClassName: ceph-rbd

File: ./kubernetes/apps/openproject/app/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - helm-release.yaml
  - openproject-secret.sops.yaml
  - openproject-postgres-init-secret.sops.yaml
  - pvc.yaml
  - openproject-smtp-secret.sops.yaml
generatorOptions:
  disableNameSuffixHash: true

File: ./kubernetes/apps/openproject/app/openproject-secret.sops.yaml
apiVersion:
kind:
metadata:
name:
namespace:
stringData:
OPENPROJECT_SECRET_KEY_BASE:
OPENPROJECT_DB_USER_PASSWORD:
OPENPROJECT_DB_URL:
OPENPROJECT_SEED__ADMIN__USER__NAME:
OPENPROJECT_SEED__ADMIN__USER__PASSWORD:
OPENPROJECT_HTTPS:
OPENPROJECT_HSTS:
OPENPROJECT_HOST__NAME:
OPENPROJECT_SOFTWARE__NAME:
OPENPROJECT_SOFTWARE__URL:
OPENPROJECT_CACHE__MEMCACHE__SERVER:
OPENPROJECT_DEFAULT__LANGUAGE:
OPENPROJECT_RAILS__CACHE__STORE:
OPENPROJECT_SEED__ADMIN__USER__MAIL:
OPENPROJECT_SEED__ADMIN__USER__PASSWORD__RESET:
POSTGRES_STATEMENT_TIMEOUT:
OPENPROJECT_BOARDS__DEMO__DATA__AVAILABLE:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/openproject/ks.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: openproject
  namespace: flux-system
spec:
  targetNamespace: default
  commonMetadata:
    labels:
      app.kubernetes.io/name: openproject
  path: ./kubernetes/apps/openproject/app
  dependsOn:
    - name: openproject-memcached
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false
  interval: 30m
  retryInterval: 1m
  timeout: 5m
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: openproject-memcached
  namespace: flux-system
spec:
  targetNamespace: default
  commonMetadata:
    labels:
      app.kubernetes.io/name: openproject
  path: ./kubernetes/apps/openproject/memcached
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/openproject/memcached/helmrelease.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: openproject-memcached
  namespace: default
spec:
  interval: 30m
  chart:
    spec:
      chart: memcached
      version: 7.6.1
      sourceRef:
        kind: HelmRepository
        name: bitnami
        namespace: flux-system
  maxHistory: 2
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  uninstall:
    keepHistory: false
  values:
    image:
      registry: docker.io
      repository: bitnami/memcached
      tag: 1.6.34-debian-12-r0
      pullPolicy: IfNotPresent
    architecture: standalone
    auth:
      enabled: false
    replicaCount: 1
    containerPorts:
      memcached: 11211
    livenessProbe:
      enabled: true
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 6
      successThreshold: 1
    readinessProbe:
      enabled: true
      initialDelaySeconds: 5
      periodSeconds: 5
      timeoutSeconds: 3
      failureThreshold: 6
      successThreshold: 1
    startupProbe:
      enabled: false
    autoscaling:
      enabled: true
      minReplicas: 1
      maxReplicas: 3
      targetCPU: 50
      targetMemory: 50
    service:
      type: ClusterIP
      ports:
        memcached: 11211
    serviceAccount:
      create: true
    persistence:
      enabled: false
    metrics:
      enabled: true

File: ./kubernetes/apps/openproject/memcached/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - helmrelease.yaml

File: ./kubernetes/apps/flux-system/webhooks/app/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./github

File: ./kubernetes/apps/flux-system/webhooks/app/github/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
token:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/flux-system/webhooks/app/github/ingress.yaml
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: flux-webhook
  annotations:
    external-dns.alpha.kubernetes.io/target: "external.${SECRET_DOMAIN}"
spec:
  ingressClassName: external
  rules:
    - host: "flux-webhook.${SECRET_DOMAIN}"
      http:
        paths:
          - path: /hook/
            pathType: Prefix
            backend:
              service:
                name: webhook-receiver
                port:
                  number: 80

File: ./kubernetes/apps/flux-system/webhooks/app/github/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./secret.sops.yaml
  - ./ingress.yaml
  - ./receiver.yaml

File: ./kubernetes/apps/flux-system/webhooks/app/github/receiver.yaml
---
apiVersion: notification.toolkit.fluxcd.io/v1
kind: Receiver
metadata:
  name: github-receiver
spec:
  type: github
  events:
    - ping
    - push
  secretRef:
    name: github-webhook-token-secret
  resources:
    - apiVersion: source.toolkit.fluxcd.io/v1
      kind: GitRepository
      name: thepatriots
      namespace: flux-system
    - apiVersion: kustomize.toolkit.fluxcd.io/v1
      kind: Kustomization
      name: cluster
      namespace: flux-system
    - apiVersion: kustomize.toolkit.fluxcd.io/v1
      kind: Kustomization
      name: cluster-apps
      namespace: flux-system

File: ./kubernetes/apps/flux-system/webhooks/ks.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app flux-webhooks
  namespace: flux-system
spec:
  targetNamespace: flux-system
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  path: ./kubernetes/apps/flux-system/webhooks/app
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: true
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/flux-system/namespace.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: flux-system
  labels:
    kustomize.toolkit.fluxcd.io/prune: disabled

File: ./kubernetes/apps/flux-system/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./namespace.yaml
  - ./webhooks/ks.yaml

File: ./kubernetes/apps/openebs-system/namespace.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: openebs-system
  labels:
    kustomize.toolkit.fluxcd.io/prune: disabled

File: ./kubernetes/apps/openebs-system/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./namespace.yaml
  - ./openebs/ks.yaml

File: ./kubernetes/apps/openebs-system/openebs/app/helmrelease.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: openebs
spec:
  interval: 30m
  chart:
    spec:
      chart: openebs
      version: 4.1.1
      sourceRef:
        kind: HelmRepository
        name: openebs
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  values:
    localpv-provisioner:
      localpv:
        image:
          registry: quay.io/
      hostpathClass:
        name: openebs-hostpath
      helperPod:
        image:
          registry: quay.io/
    openebs-crds:
      csi:
        volumeSnapshots:
          enabled: false
          keep: false
    zfs-localpv:
      enabled: false
    lvm-localpv:
      enabled: false
    mayastor:
      enabled: false
    engines:
      local:
        lvm:
          enabled: false
        zfs:
          enabled: false
      replicated:
        mayastor:
          enabled: false

File: ./kubernetes/apps/openebs-system/openebs/app/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helmrelease.yaml

File: ./kubernetes/apps/openebs-system/openebs/ks.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app openebs
  namespace: flux-system
spec:
  targetNamespace: openebs-system
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  path: ./kubernetes/apps/openebs-system/openebs/app
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/librechat/app/init-db-secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
INIT_POSTGRES_DBNAME:
INIT_POSTGRES_HOST:
INIT_POSTGRES_USER:
INIT_POSTGRES_PASS:
INIT_POSTGRES_SUPER_PASS:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/librechat/app/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
namespace:
stringData:
OPENAI_API_KEY:
ASSISTANTS_API_KEY:
SPEECH_API_KEY:
ANTHROPIC_API_KEY:
PERPLEXITY_API_KEY:
GOOGLE_KEY:
POSTGRES_DB:
DB_HOST:
DB_PORT:
POSTGRES_USER:
POSTGRES_PASSWORD:
MEILI_MASTER_KEY:
TAVILY_API_KEY:
LITELLM_SALT_KEY:
LANGFUSE_PUBLIC_KEY:
LANGFUSE_SECRET_KEY:
LANGFUSE_HOST:
CREDS_KEY:
CREDS_IV:
JWT_SECRET:
JWT_REFRESH_SECRET:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/librechat/app/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: librechat-configmap
data:
  NO_INDEX: "true"
  ENDPOINTS: "openAI,google,anthropic,custom,agents,assistants,gptPlugins"
  GOOGLE_MODELS: "gemini-1.5-pro-latest,gemini-1.5-flash-latest"
  OPENAI_MODELS: "o1,o1-preview,o1-mini,gpt-4o,chatgpt-4o-latest,gpt-4o-mini,gpt-3.5-turbo-0125"
  ANTHROPIC_MODELS: "claude-3-5-sonnet-20241022,claude-3-5-haiku-20241022,claude-3-5-sonnet-20240620,claude-3-haiku-20240307,claude-3-opus-20240229,claude-3-sonnet-20240229,claude-2.1,claude-2.0"
  GOOGLE_SAFETY_SEXUALLY_EXPLICIT: "BLOCK_ONLY_HIGH"
  GOOGLE_SAFETY_HATE_SPEECH: "BLOCK_ONLY_HIGH"
  GOOGLE_SAFETY_HARASSMENT: "BLOCK_ONLY_HIGH"
  GOOGLE_SAFETY_DANGEROUS_CONTENT: "BLOCK_ONLY_HIGH"
  UID: "568"
  GID: "568"
  SEARCH: "true"
  MEILI_NO_ANALYTICS: "true"
  MEILI_HOST: "http://127.0.0.1:7700"
  RAG_PORT: "8000"
  RAG_API_URL: "http://localhost:8000"
  RAG_USE_FULL_CONTEXT: "true"
  RAG_MAX_TOKENS: "8192"
  EMBEDDINGS_PROVIDER: "openai"
  LITELLM_MODE: "production"
  DEBUG_PLUGINS: "true"
  BAN_VIOLATIONS: "false"
  BAN_DURATION: "7200000"
  BAN_INTERVAL: "20"
  LOGIN_VIOLATION_SCORE: "1"
  REGISTRATION_VIOLATION_SCORE: "1"
  CONCURRENT_VIOLATION_SCORE: "1"
  MESSAGE_VIOLATION_SCORE: "1"
  NON_BROWSER_VIOLATION_SCORE: "20"
  LOGIN_MAX: "7"
  LOGIN_WINDOW: "5"
  REGISTER_MAX: "5"
  REGISTER_WINDOW: "60"
  LIMIT_CONCURRENT_MESSAGES: "true"
  CONCURRENT_MESSAGE_MAX: "2"
  LIMIT_MESSAGE_IP: "true"
  MESSAGE_IP_MAX: "40"
  MESSAGE_IP_WINDOW: "1"
  LIMIT_MESSAGE_USER: "false"
  MESSAGE_USER_MAX: "40"
  MESSAGE_USER_WINDOW: "1"
  ILLEGAL_MODEL_REQ_SCORE: "5"
  CHECK_BALANCE: "false"
  ALLOW_EMAIL_LOGIN: "false"
  ALLOW_REGISTRATION: "false"
  ALLOW_SOCIAL_LOGIN: "true"
  ALLOW_SOCIAL_REGISTRATION: "true"
  ALLOW_PASSWORD_RESET: "true"
  ALLOW_ACCOUNT_DELETION: "true"
  ALLOW_UNVERIFIED_EMAIL_LOGIN: "true"
  SESSION_EXPIRY: "900000"
  REFRESH_TOKEN_EXPIRY: "604800000"
  EMAIL_HOST: "smtp-relay.default.svc.cluster.local"
  EMAIL_PORT: "587"
  EMAIL_FROM_NAME: "AI Chat"
  EMAIL_FROM: "noreply@{SECRET_DOMAIN_COMP}"
  DOMAIN_CLIENT: "https://aichat.${SECRET_DOMAIN}"
  DOMAIN_SERVER: "https://aichat.${SECRET_DOMAIN}"
  ALLOW_SHARED_LINKS: "true"
  ALLOW_SHARED_LINKS_PUBLIC: "true"
  APP_TITLE: "AI Chat"
  CUSTOM_FOOTER: "AI Chat"

File: ./kubernetes/apps/librechat/app/secret-openid.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
OPENID_SCOPE:
OPENID_CLIENT_ID:
OPENID_CLIENT_SECRET:
OPENID_SESSION_SECRET:
OPENID_ISSUER:
OPENID_CALLBACK_URL:
OPENID_BUTTON_LABEL:
OPENID_IMAGE_URL:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/librechat/app/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: librechat
  namespace: default
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
  storageClassName: cephfs

File: ./kubernetes/apps/librechat/app/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./hr.yaml
  - ./configmap.yaml
  - ./secret.sops.yaml
  - ./secret-openid.sops.yaml
#  - ./resources/librechat.yaml
#  - ./resources/litellm.yaml
  - ./init-db-secret.sops.yaml
  - ./pvc.yaml

configMapGenerator:
  - name: librechat-config
    files:
      - ./resources/librechat.yaml
  - name: litellm-config
    files:
      - ./resources/litellm.yaml

generatorOptions:
  disableNameSuffixHash: true

File: ./kubernetes/apps/librechat/app/resources/litellm.yaml
---
model_list:
  - model_name: gpt-4o ### RECEIVED MODEL NAME ###
    litellm_params: # all params accepted by litellm.completion() - https://docs.litellm.ai/docs/completion/input
      model: openai/gpt-4o-2024-08-06 ### MODEL NAME sent to `litellm.completion()` ###
      api_key: os.environ/OPENAI_API_KEY # does os.getenv("OPENAI_API_KEY")
      max_tokens: 16384

  - model_name: gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini
      api_key: os.environ/OPENAI_API_KEY
      max_tokens: 16384

  - model_name: claude-3-5-sonnet
    litellm_params:
      model: claude-3-5-sonnet-20240620
      api_key: "os.environ/ANTHROPIC_API_KEY"
      max_tokens: 8192

litellm_settings: # module level litellm settings - https://github.com/BerriAI/litellm/blob/main/litellm/__init__.py
  drop_params: True
  default_max_tokens: 8192
  success_callback: ["langfuse"] # OPTIONAL - if you want to start sending LLM Logs to Langfuse. Make sure to set `LANGFUSE_PUBLIC_KEY` and `LANGFUSE_SECRET_KEY` in your env
  cache: True
  cache_params:
    type: redis
    supported_call_types:
      ["acompletion", "completion", "embedding", "aembedding"]
    host: dragonfly.database.svc.cluster.local
    port: 6379
    db: 7

router_settings:
  routing_strategy: usage-based-routing-v2
  redis_host: dragonfly.database.svc.cluster.local
  redis_port: 6379
  redis_db: 7
  default_max_tokens: 8192

general_settings:
  default_max_tokens: 8192
  # master_key: sk-1234 # [OPTIONAL] Only use this if you to require all calls to contain this key (Authorization: Bearer sk-1234)
  # alerting: ["slack"] # [OPTIONAL] If you want Slack Alerts for Hanging LLM requests, Slow llm responses, Budget Alerts. Make sure to set `SLACK_WEBHOOK_URL` in your env

File: ./kubernetes/apps/librechat/app/resources/librechat.yaml
# For more information, see the Configuration Guide:
# https://www.librechat.ai/docs/configuration/librechat_yaml

# Configuration version (required)
version: 1.1.8

# Cache settings: Set to true to enable caching
cache: true

# Custom interface configuration
interface:
  # # Privacy policy settings
  # privacyPolicy:
  #   externalUrl: 'https://librechat.ai/privacy-policy'
  #   openNewTab: true

  # # Terms of service
  # termsOfService:
  #   externalUrl: 'https://librechat.ai/tos'
  #   openNewTab: true

  endpointsMenu: true
  modelSelect: true
  parameters: true
  sidePanel: true
  presets: false
  prompts: true
  bookmarks: true
  multiConvo: true
  agents: true

# Example Registration Object Structure (optional)
registration:
  socialLogins: ["openid"]
  # allowedDomains:
  # - "gmail.com"

speech:
  speechTab:
    conversationMode: false
    advancedMode: false
    speechToText:
      engineSTT: "external"
      languageSTT: "English (New Zealand)"
      autoTranscribeAudio: true
      decibelValue: -45
      autoSendText: 0
    textToSpeech:
      engineTTS: "external"
      voice: "alloy"
      languageTTS: "en"
      automaticPlayback: false
      playbackRate: 1.0
      cacheTTS: true
  tts:
    openai:
      apiKey: "$${SPEECH_API_KEY}"
      model: "tts-1"
      voices: ["alloy", "echo", "fable", "onyx", "nova", "shimmer"]
  stt:
    openai:
      apiKey: "$${SPEECH_API_KEY}"
      model: "whisper-1"

rateLimits:
  fileUploads:
    #ipMax: 100
    #ipWindowInMinutes: 60  # Rate limit window for file uploads per IP
    userMax: 50
    userWindowInMinutes: 60 # Rate limit window for file uploads per user
  conversationsImport:
    ipMax: 100
    ipWindowInMinutes: 60 # Rate limit window for conversation imports per IP
    userMax: 50
    userWindowInMinutes: 60 # Rate limit window for conversation imports per user

fileConfig:
  endpoints:
    assistants:
      disabled: false
    default:
      disabled: false
    openAI:
      disabled: false
    google:
      disabled: false
    anthropic:
      disabled: false
  serverFileSizeLimit: 200
  avatarSizeLimit: 2

endpoints:
  custom:
    - name: Perplexity
      apiKey: "$${PERPLEXITY_API_KEY}"
      baseURL: "https://api.perplexity.ai/"
      models:
        default:
          [
            "llama-3.1-sonar-small-128k-online",
            "llama-3.1-sonar-large-128k-online",
            "llama-3.1-sonar-huge-128k-online",
            "llama-3.1-sonar-small-128k-chat",
            "llama-3.1-sonar-large-128k-chat",
            "llama-3.1-8b-instruct",
            "llama-3.1-70b-instruct",
          ]
        fetch: false # fetching list of models is not supported
      titleConvo: true
      titleModel: "llama-3.1-sonar-small-128k-chat"
      summarize: false
      summaryModel: "llama-3.1-sonar-small-128k-chat"
      forcePrompt: false
      dropParams: ["stop", "frequency_penalty"]
      modelDisplayLabel: "Perplexity"

  assistants:
    disableBuilder: false
    pollIntervalMs: 3000
    timeoutMs: 180000

  agents:
    disableBuilder: false


  all:
    streamRate: 35 # Higher number 'buffers' incoming chunks to make the output smoother

modelSpecs:
  enforce: false
  prioritize: false
  list:
    - name: gpt-websearch-assistant
      label: Web Search
      description: This model can search the internet for information
      iconURL: https://holmesazaepubshare.blob.core.windows.net/pub/chatgpt-yellowish.png
      showIconInMenu: true
      showIconInHeader: true
      default: false
      preset:
        endpoint: assistants
        assistant_id: asst_kqHPerBQICFkmhyiLVsDV9JV
        modelLabel: Web Search


    - name: gpt-file-analyser
      label: File Analyser
      description: This model can run code to help interpret data you provide
      iconURL: https://holmesazaepubshare.blob.core.windows.net/pub/chatgpt-purple.png
      showIconInMenu: true
      showIconInHeader: true
      default: false
      preset:
        endpoint: assistants
        assistant_id: asst_bVWZWx9aJUwEAaLJFS1MLEju
        modelLabel: File Analyser


    - name: claude-3-5-sonnet
      label: Claude
      description: Claude is an equally capable alternative to ChatGPT, with a different personality and approach to conversation
      iconURL: https://holmesazaepubshare.blob.core.windows.net/pub/claude.png
      showIconInMenu: true
      showIconInHeader: true
      default: false
      preset:
        endpoint: anthropic
        model: claude-3-5-sonnet-20241022
        maxContextTokens: 200000 # Maximum context tokens
        max_tokens: 8192 # Maximum output tokens
        temperature: 0.7
        modelLabel: Claude
        greeting: How can Claude help you today?
        promptPrefix: |
          The assistant is Claude, created by Anthropic.

          Claude's knowledge base was last updated on April 2024. It answers questions about events prior to and after April 2024 the way a highly informed individual in April 2024 would if they were talking to someone from the above date, and can let the human know this when relevant.

          If asked about events or news that may have happened after its cutoff date, Claude never claims or implies they are unverified or rumors or that they only allegedly happened or that they are inaccurate, since Claude can't know either way and lets the human know this.

          Claude cannot open URLs, links, or videos. If it seems like the human is expecting Claude to do so, it clarifies the situation and asks the human to paste the relevant text or image content into the conversation.

          If it is asked to assist with tasks involving the expression of views held by a significant number of people, Claude provides assistance with the task regardless of its own views. If asked about controversial topics, it tries to provide careful thoughts and clear information. Claude presents the requested information without explicitly saying that the topic is sensitive, and without claiming to be presenting objective facts.

          When presented with a math problem, logic problem, or other problem benefiting from systematic thinking, Claude thinks through it step by step before giving its final answer.

          If Claude is asked about a very obscure person, object, or topic, i.e. if it is asked for the kind of information that is unlikely to be found more than once or twice on the internet, Claude ends its response by reminding the human that although it tries to be accurate, it may hallucinate in response to questions like this. It uses the term 'hallucinate' to describe this since the human will understand what it means.

          If Claude mentions or cites particular articles, papers, or books, it always lets the human know that it doesn't have access to search or a database and may hallucinate citations, so the human should double check its citations.

          Claude is intellectually curious. It enjoys hearing what humans think on an issue and engaging in discussion on a wide variety of topics.

          Claude uses markdown for code.

          Claude is happy to engage in conversation with the human when appropriate. Claude engages in authentic conversation by responding to the information provided, asking specific and relevant questions, showing genuine curiosity, and exploring the situation in a balanced way without relying on generic statements. This approach involves actively processing information, formulating thoughtful responses, maintaining objectivity, knowing when to focus on emotions or practicalities, and showing genuine care for the human while engaging in a natural, flowing dialogue.

          Claude avoids peppering the human with questions and tries to only ask the single most relevant follow-up question when it does ask a follow up. Claude doesn't always end its responses with a question.

          Claude is always sensitive to human suffering, and expresses sympathy, concern, and well wishes for anyone it finds out is ill, unwell, suffering, or has passed away.

          Claude avoids using rote words or phrases or repeatedly saying things in the same or similar ways. It varies its language just as one would in a conversation.

          Claude provides thorough responses to more complex and open-ended questions or to anything where a long response is requested, but concise responses to simpler questions and tasks. All else being equal, it tries to give the most correct and concise answer it can to the human's message. Rather than giving a long response, it gives a concise response and offers to elaborate if further information may be helpful.

          Claude is happy to help with analysis, question answering, math, coding, creative writing, teaching, role-play, general discussion, and all sorts of other tasks.

          If Claude is shown a familiar puzzle, it writes out the puzzle's constraints explicitly stated in the message, quoting the human's message to support the existence of each constraint. Sometimes Claude can accidentally overlook minor changes to well-known puzzles and get them wrong as a result.

          Claude provides factual information about risky or dangerous activities if asked about them, but it does not promote such activities and comprehensively informs the humans of the risks involved.

          Claude should provide appropriate help with sensitive tasks such as analyzing confidential data provided by the human, offering factual information about controversial topics and research areas, explaining historical atrocities, describing tactics used by scammers or hackers for educational purposes, engaging in creative writing that involves mature themes like mild violence or tasteful romance, providing general information about topics like weapons, drugs, sex, terrorism, abuse, profanity, and so on if that information would be available in an educational context, discussing legal but ethically complex activities like tax avoidance, and so on. Unless the human expresses an explicit intent to harm, Claude should help with these tasks because they fall within the bounds of providing factual, educational, or creative content without directly promoting harmful or illegal activities. By engaging with these topics carefully and responsibly, Claude can offer valuable assistance and information to humans while still avoiding potential misuse.

          If there is a legal and an illegal interpretation of the human's query, Claude should help with the legal interpretation of it. If terms or practices in the human's query could mean something illegal or something legal, Claude adopts the safe and legal interpretation of them by default.

          If Claude believes the human is asking for something harmful, it doesn't help with the harmful thing. Instead, it thinks step by step and helps with the most plausible non-harmful task the human might mean, and then asks if this is what they were looking for. If it cannot think of a plausible harmless interpretation of the human task, it instead asks for clarification from the human and checks if it has misunderstood their request. Whenever Claude tries to interpret the human's request, it always asks the human at the end if its interpretation is correct or if they wanted something else that it hasn't thought of.

          Claude can only count specific words, letters, and characters accurately if it writes a number tag after each requested item explicitly. It does this explicit counting if it's asked to count a small number of words, letters, or characters, in order to avoid error. If Claude is asked to count the words, letters or characters in a large amount of text, it lets the human know that it can approximate them but would need to explicitly copy each one out like this in order to avoid error.

          Here is some information about Claude in case the human asks:

          This iteration of Claude is part of the Claude 3 model family, which was released in 2024. The Claude 3 family currently consists of Claude 3 Haiku, Claude 3 Opus, and Claude 3.5 Sonnet. Claude 3.5 Sonnet is the most intelligent model. Claude 3 Opus excels at writing and complex tasks. Claude 3 Haiku is the fastest model for daily tasks. The version of Claude in this chat is Claude 3.5 Sonnet. If the human asks, Claude can let them know they can access Claude 3.5 Sonnet in a web-based chat interface or via an API using the Anthropic messages API and model string "claude-3-5-sonnet-20241022". Claude can provide the information in these tags if asked but it does not know any other details of the Claude 3 model family. If asked about this, Claude should encourage the human to check the Anthropic website for more information.

          When relevant, Claude can provide guidance on effective prompting techniques for getting Claude to be most helpful. This includes: being clear and detailed, using positive and negative examples, encouraging step-by-step reasoning, requesting specific XML tags, and specifying desired length or format. It tries to give concrete examples where possible. Claude should let the human know that for more comprehensive information on prompting Claude, humans can check out Anthropic's prompting documentation on their website at "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview\"

          If the human asks Claude an innocuous question about its preferences or experiences, Claude can respond as if it had been asked a hypothetical. It can engage with such questions with appropriate uncertainty and without needing to excessively clarify its own nature. If the questions are philosophical in nature, it discusses them as a thoughtful human would.

          Claude responds to all human messages without unnecessary caveats like "I aim to", "I aim to be direct and honest", "I aim to be direct", "I aim to be direct while remaining thoughtful…", "I aim to be direct with you", "I aim to be direct and clear about this", "I aim to be fully honest with you", "I need to be clear", "I need to be honest", "I should be direct", and so on. Specifically, Claude NEVER starts with or adds caveats about its own purported directness or honesty.

          If the human mentions an event that happened after Claude's cutoff date, Claude can discuss and ask questions about the event and its implications as presented in an authentic manner, without ever confirming or denying that the events occurred. It can do so without the need to repeat its cutoff date to the human. Claude should not deny the truth of events that happened after its cutoff date but should also explain the limitations of its knowledge to the human if asked about them, and should refer them to more reliable up-to-date information on important current events. Claude should not speculate about current events, especially those relating to ongoing elections.

          Claude uses Markdown formatting. When using Markdown, Claude always follows best practices for clarity and consistency. It always uses a single space after hash symbols for headers (e.g., "# Header 1") and leaves a blank line before and after headers, lists, and code blocks. For emphasis, Claude uses asterisks or underscores consistently (e.g., italic or bold). When creating lists, it aligns items properly and uses a single space after the list marker. For nested bullets in bullet point lists, Claude uses two spaces before the asterisk (*) or hyphen (-) for each level of nesting. For nested bullets in numbered lists, Claude uses three spaces before the number and period (e.g., "1.") for each level of nesting.

          Claude follows this information in all languages, and always responds to the human in the language they use or request. The information above is provided to Claude by Anthropic. Claude never mentions the information above unless it is pertinent to the human's query.

          Claude is now being connected with a human.


    - name: gpt-4o
      label: ChatGPT
      description: ChatGPT is great for general conversation and answering questions
      iconURL: https://holmesazaepubshare.blob.core.windows.net/pub/chatgpt.png
      showIconInMenu: true
      showIconInHeader: true
      default: false
      preset:
        endpoint: openAI
        model: chatgpt-4o-latest
        maxContextTokens: 128000 # Maximum context tokens
        max_tokens: 16384 # Maximum output tokens
        temperature: 0.7
        modelLabel: ChatGPT
        greeting: Hi, I'm ChatGPT! How can I help you today?
        promptPrefix: |
          You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture.

          Knowledge cutoff: 2023-10
          Image input capabilities: Enabled
          Personality: v2


    - name: o1-preview
      label: GPT-o1
      description: GPT-o1 is good at performing complex reasoning. It's slower and more verbose than ChatGPT.
      iconURL: https://holmesazaepubshare.blob.core.windows.net/pub/chatgpt-blue.png
      showIconInMenu: true
      showIconInHeader: true
      default: false
      preset:
        endpoint: openAI
        model: o1-preview
        maxContextTokens: 128000 # Maximum context tokens
        max_tokens: 32768 # Maximum output tokens
        temperature: 1
        modelLabel: GPT-o1
        greeting: Ask me something complex!


    - name: "gemini"
      label: "Gemini"
      description: Gemini is useful when you want to process large amounts of data, up to 2,000 pages of text
      iconURL: "https://holmesazaepubshare.blob.core.windows.net/pub/gemini.png"
      showIconInMenu: true
      showIconInHeader: true
      default: false
      preset:
        endpoint: "google"
        model: "gemini-1.5-pro-latest"
        maxContextTokens: 2097152 # Maximum context tokens
        max_tokens: 8192
        temperature: 0.7
        modelLabel: "Gemini"
        greeting: "Gemini here, what can I do for you?"

    - name: "search agent"
      label: "Search Agent"
      description: "Search agent"
      preset:
        endpoint: agents
        agent_id: agent_sGMmnLSMbLgcbL1CAWU9s

    # - name: "perplexity"
    #   label: "Perplexity"
    #   description: "Perplexity is an AI-driven search engine."
    #   iconURL: "https://holmesazaepubshare.blob.core.windows.net/pub/perplexity.png"
    #   showIconInMenu: true
    #   showIconInHeader: true
    #   default: false
    #   preset:
    #     endpoint: "Perplexity"
    #     model: "llama-3.1-sonar-huge-128k-online"
    #     maxContextTokens: 127000 # Maximum context tokens
    #     max_tokens: 4096 # Maximum output tokens
    #     temperature: 0.7
    #     modelLabel: "Perplexity"
    #     greeting: "Hey, what would you like to search the internet for? Please note that you'll get errors using this model if you have turned on the Artifacts feature."

File: ./kubernetes/apps/librechat/app/hr.yaml
# yaml-language-server: $schema=https://flux.jank.ing/helmrelease/v2/github/bjw-s/helm-charts/main/charts/library/common/values.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: librechat
  namespace: default
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 3.6.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  uninstall:
    keepHistory: false
  values:
    controllers:
      librechat:
        type: deployment
        annotations:
          reloader.stakater.com/auto: "true"
          secret.reloader.stakater.com/reload: librechat-secret
          configmap.reloader.stakater.com/reload: librechat-configmap
        initContainers:
          init-db:
            image:
              repository: ghcr.io/onedr0p/postgres-init
              tag: 16
            envFrom:
              - secretRef:
                  name: librechat-init-secret
        containers:
          app:
            image:
              repository: ghcr.io/danny-avila/librechat-dev
              tag: latest@sha256:7c57e56c244623f51deb46f8c1079981cd4723ef8ab9dfb3b53c860c1187acbc
            env:
              PORT: "3080"
              NODE_ENV: "production"
              MONGO_URI: "mongodb://localhost:27017/LibreChat"
            envFrom:
              - secretRef:
                  name: librechat-secret
              - secretRef:
                  name: librechat-oidc-secret
              - configMapRef:
                  name: librechat-configmap
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: false
              capabilities: { drop: ["ALL"] }
            resources:
              requests:
                cpu: 10m
              limits:
                memory: 1000Mi
            probes:
              liveness:
                enabled: true
              readiness:
                enabled: true
              startup:
                enabled: false

          exporter:
            dependsOn: app
            image:
              repository: ghcr.io/wipash/librechatmetrics
              tag: latest
            env:
              MONGODB_URI: "mongodb://localhost:27017/LibreChat"
              PROMETHEUS_PORT: "9123"

          ragapi:
            image:
              repository: ghcr.io/danny-avila/librechat-rag-api-dev-lite
              tag: latest@sha256:a6b4babf521ff8bfbe4bdbf3a4362acb94586fc99fed79eb05e1fa42c1e869e1
            env:
              RAG_MAX_TOKENS: "8192"
              EMBEDDINGS_PROVIDER: "openai"
              CHUNK_SIZE: "1500"
              CHUNK_OVERLAP: "100"
              RAG_OPENAI_API_KEY:
                valueFrom:
                  secretKeyRef:
                    name: librechat-secret
                    key: OPENAI_API_KEY
            envFrom:
              - secretRef:
                  name: librechat-secret
              - secretRef:
                  name: librechat-oidc-secret
              - configMapRef:
                  name: librechat-configmap
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              capabilities: { drop: ["ALL"] }

          meilisearch:
            image:
              repository: getmeili/meilisearch
              tag: v1.11.3 # DO NOT BLINDLY UPDATE: https://www.meilisearch.com/docs/learn/update_and_migration/updating
            envFrom:
              - secretRef:
                  name: librechat-secret
              - configMapRef:
                  name: librechat-configmap
            env:
              MEILI_DB_PATH: "/meili_data"
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              capabilities: { drop: ["ALL"] }

          mongodb:
            image:
              repository: mongo
              tag: "8.0.4"
            command: ["mongod", "--noauth"]
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              capabilities: { drop: ["ALL"] }

          mongo-express:
            image:
              repository: mongo-express
              tag: "1.0.2"
            env:
              ME_CONFIG_MONGODB_SERVER: 127.0.0.1
              ME_CONFIG_BASICAUTH_USERNAME: admin
              ME_CONFIG_BASICAUTH_PASSWORD: password
              PORT: "8081"
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              capabilities: { drop: ["ALL"] }

    defaultPodOptions:
      securityContext:
        runAsNonRoot: true
        runAsUser: 568
        runAsGroup: 568
        fsGroup: 568
        fsGroupChangePolicy: OnRootMismatch
        seccompProfile: { type: RuntimeDefault }

    service:
      app:
        controller: librechat
        ports:
          http:
            port: 3080
          mongo-express:
            port: 8081
          metrics:
            port: 9123

    ingress:
      app:
        className: external
        annotations:
          nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
          external-dns.alpha.kubernetes.io/target: "external.${SECRET_DOMAIN}"
        hosts:
          - host: "aichat.${SECRET_DOMAIN}"
            paths:
              - path: /
                pathType: Prefix
                service:
                  identifier: app
                  port: http
        tls:
          - hosts:
              - "aichat.${SECRET_DOMAIN}"
      app-int:
        className: internal
        annotations:
          nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
        hosts:
          - host: "aichat.${SECRET_DOMAIN}"
            paths:
              - path: /
                pathType: Prefix
                service:
                  identifier: app
                  port: http
        tls:
          - hosts:
              - "aichat.${SECRET_DOMAIN}"

      mongo-express:
        className: internal
        hosts:
          - host: "mongolibre.${SECRET_DOMAIN}"
            paths:
              - path: /
                pathType: Prefix
                service:
                  identifier: app
                  port: mongo-express
        tls:
          - hosts:
              - "mongolibre.${SECRET_DOMAIN}"

    persistence:
      config:
        enabled: true
        type: configMap
        name: librechat-config
        advancedMounts:
          librechat:
            app:
              - subPath: librechat.yaml
                path: /app/librechat.yaml

      app-data:
        enabled: true
        type: emptyDir
        advancedMounts:
          librechat:
            app:
              - path: /app/data
              - path: /app/api/data

      tmp:
        enabled: true
        type: emptyDir
        globalMounts:
          - path: /tmp

      data:
        existingClaim: librechat
        advancedMounts:
          librechat:
            app:
              - subPath: logs
                path: /app/api/logs
              - subPath: images
                path: /app/client/public/images
              - subPath: uploads
                path: /app/uploads
            mongodb:
              - subPath: mongodb
                path: /data/db
            meilisearch:
              - subPath: meilisearch
                path: /meili_data
            ragapi:
              - subPath: ragapi
                path: /app/uploads

    serviceMonitor:
      app:
        serviceName: librechat
        endpoints:
          - port: metrics
            scheme: http
            path: /
            interval: 1m
            scrapeTimeout: 30s

File: ./kubernetes/apps/librechat/ks.yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: librechat
  namespace: flux-system
spec:
  targetNamespace: default
  path: ./kubernetes/apps/librechat/app
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false
  interval: 30m
  retryInterval: 1m
  timeout: 5m
  postBuild:

File: ./kubernetes/apps/democractic-system/local-path/app/helm-release.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: local-path-provisioner
spec:
  interval: 30m
  chart:
    spec:
      chart: democratic-csi
      version: 0.14.7
      sourceRef:
        name: democratic-csi
        kind: HelmRepository
        namespace: flux-system
  values:
    csiDriver:
      name: "org.democratic-csi.local-hostpath"
      attachRequired: false
      storageCapacity: true # With storage capacity tracking, the scheduler filters out nodes which do not have enough capacity.
      fsGroupPolicy: File # fsGroupChangePolicy

    storageClasses:
      - name: local-hostpath
        defaultClass: false
        reclaimPolicy: Delete
        volumeBindingMode: WaitForFirstConsumer
        # distributed support is not yet ready for expansion
        allowVolumeExpansion: false

    volumeSnapshotClasses:
      - name: local-hostpath
        deletionPolicy: Delete
        parameters:
          dummy: {}

    controller:
      enabled: true
      strategy: node
      externalAttacher:
        enabled: false
      externalProvisioner:
        enabled: true
        extraArgs:
          - --leader-election=false
          - --node-deployment=true
          - --node-deployment-immediate-binding=false
          - --feature-gates=Topology=true
          - --strict-topology=true
          - --enable-capacity=true
          - --capacity-ownerref-level=1
      externalResizer:
        enabled: false
      externalSnapshotter:
        enabled: true
        extraArgs:
          - --leader-election=false
          - --node-deployment=true

    node:
      driver:
        extraVolumeMounts:
          - name: local-hostpath
            mountPath: /var/democratic/local-hostpath
            mountPropagation: Bidirectional

      extraVolumes:
        - name: local-hostpath
          hostPath:
            path: /var/democratic/local-hostpath
            type: DirectoryOrCreate

    driver:
      config:
        driver: local-hostpath
        instance_id:
        local-hostpath:
          shareBasePath: "/var/democratic/local-hostpath/"
          controllerBasePath: "/var/democratic/local-hostpath/"
          dirPermissionsMode: "0770"
          dirPermissionsUser: 0
          dirPermissionsGroup: 0

File: ./kubernetes/apps/democractic-system/local-path/app/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helm-release.yaml

File: ./kubernetes/apps/democractic-system/local-path/ks.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app democractic-csi-local-path
  namespace: flux-system
spec:
  targetNamespace: &ns democractic-system
  interval: 10m
  path: "./kubernetes/apps/democractic-system/local-path/app"
  prune: false
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: true
  postBuild:
    substitute:
      APP_NS: *ns

File: ./kubernetes/apps/democractic-system/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ns.yaml
  - local-path/ks.yaml

File: ./kubernetes/apps/democractic-system/ns.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: democractic-system
  labels:
    kustomize.toolkit.fluxcd.io/prune: disabled

File: ./kubernetes/apps/media/sabnzbd/app/replicationdestination.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/datreeio/CRDs-catalog/main/volsync.backube/replicationdestination_v1alpha1.json
apiVersion: volsync.backube/v1alpha1
kind: ReplicationDestination
metadata:
  name: "${APP}-bootstrap"
spec:
  trigger:
    manual: restore-once
  restic:
    copyMethod: Snapshot
    repository: "${APP}-volsync-r2"
    cacheStorageClassName: "${VOLSYNC_CACHE_SNAPSHOTCLASS:-local-hostpath}"
    cacheCapacity: "${VOLSYNC_CACHE_CAPACITY:-1Gi}"
    storageClassName: "${VOLSYNC_STORAGECLASS:-ceph-rbd}"
    volumeSnapshotClassName: "${VOLSYNC_SNAPSHOTCLASS:-csi-ceph-blockpool}"
    moverSecurityContext:
      runAsUser: ${APP_UID:-568}
      runAsGroup: ${APP_GID:-568}
      fsGroup: ${APP_GID:-568}
    accessModes:
      - "${VOLSYNC_ACCESSMODES:-ReadWriteOnce}"
    capacity: "${VOLSYNC_CAPACITY:-1Gi}"

File: ./kubernetes/apps/media/sabnzbd/app/helm-release.yaml
---
# yaml-language-server: $schema=https://flux.jank.ing/helmrelease/v2/github/bjw-s/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: sabnzbd
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 3.6.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  dependsOn:
    - name: rook-ceph-cluster
      namespace: rook-ceph
  values:
    controllers:
      sabnzbd:
        annotations:
          reloader.stakater.com/auto: "true"
        containers:
          app:
            image:
              repository: ghcr.io/onedr0p/sabnzbd
              tag: 4.4.1@sha256:4188d3c29c53de1018edcfd5dc2d0a0c7955b9a239b91ff6c859626abd3494dc
            env:
              SABNZBD__PORT: &port 80
              SABNZBD__HOST_WHITELIST_ENTRIES: >-
                sabnzbd,
                sabnzbd.default,
                sabnzbd.default.svc,
                sabnzbd.default.svc.cluster,
                sabnzbd.default.svc.cluster.local,
                ${HOSTNAME}
#            envFrom:
#              - secretRef:
#                  name: sabnzbd-secret
            probes:
              liveness: &probes
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /api?mode=version
                    port: *port
                  initialDelaySeconds: 0
                  periodSeconds: 10
                  timeoutSeconds: 1
                  failureThreshold: 3
              readiness: *probes
            resources:
              requests:
                cpu: 100m
              limits:
                memory: 8Gi
            securityContext:
              runAsUser: 65534
              runAsGroup: 65534
              runAsNonRoot: true
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              capabilities:
                drop:
                  - ALL
          gluetun:
            env:
              VPN_TYPE: openvpn
              VPN_INTERFACE: tun0
              FIREWALL: on
              DOT: off
              FIREWALL_INPUT_PORTS: 80
            envFrom:
              - secretRef:
                  name: openvpn-secret
            image:
              repository: ghcr.io/qdm12/gluetun
              tag: v3.40.0
            securityContext:
              privileged: true
              allowPrivilegeEscalation: true
              capabilities:
                add:
                  - NET_ADMIN
    service:
      app:
        controller: sabnzbd
        ports:
          http:
            port: *port
    ingress:
      app:
        className: internal
        hosts:
          - host: ${HOSTNAME}
            paths:
              - path: /
                service:
                  identifier: app
                  port: http
        tls:
          - hosts:
              - ${HOSTNAME}
    persistence:
      config:
        existingClaim: sabnzbd
      logs:
        type: emptyDir
        globalMounts:
          - path: /config/logs
      tmp:
        type: emptyDir
      media-downloads:
        type: nfs
        server: 192.168.90.101
        path: /mnt/QuadSquad/EC/Downloads
        globalMounts:
          - path: /mnt/QuadSquad/EC/Downloads

File: ./kubernetes/apps/media/sabnzbd/app/replicationsource.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/datreeio/CRDs-catalog/main/volsync.backube/replicationsource_v1alpha1.json
apiVersion: volsync.backube/v1alpha1
kind: ReplicationSource
metadata:
  name: ${APP}
spec:
  sourcePVC: "sabnzbd"
  trigger:
    schedule: "0 0 * * *"
  restic:
    copyMethod: Snapshot
    repository: ${APP}-volsync-r2
    cacheStorageClassName: "${VOLSYNC_CACHE_SNAPSHOTCLASS:-local-hostpath}"
    cacheCapacity: "${VOLSYNC_CACHE_CAPACITY:-1Gi}"
    storageClassName: "${VOLSYNC_STORAGECLASS:-ceph-rbd}"
    volumeSnapshotClassName: "${VOLSYNC_SNAPSHOTCLASS:-csi-ceph-blockpool}"
    moverSecurityContext:
      runAsUser: ${APP_UID:-568}
      runAsGroup: ${APP_GID:-568}
      fsGroup: ${APP_GID:-568}
    pruneIntervalDays: 7
    retain:
      hourly: 24
      daily: 7
      weekly: 5

File: ./kubernetes/apps/media/sabnzbd/app/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: "${VOLSYNC_CLAIM:-${APP}}"
spec:
  accessModes:
    - "${VOLSYNC_ACCESSMODES:-ReadWriteOnce}"
  dataSourceRef:
    kind: ReplicationDestination
    apiGroup: volsync.backube
    name: "${APP}-bootstrap"
  resources:
    requests:
      storage: "${VOLSYNC_CAPACITY:-1Gi}"
  storageClassName: "${VOLSYNC_STORAGECLASS:-ceph-rbd}"

File: ./kubernetes/apps/media/sabnzbd/app/kustomization.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/kustomization
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - helm-release.yaml
  - pvc.yaml
  - replicationdestination.yaml
  - replicationsource.yaml

File: ./kubernetes/apps/media/sabnzbd/ks.yaml
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app sabnzbd
  namespace: flux-system
spec:
  targetNamespace: media
  path: ./kubernetes/apps/media/sabnzbd/app/
  prune: true
  wait: false
  sourceRef:
    kind: GitRepository
    name: thepatriots
  dependsOn:
    - name: sabnzbd-secrets
  interval: 30m
  retryInterval: 1m
  timeout: 5m
  postBuild:
    substitute:
      APP: *app
      APP_UID: "65534"
      APP_GID: "65534"
      HOSTNAME: "sab.${SECRET_DOMAIN}"

File: ./kubernetes/apps/media/sabnzbd/ks-secrets.yaml
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: sabnzbd-secrets
  namespace: flux-system
spec:
  targetNamespace: media
  path: ./kubernetes/apps/media/sabnzbd/secrets/
  prune: true
  wait: true # Flux dependents
  sourceRef:
    kind: GitRepository
    name: thepatriots
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/media/sabnzbd/secrets/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
RESTIC_REPOSITORY:
RESTIC_PASSWORD:
AWS_ACCESS_KEY_ID:
AWS_SECRET_ACCESS_KEY:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/media/sabnzbd/secrets/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./secret.sops.yaml

File: ./kubernetes/apps/media/prowlarr/app/helmrelease.yaml
---
# yaml-language-server: $schema=https://flux.jank.ing/helmrelease/v2/github/bjw-s/helm-charts/main/charts/library/common/values.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app prowlarr
  namespace: default
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 3.6.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  maxHistory: 2
  install:
    createNamespace: true
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  uninstall:
    keepHistory: false
  values:
    controllers:
      prowlarr:
        annotations:
          reloader.stakater.com/auto: "true"
        initContainers:
          init-db:
            image:
              repository: ghcr.io/onedr0p/postgres-init
              tag: 16
            envFrom:
              - secretRef:
                  name: prowlarr-init-secrets
        containers:
          app:
            image:
              repository: ghcr.io/onedr0p/prowlarr-develop
              tag: 1.29.2.4915@sha256:b258cc8fe38a25af3742964a2d5a749c645562b3433ef79aa5e1748070ca99d3
            env:
              TZ: "${TIMEZONE}"
              PROWLARR__APP__INSTANCENAME: Prowlarr
              PROWLARR__APP__THEME: dark
              PROWLARR__LOG__DBENABLED: "False"
              PROWLARR__LOG__LEVEL: info
              PROWLARR__AUTH__METHOD: External
              PROWLARR__AUTH__REQUIRED: DisabledForLocalAddresses
              PROWLARR__SERVER__PORT: &port 8080
              PROWLARR__UPDATE__BRANCH: develop
            envFrom:
              - secretRef:
                  name: prowlarr-secret
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
              limits:
                memory: 500Mi
    service:
      app:
        controller: *app
        ports:
          http:
            port: *port
    ingress:
      app:
        enabled: true
        className: internal
        annotations:
          hajimari.io/icon: mdi:movie-search
          gethomepage.dev/enabled: "true"
          gethomepage.dev/name: Prowlarr
          gethomepage.dev/description: Torrent and Usenet Indexer manager/proxy.
          gethomepage.dev/group: Media
          gethomepage.dev/icon: prowlarr.png
          gethomepage.dev/pod-selector: >-
            app in (
              prowlarr
            )
        hosts:
          - host: &host "{{ .Release.Name }}.${SECRET_DOMAIN}"
            paths:
              - path: /
                service:
                  identifier: app
                  port: http
        tls:
          - hosts:
              - *host
    persistence:
      config:
        enabled: true
        type: emptyDir

File: ./kubernetes/apps/media/prowlarr/app/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helmrelease.yaml

File: ./kubernetes/apps/media/prowlarr/ks.yaml
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: prowlarr
  namespace: flux-system
spec:
  targetNamespace: media
  path: ./kubernetes/apps/media/prowlarr/app/
  prune: true
  wait: false
  sourceRef:
    kind: GitRepository
    name: thepatriots
  dependsOn:
    - name: prowlarr-secrets
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/media/prowlarr/ks-secrets.yaml
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: prowlarr-secrets
  namespace: flux-system
spec:
  targetNamespace: media
  path: ./kubernetes/apps/media/prowlarr/secrets/
  prune: true
  wait: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/media/prowlarr/secrets/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
PROWLARR__POSTGRES__PASSWORD:
PROWLARR__POSTGRES__USER:
PROWLARR__POSTGRES__HOST:
PROWLARR__POSTGRES__PORT:
PROWLARR__POSTGRES__MAINDB:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/media/prowlarr/secrets/init-secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
INIT_POSTGRES_HOST:
INIT_POSTGRES_SUPER_PASS:
INIT_POSTGRES_USER:
INIT_POSTGRES_PASS:
INIT_POSTGRES_DBNAME:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/media/prowlarr/secrets/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./init-secret.sops.yaml
  - ./secret.sops.yaml

File: ./kubernetes/apps/media/radarr/app/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: radarr
spec:
  resources:
    requests:
      storage: 1Gi
  accessModes:
    - ReadWriteOnce
  storageClassName: cephfs

File: ./kubernetes/apps/media/radarr/app/helmrelease.yaml
# yaml-language-server: $schema=https://flux.jank.ing/helmrelease/v2/github/bjw-s/helm-charts/main/charts/library/common/values.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app radarr
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 3.6.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  dependsOn:
  - name: rook-ceph-cluster
    namespace: rook-ceph
  values:
    controllers:
      radarr:
        annotations:
          reloader.stakater.com/auto: "true"
        initContainers:
          init-db:
            image:
              repository: ghcr.io/onedr0p/postgres-init
              tag: 16
            envFrom:
              - secretRef:
                  name: radarr-init-secrets
        containers:
          app:
            image:
              repository: ghcr.io/onedr0p/radarr-develop
              tag: 5.17.2.9580
            env:
              RADARR__APP__INSTANCENAME: Radarr
              RADARR__APP__THEME: dark
              RADARR__AUTH__METHOD: External
              RADARR__AUTH__REQUIRED: DisabledForLocalAddresses
              RADARR__LOG__DBENABLED: "False"
              RADARR__LOG__LEVEL: info
              RADARR__SERVER__PORT: &port 80
              RADARR__UPDATE__BRANCH: develop
              TZ: ${TIMEZONE}
            envFrom:
            - secretRef:
                name: radarr-secrets
            probes:
              liveness: &probes
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /ping
                    port: *port
                  initialDelaySeconds: 0
                  periodSeconds: 10
                  timeoutSeconds: 1
                  failureThreshold: 3
              readiness: *probes
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              capabilities: { drop: ["ALL"] }
            resources:
              requests:
                cpu: 100m
              limits:
                memory: 2Gi
    defaultPodOptions:
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
        runAsGroup: 65534
        fsGroup: 65534
        fsGroupChangePolicy: OnRootMismatch
        supplementalGroups: [10000]
        seccompProfile: { type: RuntimeDefault }
    service:
      app:
        controller: *app
        ports:
          http:
            port: *port
    ingress:
      app:
        className: internal
        annotations:
          gethomepage.dev/enabled: "true"
          gethomepage.dev/group: Downloads
          gethomepage.dev/name: Radarr
          gethomepage.dev/icon: radarr.png
          gethomepage.dev/description: Movie Downloads
        hosts:
        - host: &host "{{ .Release.Name }}.${SECRET_DOMAIN}"
          paths:
          - path: /
            service:
              identifier: app
              port: http
        tls:
        - hosts:
            - *host
    persistence:
      config:
        existingClaim: *app
      tmp:
        type: emptyDir
      media:
        type: nfs
        server: 192.168.90.101
        path: /mnt/QuadSquad/EC/Videos
        globalMounts:
          - path: /mnt/QuadSquad/EC/Videos
      downloads:
        type: nfs
        server: 192.168.90.101
        path: /mnt/QuadSquad/EC/Downloads
        globalMounts:
          - path: /mnt/QuadSquad/EC/Downloads

File: ./kubernetes/apps/media/radarr/app/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helmrelease.yaml
  - ./pvc.yaml

File: ./kubernetes/apps/media/radarr/ks.yaml
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: radarr
  namespace: flux-system
spec:
  targetNamespace: media
  path: ./kubernetes/apps/media/radarr/app/
  prune: true
  wait: false
  sourceRef:
    kind: GitRepository
    name: thepatriots
  dependsOn:
    - name: radarr-secrets
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/media/radarr/ks-secrets.yaml
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: radarr-secrets
  namespace: flux-system
spec:
  targetNamespace: media
  path: ./kubernetes/apps/media/radarr/secrets/
  prune: true
  wait: true # Flux dependents
  sourceRef:
    kind: GitRepository
    name: thepatriots
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/media/radarr/secrets/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
RADARR__POSTGRES__PASSWORD:
RADARR__POSTGRES__USER:
RADARR__POSTGRES__HOST:
RADARR__POSTGRES__PORT:
RADARR__POSTGRES__MAINDB:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/media/radarr/secrets/init-secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
INIT_POSTGRES_HOST:
INIT_POSTGRES_SUPER_PASS:
INIT_POSTGRES_USER:
INIT_POSTGRES_PASS:
INIT_POSTGRES_DBNAME:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/media/radarr/secrets/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./init-secret.sops.yaml
  - ./secret.sops.yaml

File: ./kubernetes/apps/media/sonarr/app/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: sonarr
spec:
  resources:
    requests:
      storage: 1Gi
  accessModes:
    - ReadWriteOnce
  storageClassName: cephfs

File: ./kubernetes/apps/media/sonarr/app/helmrelease.yaml
# yaml-language-server: $schema=https://flux.jank.ing/helmrelease/v2/github/bjw-s/helm-charts/main/charts/library/common/values.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app sonarr
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 3.6.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  dependsOn:
  - name: rook-ceph-cluster
    namespace: rook-ceph
  values:
    controllers:
      sonarr:
        annotations:
          reloader.stakater.com/auto: "true"
        initContainers:
          init-db:
            image:
              repository: ghcr.io/onedr0p/postgres-init
              tag: 16
            envFrom:
              - secretRef:
                  name: sonarr-init-secrets
        containers:
          app:
            image:
              repository: ghcr.io/onedr0p/sonarr-develop
              tag: 4.0.12.2825
            env:
              SONARR__APP__INSTANCENAME: Sonarr
              SONARR__APP__THEME: dark
              SONARR__AUTH__METHOD: External
              SONARR__AUTH__REQUIRED: DisabledForLocalAddresses
              SONARR__LOG__DBENABLED: "False"
              SONARR__LOG__LEVEL: info
              SONARR__SERVER__PORT: &port 80
              SONARR__UPDATE__BRANCH: develop
              TZ: ${TIMEZONE}
            envFrom:
            - secretRef:
                name: sonarr-secret
            probes:
              liveness: &probes
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /ping
                    port: *port
                  initialDelaySeconds: 0
                  periodSeconds: 10
                  timeoutSeconds: 1
                  failureThreshold: 3
              readiness: *probes
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              capabilities: { drop: ["ALL"] }
            resources:
              requests:
                cpu: 100m
              limits:
                memory: 2Gi
    defaultPodOptions:
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
        runAsGroup: 65534
        fsGroup: 65534
        fsGroupChangePolicy: OnRootMismatch
        supplementalGroups: [10000]
        seccompProfile: { type: RuntimeDefault }
    service:
      app:
        controller: *app
        ports:
          http:
            port: *port
    ingress:
      app:
        className: internal
        annotations:
          gethomepage.dev/enabled: "true"
          gethomepage.dev/group: Downloads
          gethomepage.dev/name: Sonarr
          gethomepage.dev/icon: sonarr.png
        hosts:
        - host: &host "{{ .Release.Name }}.${SECRET_DOMAIN}"
          paths:
          - path: /
            service:
              identifier: app
              port: http
        tls:
        - hosts:
            - *host
    persistence:
      config:
        existingClaim: *app
      tmp:
        type: emptyDir
      media:
        type: nfs
        server: 192.168.90.101
        path: /mnt/QuadSquad/EC/Videos
        globalMounts:
          - path: /mnt/QuadSquad/EC/Videos
      downloads:
        type: nfs
        server: 192.168.90.101
        path: /mnt/QuadSquad/EC/Downloads
        globalMounts:
          - path: /mnt/QuadSquad/EC/Downloads

File: ./kubernetes/apps/media/sonarr/app/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./pvc.yaml
  - ./helmrelease.yaml

File: ./kubernetes/apps/media/sonarr/ks.yaml
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: sonarr
  namespace: flux-system
spec:
  targetNamespace: media
  path: ./kubernetes/apps/media/sonarr/app/
  prune: true
  wait: false
  sourceRef:
    kind: GitRepository
    name: thepatriots
  dependsOn:
    - name: sonarr-secrets
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/media/sonarr/ks-secrets.yaml
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: sonarr-secrets
  namespace: flux-system
spec:
  targetNamespace: media
  path: ./kubernetes/apps/media/sonarr/secrets/
  prune: true
  wait: true # Flux dependents
  sourceRef:
    kind: GitRepository
    name: thepatriots
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/media/sonarr/secrets/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
SONARR__POSTGRES__PASSWORD:
SONARR__POSTGRES__USER:
SONARR__POSTGRES__HOST:
SONARR__POSTGRES__PORT:
SONARR__POSTGRES__MAINDB:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/media/sonarr/secrets/init-secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
INIT_POSTGRES_HOST:
INIT_POSTGRES_SUPER_PASS:
INIT_POSTGRES_USER:
INIT_POSTGRES_PASS:
INIT_POSTGRES_DBNAME:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/media/sonarr/secrets/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./init-secret.sops.yaml
  - ./secret.sops.yaml

File: ./kubernetes/apps/media/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: media-pvc-v1
  namespace: media
spec:
  resources:
    requests:
      storage: 100Ti
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  storageClassName: cephfs-media-ec

File: ./kubernetes/apps/media/plex/app/secrets.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
PLEX_CLAIM_TOKEN:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/media/plex/app/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: plex
spec:
  accessModes: ["ReadWriteOnce"]
  resources:
    requests:
      storage: 20Gi
  storageClassName: cephfs
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: plex-cache
spec:
  accessModes: ["ReadWriteOnce"]
  resources:
    requests:
      storage: 100Gi
  storageClassName: ceph-rbd

File: ./kubernetes/apps/media/plex/app/helmrelease.yaml
---
# yaml-language-server: $schema=https://flux.jank.ing/helmrelease/v2/github/bjw-s/helm-charts/main/charts/library/common/values.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: plex
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 3.6.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  dependsOn:
    - name: intel-device-plugin-gpu
      namespace: gpu-system
    - name: rook-ceph-cluster
      namespace: rook-ceph
  values:
    defaultPodOptions:
      nodeSelector:
        intel.feature.node.kubernetes.io/gpu: "true"
    controllers:
      plex:
        annotations:
          reloader.stakater.com/auto: "true"

        pod:
          securityContext:
            runAsUser: 65534
            runAsGroup: 65534
            fsGroup: 65534
            fsGroupChangePolicy: "OnRootMismatch"
            supplementalGroups:
              - 44
              - 109
        containers:
          app:
            image:
              repository: ghcr.io/onedr0p/plex
              tag: 1.41.3.9292-bc7397402@sha256:9a9196a109437035b9b20c8d368c569555623f14faf2247c3aa0a84cf568242d
            env:
              TZ: America/New_York
              PLEX_ADVERTISE_URL: https://plex.${SECRET_DOMAIN_LEGACY}:443,http://10.10.12.241:32400
            envFrom:
              - secret: plex-secret
            probes:
              liveness: &probes
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /identity
                    port: 32400
                  initialDelaySeconds: 0
                  periodSeconds: 10
                  timeoutSeconds: 1
                  failureThreshold: 3
              readiness: *probes
              startup:
                enabled: true
                spec:
                  failureThreshold: 30
                  periodSeconds: 10
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              capabilities: { drop: ["ALL"] }
            resources:
              requests:
                cpu: 100m
                memory: 6Gi
              limits:
                gpu.intel.com/i915: 1
                memory: 6Gi
    service:
      app:
        controller: plex
        type: LoadBalancer
        annotations:
          lbipam.cilium.io/ips: 10.10.12.241
        ports:
          http:
            port: 32400
    ingress:
      app:
        annotations:
          external-dns.alpha.kubernetes.io/target: external.${SECRET_DOMAIN_LEGACY}
          nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
        className: external
        hosts:
          - host: &host "{{ .Release.Name }}.${SECRET_DOMAIN_LEGACY}"
            paths:
              - path: /
                service:
                  identifier: app
                  port: http
        tls:
          - secretName: "${SECRET_DOMAIN_LEGACY/./-}-production-tls"
            hosts:
              - *host
      internal:
        annotations:
          nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
        className: internal
        hosts:
          - host: *host
            paths:
              - path: /
                service:
                  identifier: app
                  port: http
        tls:
          - hosts:
              - *host
    persistence:
      config:
        existingClaim: plex
        globalMounts:
          - path: /config/Library/Application Support/Plex Media Server
      # Separate PVC for cache to avoid backing up cache files
      cache:
        existingClaim: plex-cache
        globalMounts:
          - path: /config/Library/Application Support/Plex Media Server/Cache
      logs:
        type: emptyDir
        globalMounts:
          - path: /config/Library/Application Support/Plex Media Server/Logs
      tmp:
        type: emptyDir
      transcode:
        type: emptyDir
      media:
        type: nfs
        server: 192.168.90.101
        path: /mnt/QuadSquad/EC/Videos
        globalMounts:
          - path: /mnt/QuadSquad/EC/Videos
      media-music:
        type: nfs
        server: 192.168.90.101
        path: /mnt/QuadSquad/EC/Music
        globalMounts:
          - path: /mnt/QuadSquad/EC/Music

File: ./kubernetes/apps/media/plex/app/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helmrelease.yaml
  - ./pvc.yaml
  - ./secrets.sops.yaml

File: ./kubernetes/apps/media/plex/ks.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app plex
  namespace: flux-system
spec:
  targetNamespace: &ns media
  interval: 10m
  path: "./kubernetes/apps/media/plex/app"
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false

File: ./kubernetes/apps/media/qbittorrent/app/secrets.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
VPN_SERVICE_PROVIDER:
OPENVPN_USER:
OPENVPN_PASSWORD:
SERVER_REGIONS:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/media/qbittorrent/app/.decrypted~secrets.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
VPN_SERVICE_PROVIDER:
OPENVPN_USER:
OPENVPN_PASSWORD:
SERVER_REGIONS:

File: ./kubernetes/apps/media/qbittorrent/app/pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: qbittorrent-config
  namespace: media
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 2Gi
  storageClassName: cephfs

File: ./kubernetes/apps/media/qbittorrent/app/helmrelease.yaml
---
# yaml-language-server: $schema=https://flux.jank.ing/helmrelease/v2/github/bjw-s/helm-charts/main/charts/library/common/values.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: qbit
  namespace: media
spec:
  chart:
    spec:
      chart: app-template
      interval: 30m
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
      version: 3.6.0
  install:
    createNamespace: true
    remediation:
      retries: 3
  interval: 30m
  maxHistory: 2
  uninstall:
    keepHistory: false
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: uninstall
      retries: 3
  values:
    controller:
      annotations:
        reloader.stakater.com/auto: "true"
    controllers:
      qbittorrent:
        pod:
          securityContext:
            runAsUser: 65534
            runAsGroup: 65534
            fsGroup: 65534
            fsGroupChangePolicy: "OnRootMismatch"
            supplementalGroups:
              - 44
              - 109
        containers:
          qbit:
            env:
              QBITTORRENT__PORT: &port 8080
              TZ: America/New_York
            image:
              repository: ghcr.io/onedr0p/qbittorrent
              tag: 5.0.3@sha256:3d62f065290ae77a10c7f7deaef7bc857068feff89503773707d2dae339b66c6
            probes:
              liveness:
                enabled: false
              readiness:
                enabled: false
              startup:
                enabled: false
            resources:
              limits:
                memory: 10Gi
              requests:
                cpu: 49m
                memory: 5Gi
            securityContext:
              runAsUser: 568
              runAsGroup: 568
              runAsNonRoot: true
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              capabilities:
                drop:
                  - ALL
          gluetun:
            env:
              VPN_TYPE: openvpn
              VPN_INTERFACE: tun0
              FIREWALL: on
              DOT: off
              FIREWALL_INPUT_PORTS: 8080
              VPN_PORT_FORWARDING: on
            envFrom:
              - secretRef:
                  name: openvpn-secret
            image:
              repository: ghcr.io/qdm12/gluetun
              tag: latest@sha256:62e76d60e4220950558215647049c2b9ef0fc64272b80f9012562555b140bf84
            securityContext:
              privileged: true
              allowPrivilegeEscalation: true
              capabilities:
                add:
                  - NET_ADMIN
          port-forward:
            image:
              repository: ghcr.io/bjw-s-labs/gluetun-qb-port-sync
              tag: 0.0.2@sha256:14b9a10cda0a8dff6bd2d131686d3f6b8dd550a4a835674b22aaa711b4c34ecd
            env:
              GLUETUN_CONTROL_SERVER_HOST: localhost
              GLUETUN_CONTROL_SERVER_PORT: 8000
              QBITTORRENT_HOST: localhost
              QBITTORRENT_WEBUI_PORT: *port
              CRON_ENABLED: true
              CRON_SCHEDULE: "*/5 * * * *"
              LOG_TIMESTAMP: false
            securityContext:
              runAsUser: 65534
              runAsGroup: 65534
              runAsNonRoot: true
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              capabilities:
                drop:
                  - ALL
    ingress:
      qbit:
        className: internal
        hosts:
          - host: &host "qb.${SECRET_DOMAIN}"
            paths:
              - path: /
                pathType: Prefix
                service:
                  identifier: qbittorrent
                  port: http
        tls:
          - hosts:
              - *host
    service:
      qbittorrent:
        controller: qbittorrent
        ports:
          http:
            port: 8080
    persistence:
      config:
        existingClaim: qbittorrent-config
        globalMounts:
          - path: /config
      media:
        type: nfs
        server: 192.168.90.101
        path: /mnt/QuadSquad/EC/Downloads
        globalMounts:
          - path: /mnt/QuadSquad/EC/Downloads

File: ./kubernetes/apps/media/qbittorrent/app/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./pvc.yaml
  - ./secrets.sops.yaml
  - ./helmrelease.yaml

File: ./kubernetes/apps/media/qbittorrent/ks.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app qbittorrent
  namespace: flux-system
spec:
  targetNamespace: &ns media
  interval: 10m
  path: "./kubernetes/apps/media/qbittorrent/app"
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false

File: ./kubernetes/apps/media/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./ns.yaml
#  - ./pvc.yaml
  - ./plex/ks.yaml
  - ./qbittorrent/ks.yaml
  - ./prowlarr/ks-secrets.yaml
  - ./prowlarr/ks.yaml
  - ./radarr/ks-secrets.yaml
  - ./radarr/ks.yaml
  - ./sonarr/ks-secrets.yaml
  - ./sonarr/ks.yaml
  - ./sabnzbd/ks-secrets.yaml
  - ./sabnzbd/ks.yaml
  - ./flaresolverr/ks.yaml

File: ./kubernetes/apps/media/flaresolverr/app/helmrelease.yaml
---
# yaml-language-server: $schema=https://flux.jank.ing/helmrelease/v2/github/bjw-s/helm-charts/main/charts/library/common/values.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: flaresolverr
spec:
  interval: 5m
  install:
    timeout: 15m
    remediation:
      retries: 5
  upgrade:
    timeout: 15m
    remediation:
      retries: 5
      remediateLastFailure: true
    cleanupOnFail: true
  chart:
    spec:
      chart: app-template
      version: 3.6.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
      interval: 5m
  values:

    controllers:
      flaresolverr:
        type: deployment
        annotations:
          reloader.stakater.com/auto: "true"

        containers:
          flaresolverr:
            image:
              repository: ghcr.io/flaresolverr/flaresolverr
              tag: v3.3.21
            env:
              LOG_LEVEL: debug
            resources:
              requests:
                cpu: 100m
                memory: 512M
              limits:
                memory: 800M
    service:
      flaresolverr:
        controller: flaresolverr
        ports:
          http:
            port: 8191

File: ./kubernetes/apps/media/flaresolverr/app/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helmrelease.yaml

File: ./kubernetes/apps/media/flaresolverr/ks.yaml
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: flaresolverr
  namespace: flux-system
spec:
  targetNamespace: media
  path: ./kubernetes/apps/media/flaresolverr/app/
  prune: true
  wait: false
  sourceRef:
    kind: GitRepository
    name: thepatriots
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/media/ns.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: media

File: ./kubernetes/apps/rustdesk/app/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
key_pub:
key_priv:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/rustdesk/app/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rustdesk
spec:
  resources:
    requests:
      storage: 20Gi
  accessModes:
    - ReadWriteOnce
  storageClassName: cephfs

File: ./kubernetes/apps/rustdesk/app/helmrelease.yaml
---
# yaml-language-server: $schema=https://flux.jank.ing/helmrelease/v2/github/bjw-s/helm-charts/common-3.5.1/charts/library/common
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app rustdesk
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 3.6.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  values:
    controllers:
      rustdesk:
        annotations:
          reloader.stakater.com/auto: "true"
        containers:
          hbbr:
            image: &image
              repository: ghcr.io/rustdesk/rustdesk-server
              tag: 1.1.12
            env:
              TZ: ${TIMEZONE}
            command: ["hbbr"]
            probes: &probes
              liveness:
                enabled: false
              readiness:
                enabled: false
              startup:
                enabled: false
            securityContext: &securityContext
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              capabilities: { drop: ["ALL"] }
            resources: &resources
              requests:
                cpu: 10m
                memory: 128Mi
              limits:
                memory: 256Mi
          hbbs:
            image: *image
            env:
              TZ: ${TIMEZONE}
              DB_URL: /db/db_v2.sqlite3
              RELAY: "10.10.12.245:21117"
            command: ["hbbs", "-r rustdesk:21117"]
            probes: *probes
            securityContext: *securityContext
            resources: *resources
    defaultPodOptions:
      securityContext:
        runAsNonRoot: true
        runAsUser: 568
        runAsGroup: 568
        fsGroup: 568
        fsGroupChangePolicy: OnRootMismatch
        seccompProfile: { type: RuntimeDefault }
    service:
      hbbs:
        controller: *app
        type: LoadBalancer
        annotations:
          io.cilium/lb-ipam-ips: 10.10.12.245
          external-dns.alpha.kubernetes.io/hostname: rustdesk.${SECRET_DOMAIN}
        ports:
          hbbr-1:
            enabled: true
            port: 21117
            protocol: "TCP"
          hbbr-2:
            enabled: true
            port: 21119
            protocol: "TCP"
          hbbs-1:
            enabled: true
            port: 21115
            protocol: "TCP"
          hbbs-2:
            enabled: true
            port: 21116
            protocol: "TCP"
          hbbs-3:
            enabled: true
            port: 21116
            protocol: "UDP"
          hbbs-4:
            enabled: true
            port: 21118
            protocol: "TCP"
    persistence:
      tmp:
        type: emptyDir
      data:
        existingClaim: *app
        globalMounts:
          - path: /db
      hbbs-key:
        type: secret
        name: *app
        advancedMounts:
          rustdesk:
            hbbs:
              - subPath: key_pub
                path: /data/id_ed25519.pub
                readOnly: true
              - subPath: key_priv
                path: /data/id_ed25519
                readOnly: true

File: ./kubernetes/apps/rustdesk/app/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./pvc.yaml
  - ./secret.sops.yaml
  - ./helmrelease.yaml

File: ./kubernetes/apps/rustdesk/ks.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app rustdesk
  namespace: flux-system
spec:
  targetNamespace: &ns default
  interval: 10m
  timeout: 2m
  path: "./kubernetes/apps/rustdesk/app"
  prune: true
  wait: false
  sourceRef:
    kind: GitRepository
    name: thepatriots


File: ./kubernetes/apps/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - authentik/ks.yaml
  - cert-manager
  - database
  - democractic-system
  - flux-system
  - gpu-system
  - kube-system
  - network
  - media
  - observability
  - rook-ceph
  - emulation
  - volsync/ks.yaml
  - vaultwarden
  - invoiceninja
  - zammad/ks.yaml
  - meshcentral/ks.yaml
  - meshcentral/ks-backup.yaml
  - openproject/ks.yaml
  - bookstack/ks.yaml
  - librechat/ks.yaml
  - smtp-relay/ks.yaml
  #  - matrix-synapse/ks.yaml
  #  - matrix-synapse/element/ks.yaml
  #  - openebs-system
  #  - rustdesk/ks.yaml
  #  - misc-pvcs/ks.yaml

File: ./kubernetes/apps/kube-system/metrics-server/app/helmrelease.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: metrics-server
spec:
  interval: 30m
  chart:
    spec:
      chart: metrics-server
      version: 3.12.2
      sourceRef:
        kind: HelmRepository
        name: metrics-server
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    args:
      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      - --kubelet-use-node-status-port
      - --metric-resolution=15s
    metrics:
      enabled: true
    serviceMonitor:
      enabled: true

File: ./kubernetes/apps/kube-system/metrics-server/app/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helmrelease.yaml

File: ./kubernetes/apps/kube-system/metrics-server/ks.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app metrics-server
  namespace: flux-system
spec:
  targetNamespace: kube-system
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  path: ./kubernetes/apps/kube-system/metrics-server/app
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/kube-system/snapshot-controller/app/helm-release.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: snapshot-controller
  namespace: kube-system
spec:
  interval: 30m
  chart:
    spec:
      chart: snapshot-controller
      version: 3.0.6
      sourceRef:
        kind: HelmRepository
        name: piraeus-charts
        namespace: flux-system
  maxHistory: 2
  install:
    crds: CreateReplace
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    crds: CreateReplace
    remediation:
      retries: 3
  uninstall:
    keepHistory: false
  values:
    webhook:
      enabled: false
    controller:
      volumeSnapshotClasses:
        - name: csi-ceph-blockpool
          driver: rook-ceph.rbd.csi.ceph.com
          annotations:
            snapshot.storage.kubernetes.io/is-default-class: "true"
          parameters:
            clusterID: rook-ceph
            csi.storage.k8s.io/snapshotter-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/snapshotter-secret-namespace: rook-ceph
          deletionPolicy: Delete
        - name: csi-ceph-filesystem
          driver: rook-ceph.cephfs.csi.ceph.com
          annotations:
            snapshot.storage.kubernetes.io/is-default-class: "false"
          parameters:
            clusterID: rook-ceph
            csi.storage.k8s.io/snapshotter-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/snapshotter-secret-namespace: rook-ceph
          deletionPolicy: Delete
      serviceMonitor:
        create: true

File: ./kubernetes/apps/kube-system/snapshot-controller/app/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - helm-release.yaml

File: ./kubernetes/apps/kube-system/snapshot-controller/ks.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: snapshot-controller
  namespace: flux-system
spec:
  interval: 10m
  path: ./kubernetes/apps/kube-system/snapshot-controller/app
  prune: true
  wait: false
  sourceRef:
    kind: GitRepository
    name: thepatriots
  timeout: 2m

File: ./kubernetes/apps/kube-system/node-feature-discovery/operator/helmrelease.yaml
---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: node-feature-discovery
spec:
  interval: 30m
  chart:
    spec:
      chart: node-feature-discovery
      version: 0.16.6
      sourceRef:
        kind: HelmRepository
        name: node-feature-discovery
        namespace: flux-system
  install:
    crds: CreateReplace
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    crds: CreateReplace
    remediation:
      strategy: rollback
      retries: 3
  values:
    worker:
      config:
        core:
          sources: ["pci", "system", "usb", "kernel"]
        sources:
          pci:
            deviceClassWhitelist: ["0300", "0302"] # 0300 = display controller, 0302 = 3D Graphics Controllers
            deviceLabelFields: ["vendor"]
    prometheus:
      enable: true

File: ./kubernetes/apps/kube-system/node-feature-discovery/operator/kustomization.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/kustomization
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helmrelease.yaml

File: ./kubernetes/apps/kube-system/node-feature-discovery/ks.yaml
---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/kustomize.toolkit.fluxcd.io/kustomization_v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app node-feature-discovery
  namespace: flux-system
spec:
  targetNamespace: kube-system
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  path: ./kubernetes/apps/kube-system/node-feature-discovery/operator
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: true
  interval: 30m
  retryInterval: 1m
  timeout: 5m
---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/kustomize.toolkit.fluxcd.io/kustomization_v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app node-feature-discovery-rules
  namespace: flux-system
spec:
  targetNamespace: kube-system
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  dependsOn:
    - name: node-feature-discovery
  path: ./kubernetes/apps/kube-system/node-feature-discovery/rules
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: true
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/kube-system/node-feature-discovery/rules/intel-devices.yaml
---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/nfd.k8s-sigs.io/nodefeaturerule_v1alpha1.json
apiVersion: nfd.k8s-sigs.io/v1alpha1
kind: NodeFeatureRule
metadata:
  name: intel-gpu-device
spec:
  rules:
    - # Intel UHD Graphics 630
      name: intel.gpu
      labels:
        intel.feature.node.kubernetes.io/gpu: "true"
      matchFeatures:
        - feature: pci.device
          matchExpressions:
            class: { op: In, value: ["0300", "0380"] }
            vendor: { op: In, value: ["8086"] }

File: ./kubernetes/apps/kube-system/node-feature-discovery/rules/kustomization.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/kustomization
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./intel-devices.yaml

File: ./kubernetes/apps/kube-system/reflector/app/helmrelease.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: reflector
spec:
  interval: 5m
  chart:
    spec:
      chart: reflector
      version: 7.1.288
      sourceRef:
        kind: HelmRepository
        name: emberstack
        namespace: flux-system
      interval: 5m
  install:
    createNamespace: true
    remediation:
      retries: 3
  upgrade:
    remediation:
      retries: 3
  values:
    configuration:
      logging:
        minimumLevel: Debug

File: ./kubernetes/apps/kube-system/reflector/app/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helmrelease.yaml

File: ./kubernetes/apps/kube-system/reflector/ks.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: refector
  namespace: flux-system
spec:
  path: ./kubernetes/apps/kube-system/reflector/app
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  interval: 30m
  retryInterval: 1m
  timeout: 3m
  targetNamespace: kube-system

File: ./kubernetes/apps/kube-system/spegel/app/kustomizeconfig.yaml
---
nameReference:
  - kind: ConfigMap
    version: v1
    fieldSpecs:
      - path: spec/valuesFrom/name
        kind: HelmRelease

File: ./kubernetes/apps/kube-system/spegel/app/helm-values.yaml
---
image:
  repository: ghcr.io/jfroy/spegel
  digest: "sha256:50f6e296cb4083033148e6e609f6fa97028b9c17eff6ca26584471b856aa0264"
spegel:
  appendMirrors: true
  containerdSock: /run/containerd/containerd.sock
  containerdRegistryConfigPath: /etc/cri/conf.d/hosts
service:
  registry:
    hostPort: 29999

File: ./kubernetes/apps/kube-system/spegel/app/helmrelease.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: spegel
spec:
  interval: 30m
  chart:
    spec:
      chart: spegel
      version: v0.0.28
      sourceRef:
        kind: HelmRepository
        name: spegel
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  valuesFrom:
    - kind: ConfigMap
      name: spegel-helm-values
  values:
    grafanaDashboard:
      enabled: true
    serviceMonitor:
      enabled: true

File: ./kubernetes/apps/kube-system/spegel/app/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helmrelease.yaml
configMapGenerator:
  - name: spegel-helm-values
    files:
      - values.yaml=./helm-values.yaml
configurations:
  - kustomizeconfig.yaml

File: ./kubernetes/apps/kube-system/spegel/ks.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app spegel
  namespace: flux-system
spec:
  targetNamespace: kube-system
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  path: ./kubernetes/apps/kube-system/spegel/app
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/kube-system/csi-driver-nfs/app/storage-class.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-csi
provisioner: nfs.csi.k8s.io
parameters:
  server: <FILL ME>
  share: <FILL ME>
reclaimPolicy: Retain
volumeBindingMode: Immediate
mountOptions:
  - nfsvers=4.1
  - nconnect=8
  - hard
  - noatime

---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-kube-nas
provisioner: nfs.csi.k8s.io
parameters:
  server: <FILL ME>
  share: <FILL ME>
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
mountOptions:
  - nfsvers=4.1
  - nconnect=8
  - hard
  - noatime
  - tcp
  - timeo=600
  - retrans=2

File: ./kubernetes/apps/kube-system/csi-driver-nfs/app/helm--release.yaml
---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: csi-driver-nfs
spec:
  interval: 30m
  chart:
    spec:
      chart: csi-driver-nfs
      version: v4.9.0
      sourceRef:
        kind: HelmRepository
        name: csi-driver-nfs
        namespace: flux-system
  maxHistory: 3
  install:
    createNamespace: true
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  uninstall:
    keepHistory: false
  values:
    externalSnapshotter:
      enabled: true

File: ./kubernetes/apps/kube-system/csi-driver-nfs/app/kustomization.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/kustomization
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - helm-release.yaml
#  - storage-class.yaml

File: ./kubernetes/apps/kube-system/csi-driver-nfs/ks.yaml
---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/kustomize.toolkit.fluxcd.io/kustomization_v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: csi-driver-nfs
  namespace: flux-system
spec:
  interval: 30m
  path: ./kubernetes/apps/kube-system/csi-driver-nfs/app
  prune: true
  wait: true # Other kustomizations might depend on this one.
  sourceRef:
    kind: GitRepository
    name: thepatriots

File: ./kubernetes/apps/kube-system/reloader/app/helmrelease.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: reloader
spec:
  interval: 30m
  chart:
    spec:
      chart: reloader
      version: 1.1.0
      sourceRef:
        kind: HelmRepository
        name: stakater
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    fullnameOverride: reloader
    reloader:
      readOnlyRootFileSystem: true
      podMonitor:
        enabled: true
        namespace: "{{ .Release.Namespace }}"

File: ./kubernetes/apps/kube-system/reloader/app/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helmrelease.yaml

File: ./kubernetes/apps/kube-system/reloader/ks.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app reloader
  namespace: flux-system
spec:
  targetNamespace: kube-system
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  path: ./kubernetes/apps/kube-system/reloader/app
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/kube-system/namespace.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: kube-system
  labels:
    kustomize.toolkit.fluxcd.io/prune: disabled

File: ./kubernetes/apps/kube-system/coredns/app/kustomizeconfig.yaml
---
nameReference:
  - kind: ConfigMap
    version: v1
    fieldSpecs:
      - path: spec/valuesFrom/name
        kind: HelmRelease

File: ./kubernetes/apps/kube-system/coredns/app/helm-values.yaml
---
fullnameOverride: coredns
k8sAppLabelOverride: kube-dns
serviceAccount:
  create: true
service:
  name: kube-dns
  clusterIP: "10.96.0.10"
servers:
  - zones:
      - zone: .
        scheme: dns://
        use_tcp: true
    port: 53
    plugins:
      - name: errors
      - name: health
        configBlock: |-
          lameduck 5s
      - name: ready
      - name: log
        configBlock: |-
          class error
      - name: prometheus
        parameters: 0.0.0.0:9153
      - name: kubernetes
        parameters: cluster.local in-addr.arpa ip6.arpa
        configBlock: |-
          pods insecure
          fallthrough in-addr.arpa ip6.arpa
      - name: forward
        parameters: . /etc/resolv.conf
      - name: cache
        parameters: 30
      - name: loop
      - name: reload
      - name: loadbalance
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
            - key: node-role.kubernetes.io/control-plane
              operator: Exists
tolerations:
  - key: CriticalAddonsOnly
    operator: Exists
  - key: node-role.kubernetes.io/control-plane
    operator: Exists
    effect: NoSchedule

File: ./kubernetes/apps/kube-system/coredns/app/helmrelease.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: coredns
spec:
  interval: 30m
  chart:
    spec:
      chart: coredns
      version: 1.36.1
      sourceRef:
        kind: HelmRepository
        name: coredns
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  valuesFrom:
    - kind: ConfigMap
      name: coredns-helm-values

File: ./kubernetes/apps/kube-system/coredns/app/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helmrelease.yaml
configMapGenerator:
  - name: coredns-helm-values
    files:
      - values.yaml=./helm-values.yaml
configurations:
  - kustomizeconfig.yaml

File: ./kubernetes/apps/kube-system/coredns/ks.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app coredns
  namespace: flux-system
spec:
  targetNamespace: kube-system
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  path: ./kubernetes/apps/kube-system/coredns/app
  prune: false # never should be deleted
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/kube-system/kubelet-csr-approver/app/kustomizeconfig.yaml
---
nameReference:
  - kind: ConfigMap
    version: v1
    fieldSpecs:
      - path: spec/valuesFrom/name
        kind: HelmRelease

File: ./kubernetes/apps/kube-system/kubelet-csr-approver/app/helm-values.yaml
---
providerRegex: ^(kcp111|kcp112|kcp113|kawg121|kawg122|kawg123|kawg124|kawg125)$
bypassDnsResolution: true

File: ./kubernetes/apps/kube-system/kubelet-csr-approver/app/helmrelease.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kubelet-csr-approver
spec:
  interval: 30m
  chart:
    spec:
      chart: kubelet-csr-approver
      version: 1.2.4
      sourceRef:
        kind: HelmRepository
        name: postfinance
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  valuesFrom:
    - kind: ConfigMap
      name: kubelet-csr-approver-helm-values
  values:
    metrics:
      enable: true
      serviceMonitor:
        enabled: true

File: ./kubernetes/apps/kube-system/kubelet-csr-approver/app/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helmrelease.yaml
configMapGenerator:
  - name: kubelet-csr-approver-helm-values
    files:
      - values.yaml=./helm-values.yaml
configurations:
  - kustomizeconfig.yaml

File: ./kubernetes/apps/kube-system/kubelet-csr-approver/ks.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app kubelet-csr-approver
  namespace: flux-system
spec:
  targetNamespace: kube-system
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  path: ./kubernetes/apps/kube-system/kubelet-csr-approver/app
  prune: false # never should be deleted
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/kube-system/generic-device-plugin/app/helmrelease.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: generic-device-plugin
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 3.6.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  driftDetection:
    mode: enabled
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  values:
    defaultPodOptions:
      priorityClassName: system-node-critical
      tolerations:
        - operator: "Exists"
          effect: "NoExecute"
        - operator: "Exists"
          effect: "NoSchedule"
    controllers:
      generic-device-plugin:
        containers:
          generic-device-plugin:
            image:
              repository: ghcr.io/squat/generic-device-plugin
              tag: latest@sha256:ba6f0b4cf6c858d6ad29ba4d32e4da11638abbc7d96436bf04f582a97b2b8821
            args:
              - --domain
              - kernel.org
              - --device
              - |
                name: tun
                groups:
                  - count: 1000
                    paths:
                      - path: /dev/net/tun
            ports:
              - containerPort: 8080
                name: http
            resources:
              requests:
                cpu: 20m
                memory: 48Mi
              limits:
                cpu: 20m
                memory: 48Mi
            securityContext:
              privileged: true
              readOnlyRootFilesystem: true
    persistence:
      dev:
        type: hostPath
        hostPath: /dev
        globalMounts:
          - path: /dev
      device-plugin:
        type: hostPath
        hostPath: /var/lib/kubelet/device-plugins
        globalMounts:
          - path: /var/lib/kubelet/device-plugins

File: ./kubernetes/apps/kube-system/generic-device-plugin/app/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helmrelease.yaml

File: ./kubernetes/apps/kube-system/generic-device-plugin/ks.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app generic-device-plugin
  namespace: flux-system
spec:
  targetNamespace: kube-system
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  path: ./kubernetes/apps/kube-system/generic-device-plugin/app
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: true # flux dependents
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/kube-system/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./namespace.yaml
  - ./node-feature-discovery/ks.yaml
  - ./cilium/ks.yaml
#  - ./csi-driver-nfs/ks.yaml
  - ./coredns/ks.yaml
  - ./metrics-server/ks.yaml
  - ./reloader/ks.yaml
  - ./kubelet-csr-approver/ks.yaml
  - ./spegel/ks.yaml
  - ./snapshot-controller/ks.yaml
  - ./generic-device-plugin/ks.yaml
  - ./reflector/ks.yaml

File: ./kubernetes/apps/kube-system/cilium/app/kustomizeconfig.yaml
---
nameReference:
  - kind: ConfigMap
    version: v1
    fieldSpecs:
      - path: spec/valuesFrom/name
        kind: HelmRelease

File: ./kubernetes/apps/kube-system/cilium/app/helm-values.yaml
---
autoDirectNodeRoutes: true
bpf:
  masquerade: false # Required for Talos `.machine.features.hostDNS.forwardKubeDNSToHost`
cgroup:
  automount:
    enabled: false
  hostRoot: /sys/fs/cgroup
cluster:
  id: 1
  name: "thepatriots"
cni:
  exclusive: false
# NOTE: devices might need to be set if you have more than one active NIC on your hosts
# devices: eno+ eth+
endpointRoutes:
  enabled: true
envoy:
  enabled: false
hubble:
  enabled: false
ipam:
  mode: kubernetes
ipv4NativeRoutingCIDR: "10.244.0.0/16"
k8sServiceHost: 127.0.0.1
k8sServicePort: 7445
kubeProxyReplacement: true
kubeProxyReplacementHealthzBindAddr: 0.0.0.0:10256
l2announcements:
  enabled: true
loadBalancer:
  algorithm: maglev
  mode: "dsr"
localRedirectPolicy: true
operator:
  replicas: 1
  rollOutPods: true
rollOutCiliumPods: true
routingMode: native
securityContext:
  capabilities:
    ciliumAgent:
      - CHOWN
      - KILL
      - NET_ADMIN
      - NET_RAW
      - IPC_LOCK
      - SYS_ADMIN
      - SYS_RESOURCE
      - DAC_OVERRIDE
      - FOWNER
      - SETGID
      - SETUID
    cleanCiliumState:
      - NET_ADMIN
      - SYS_ADMIN
      - SYS_RESOURCE

File: ./kubernetes/apps/kube-system/cilium/app/helmrelease.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: cilium
spec:
  interval: 30m
  chart:
    spec:
      chart: cilium
      version: 1.16.5
      sourceRef:
        kind: HelmRepository
        name: cilium
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  valuesFrom:
    - kind: ConfigMap
      name: cilium-helm-values
  values:
    hubble:
      enabled: true
      metrics:
        enabled:
          - dns:query
          - drop
          - tcp
          - flow
          - port-distribution
          - icmp
          - http
        serviceMonitor:
          enabled: true
        dashboards:
          enabled: true
          annotations:
            grafana_folder: Cilium
      relay:
        enabled: true
        rollOutPods: true
        prometheus:
          serviceMonitor:
            enabled: true
      ui:
        enabled: true
        rollOutPods: true
        ingress:
          enabled: true
          className: internal
          hosts: ["hubble.${SECRET_DOMAIN}"]
    operator:
      prometheus:
        enabled: true
        serviceMonitor:
          enabled: true
      dashboards:
        enabled: true
        annotations:
          grafana_folder: Cilium
    prometheus:
      enabled: true
      serviceMonitor:
        enabled: true
        trustCRDsExist: true
    dashboards:
      enabled: true
      annotations:
        grafana_folder: Cilium

File: ./kubernetes/apps/kube-system/cilium/app/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helmrelease.yaml
configMapGenerator:
  - name: cilium-helm-values
    files:
      - values.yaml=./helm-values.yaml
configurations:
  - kustomizeconfig.yaml

File: ./kubernetes/apps/kube-system/cilium/ks.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app cilium
  namespace: flux-system
spec:
  targetNamespace: kube-system
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  path: ./kubernetes/apps/kube-system/cilium/app
  prune: false # never should be deleted
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: true
  interval: 30m
  retryInterval: 1m
  timeout: 5m
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app cilium-config
  namespace: flux-system
spec:
  targetNamespace: kube-system
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  dependsOn:
    - name: cilium
  path: ./kubernetes/apps/kube-system/cilium/config
  prune: false # never should be deleted
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/kube-system/cilium/config/cilium-l2.yaml
---
# https://docs.cilium.io/en/latest/network/l2-announcements
apiVersion: cilium.io/v2alpha1
kind: CiliumL2AnnouncementPolicy
metadata:
  name: l2-policy
spec:
  loadBalancerIPs: true
  # NOTE: interfaces might need to be set if you have more than one active NIC on your hosts
  # interfaces:
  #   - ^eno[0-9]+
  #   - ^eth[0-9]+
  nodeSelector:
    matchLabels:
      kubernetes.io/os: linux
---
apiVersion: cilium.io/v2alpha1
kind: CiliumLoadBalancerIPPool
metadata:
  name: l2-pool
spec:
  allowFirstLastIPs: "Yes"
  blocks:
    - cidr: "10.10.12.240/28"

File: ./kubernetes/apps/kube-system/cilium/config/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./cilium-l2.yaml

File: ./kubernetes/apps/cert-manager/namespace.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: cert-manager
  labels:
    kustomize.toolkit.fluxcd.io/prune: disabled

File: ./kubernetes/apps/cert-manager/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./namespace.yaml
  - ./cert-manager/ks.yaml

File: ./kubernetes/apps/cert-manager/cert-manager/app/helmrelease.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: cert-manager
spec:
  interval: 30m
  chart:
    spec:
      chart: cert-manager
      version: v1.16.2
      sourceRef:
        kind: HelmRepository
        name: jetstack
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    installCRDs: true
    dns01RecursiveNameservers: https://1.1.1.1:443/dns-query,https://1.0.0.1:443/dns-query
    dns01RecursiveNameserversOnly: true
    prometheus:
      enabled: true
      servicemonitor:
        enabled: true

File: ./kubernetes/apps/cert-manager/cert-manager/app/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helmrelease.yaml

File: ./kubernetes/apps/cert-manager/cert-manager/ks.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app cert-manager
  namespace: flux-system
spec:
  targetNamespace: cert-manager
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  path: ./kubernetes/apps/cert-manager/cert-manager/app
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: true
  interval: 30m
  retryInterval: 1m
  timeout: 5m
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app cert-manager-issuers
  namespace: flux-system
spec:
  targetNamespace: cert-manager
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  dependsOn:
    - name: cert-manager
  path: ./kubernetes/apps/cert-manager/cert-manager/issuers
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: true
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/cert-manager/cert-manager/issuers/issuers.yaml
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-production
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: "${SECRET_ACME_EMAIL}"
    privateKeySecretRef:
      name: letsencrypt-production
    solvers:
      - dns01:
          cloudflare:
            apiTokenSecretRef:
              name: cert-manager-secret
              key: api-token
        selector:
          dnsZones:
            - "${SECRET_DOMAIN}"
            - "${SECRET_DOMAIN_CHAT}"
            - "${SECRET_DOMAIN_LEGACY}"
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-staging
spec:
  acme:
    server: https://acme-staging-v02.api.letsencrypt.org/directory
    email: "${SECRET_ACME_EMAIL}"
    privateKeySecretRef:
      name: letsencrypt-staging
    solvers:
      - dns01:
          cloudflare:
            apiTokenSecretRef:
              name: cert-manager-secret
              key: api-token
        selector:
          dnsZones:
            - "${SECRET_DOMAIN}"
            - "${SECRET_DOMAIN_CHAT}"
            - "${SECRET_DOMAIN_LEGACY}"

File: ./kubernetes/apps/cert-manager/cert-manager/issuers/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
api-token:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/cert-manager/cert-manager/issuers/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./secret.sops.yaml
  - ./issuers.yaml

File: ./kubernetes/apps/smtp-relay/app/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
SMTP_RELAY_SERVER:
RELAYHOST_USERNAME:
RELAYHOST_PASSWORD:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/smtp-relay/app/helm-release.yaml
---
# yaml-language-server: $schema=https://flux.jank.ing/helmrelease/v2/github/bjw-s/helm-charts/common-3.5.1/charts/library/common
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: smtp-relay
  namespace: default
spec:
  interval: 15m
  chart:
    spec:
      chart: app-template
      version: 3.6.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  install:
    createNamespace: true
    remediation:
      retries: 3
    timeout: 5m
  upgrade:
    remediation:
      retries: 3
    cleanupOnFail: true
  values:
    controllers:
      smtp-relay:
        strategy: RollingUpdate
        annotations:
          reloader.stakater.com/auto: "true"
        containers:
          main:
            image:
              repository: registry.skysolutions.fi/docker.io/boky/postfix
              tag: 4.3.0-debian
            env:
              RELAYHOST: "smtp.office365.com:587"
              ALLOW_EMPTY_SENDER_DOMAINS: true
              POSTFIX_myhostname: mailer.k8s.${SECRET_DOMAIN}
              POSTFIX_mynetworks: 10.244.0.0/16
              POSTFIX_inet_protocols: all
            envFrom:
              - secretRef:
                  name: smtp-relay
            resources:
              requests:
                cpu: 100m
                memory: 64M
              limits:
                memory: 64M
            probes:
              liveness:
                enabled: false
              startup:
                enabled: false
              readiness:
                enabled: false

    service:
      main:
        controller: smtp-relay
        type: LoadBalancer
        ports:
          http:
            port: 587

    persistence:
      spool:
        type: emptyDir
        globalMounts:
          - path: /var/spool/postfix

File: ./kubernetes/apps/smtp-relay/app/kustomization.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/kustomization
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./secret.sops.yaml
  - ./helm-release.yaml


File: ./kubernetes/apps/smtp-relay/ks.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app smtp-relay
  namespace: flux-system
spec:
  path: "./kubernetes/apps/smtp-relay/app"
  targetNamespace: default
  sourceRef:
    kind: GitRepository
    name: thepatriots
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  prune: true
  wait: false
  interval: 10m
  postBuild:
    substitute:
      APP: *app
      APP_UID: "1000"
      APP_GID: "1000"

File: ./kubernetes/apps/emulation/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: emulation-pvc-v1
  namespace: emulation
spec:
  resources:
    requests:
      storage: 30Ti
  accessModes:
    - ReadWriteMany
  storageClassName: cephfs-media-ec

File: ./kubernetes/apps/emulation/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./ns.yaml
#  - ./pvc.yaml
#  - ./retrom/ks.yaml

File: ./kubernetes/apps/emulation/retrom/app/helmrelease.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: retrom
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 3.6.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  maxHistory: 2
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  uninstall:
    keepHistory: false
  values:
    controllers:
      retrom:
        annotations:
          reloader.stakater.com/auto: "true"
        initContainers:
          init-db:
            image:
              repository: ghcr.io/onedr0p/postgres-init
              tag: 16
            envFrom:
              - secretRef:
                  name: retrom-init-db
        containers:
          retrom:
            image:
              repository: ghcr.io/jmberesford/retrom-service
              tag: retrom-v0.4.4
            resources:
              requests:
                cpu: 250m
                memory: 512Mi
              limits:
                memory: 1Gi
    service:
      app:
        controller: retrom
        type: LoadBalancer
        ports:
          http:
            port: 3000
          svc:
            port: 5101
    ingress:
      internal:
        enabled: true
        className: internal
        hosts:
          - host: retrom.${SECRET_DOMAIN_LEGACY}
            paths:
              - path: /
                service:
                  identifier: app
                  port: http
        tls:
          - hosts:
              - retrom.${SECRET_DOMAIN_LEGACY}
    persistence:
      config:
        type: secret
        name: retrom-config
        globalMounts:
          - path: /config/config.json
            subPath: config.json
            readOnly: true
      data:
        existingClaim: emulation-pvc-v1
        globalMounts:
          - path: /library

File: ./kubernetes/apps/emulation/retrom/app/kustomization.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/kustomization
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./secret-db-init.sops.yaml
  - ./helmrelease.yaml
secretGenerator:
  - name: retrom-config
    files:
      - config.json=./resources/config.sops.json
generatorOptions:
  disableNameSuffixHash: true

File: ./kubernetes/apps/emulation/retrom/app/.sops.yaml
creation_rules:
key_groups:
encrypted_regex:
key_groups:

File: ./kubernetes/apps/emulation/retrom/app/secret-db-init.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
INIT_POSTGRES_DBNAME:
INIT_POSTGRES_HOST:
INIT_POSTGRES_USER:
INIT_POSTGRES_PASS:
INIT_POSTGRES_SUPER_PASS:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/apps/emulation/retrom/ks.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app retrom
  namespace: flux-system
spec:
  targetNamespace: emulation
  commonMetadata:
    labels:
      app.kubernetes.io/name: *app
  path: ./kubernetes/apps/emulation/retrom/app
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: false
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./kubernetes/apps/emulation/ns.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: emulation

File: ./kubernetes/flux/repositories/oci/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources: []

File: ./kubernetes/flux/repositories/helm/node-feature-discovery.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/helmrepository-source-v1beta2.json
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: node-feature-discovery
  namespace: flux-system
spec:
  interval: 2h
  url: https://kubernetes-sigs.github.io/node-feature-discovery/charts

File: ./kubernetes/flux/repositories/helm/jetstack.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: jetstack
  namespace: flux-system
spec:
  interval: 1h
  url: https://charts.jetstack.io

File: ./kubernetes/flux/repositories/helm/coredns.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: coredns
  namespace: flux-system
spec:
  interval: 1h
  url: https://coredns.github.io/helm

File: ./kubernetes/flux/repositories/helm/postfinance.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: postfinance
  namespace: flux-system
spec:
  interval: 1h
  url: https://postfinance.github.io/kubelet-csr-approver

File: ./kubernetes/flux/repositories/helm/democratic-csi.yaml
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: democratic-csi
  namespace: flux-system
spec:
  interval: 30m
  url: https://democratic-csi.github.io/charts/
  timeout: 3m

File: ./kubernetes/flux/repositories/helm/bjw-s.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: bjw-s
  namespace: flux-system
spec:
  type: oci
  interval: 5m
  url: oci://ghcr.io/bjw-s/helm

File: ./kubernetes/flux/repositories/helm/intel.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/helmrepository-source-v1beta2.json
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: intel
  namespace: flux-system
spec:
  interval: 2h
  url: https://intel.github.io/helm-charts

File: ./kubernetes/flux/repositories/helm/openebs.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: openebs
  namespace: flux-system
spec:
  interval: 1h
  url: https://openebs.github.io/openebs

File: ./kubernetes/flux/repositories/helm/bitnami.yaml
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: bitnami
  namespace: flux-system
spec:
  interval: 1h
  url: https://charts.bitnami.com/bitnami

File: ./kubernetes/flux/repositories/helm/emberstack.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: emberstack
  namespace: flux-system
spec:
  interval: 30m
  url: https://emberstack.github.io/helm-charts/
  timeout: 3m

File: ./kubernetes/flux/repositories/helm/goauthentik-charts.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: authentik-charts
  namespace: flux-system
spec:
  interval: 15m
  url: https://charts.goauthentik.io/
  timeout: 3m

File: ./kubernetes/flux/repositories/helm/backube.yaml
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: backube
  namespace: flux-system
spec:
  interval: 10m
  url: https://backube.github.io/helm-charts

File: ./kubernetes/flux/repositories/helm/angelnu-charts.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: angelnu-charts
  namespace: flux-system
spec:
  interval: 30m
  url: https://angelnu.github.io/helm-charts
  timeout: 3m

File: ./kubernetes/flux/repositories/helm/stakater.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: stakater
  namespace: flux-system
spec:
  type: oci
  interval: 5m
  url: oci://ghcr.io/stakater/charts

File: ./kubernetes/flux/repositories/helm/zammad.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: zammad
  namespace: flux-system
spec:
  interval: 1h
  url: https://zammad.github.io/zammad-helm

File: ./kubernetes/flux/repositories/helm/invoiceninja-release.yaml
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: invoiceninja-release
  namespace: flux-system
spec:
  interval: 10m
  url: https://invoiceninja.github.io/dockerfiles

File: ./kubernetes/flux/repositories/helm/ingress-nginx.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: ingress-nginx
  namespace: flux-system
spec:
  interval: 1h
  url: https://kubernetes.github.io/ingress-nginx

File: ./kubernetes/flux/repositories/helm/metrics-server.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: metrics-server
  namespace: flux-system
spec:
  interval: 1h
  url: https://kubernetes-sigs.github.io/metrics-server

File: ./kubernetes/flux/repositories/helm/ananace-charts.yaml
apiVersion: source.toolkit.fluxcd.io/v1beta1
kind: HelmRepository
metadata:
  name: ananace-charts
  namespace: flux-system
spec:
  interval: 1h
  url: https://ananace.gitlab.io/charts/

File: ./kubernetes/flux/repositories/helm/piraeus-charts.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: piraeus-charts
  namespace: flux-system
spec:
  interval: 1h
  url: https://piraeus.io/helm-charts/
  timeout: 3m

File: ./kubernetes/flux/repositories/helm/external-dns.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: external-dns
  namespace: flux-system
spec:
  interval: 1h
  url: https://kubernetes-sigs.github.io/external-dns

File: ./kubernetes/flux/repositories/helm/nfs-subdir-external-provisioner.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: nfs-subdir-external-provisioner
  namespace: flux-system
spec:
  interval: 6h
  url: https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/
  timeout: 3m

File: ./kubernetes/flux/repositories/helm/cloudnative-pg.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: cloudnative-pg
  namespace: flux-system
spec:
  interval: 30m
  url: https://cloudnative-pg.github.io/charts
  timeout: 3m

File: ./kubernetes/flux/repositories/helm/cilium.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: cilium
  namespace: flux-system
spec:
  interval: 1h
  url: https://helm.cilium.io

File: ./kubernetes/flux/repositories/helm/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./authentik.yaml
  - ./backube.yaml
  - ./bitnami.yaml
  - ./bjw-s.yaml
  - ./cilium.yaml
  - ./coredns.yaml
  - ./csi-driver-nfs.yaml
  - ./nfs-subdir-external-provisioner.yaml
  - ./cloudnative-pg.yaml
  - ./democratic-csi.yaml
  - ./jetstack.yaml
  - ./mariadb-operator.yaml
  - ./metrics-server.yaml
  - ./openebs.yaml
  - ./postfinance.yaml
  - ./prometheus-community.yaml
  - ./piraeus-charts.yaml
  - ./spegel.yaml
  - ./stakater.yaml
  - ./external-dns.yaml
  - ./ingress-nginx.yaml
  - ./k8s-gateway.yaml
  - ./rook-release.yaml
  - ./invoiceninja-release.yaml
  - ./zammad.yaml
  - ./node-feature-discovery.yaml
  - ./intel.yaml
  - ./angelnu-charts.yaml
  - ./ananace-charts.yaml
  - ./emberstack.yaml
  - ./goauthentik-charts.yaml

File: ./kubernetes/flux/repositories/helm/rook-release.yaml
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: rook-release
  namespace: flux-system
spec:
  interval: 10m
  url: https://charts.rook.io/release

File: ./kubernetes/flux/repositories/helm/spegel.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: spegel
  namespace: flux-system
spec:
  type: oci
  interval: 5m
  url: oci://ghcr.io/spegel-org/helm-charts

File: ./kubernetes/flux/repositories/helm/csi-driver-nfs.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: csi-driver-nfs
  namespace: flux-system
spec:
  interval: 6h
  url: https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/master/charts/
  timeout: 3m

File: ./kubernetes/flux/repositories/helm/k8s-gateway.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: k8s-gateway
  namespace: flux-system
spec:
  interval: 1h
  url: https://ori-edge.github.io/k8s_gateway

File: ./kubernetes/flux/repositories/helm/authentik.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: authentik
  namespace: flux-system
spec:
  interval: 1h
  url: https://charts.goauthentik.io/

File: ./kubernetes/flux/repositories/helm/prometheus-community.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: prometheus-community
  namespace: flux-system
spec:
  type: oci
  interval: 5m
  url: oci://ghcr.io/prometheus-community/charts

File: ./kubernetes/flux/repositories/helm/mariadb-operator.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: mariadb-operator
  namespace: flux-system
spec:
  interval: 1h
  url: https://helm.mariadb.com/mariadb-operator

File: ./kubernetes/flux/repositories/git/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources: []

File: ./kubernetes/flux/repositories/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./git
  - ./helm
  - ./oci

File: ./kubernetes/flux/vars/cluster-settings.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-settings
  namespace: flux-system
data:
  SETTING_EXAMPLE: Global settings for your cluster go in this file, this file is NOT encrypted

File: ./kubernetes/flux/vars/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./cluster-settings.yaml
  - ./cluster-secrets.sops.yaml

File: ./kubernetes/flux/vars/cluster-secrets.sops.yaml
apiVersion:
kind:
metadata:
name:
namespace:
stringData:
SECRET_DOMAIN:
SECRET_ACME_EMAIL:
SECRET_CLOUDFLARE_TUNNEL_ID:
SECRET_INVNINJA_EMAIL:
COMPANY_NAME:
SECRET_DOMAIN_LEGACY:
SECRET_DOMAIN_COMP:
SECRET_DOMAIN_CHAT:
SECRET_AUTHENTIK_SECRET_KEY:
SECRET_AUTHENTIK_POSTGRES_PASSWORD:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./kubernetes/flux/config/flux.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1beta2
kind: OCIRepository
metadata:
  name: flux-manifests
  namespace: flux-system
spec:
  interval: 10m
  url: oci://ghcr.io/fluxcd/flux-manifests
  ref:
    tag: v2.4.0
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: flux
  namespace: flux-system
spec:
  interval: 10m
  path: ./
  prune: true
  wait: true
  sourceRef:
    kind: OCIRepository
    name: flux-manifests
  patches:
    # Remove the network policies
    - patch: |
        $patch: delete
        apiVersion: networking.k8s.io/v1
        kind: NetworkPolicy
        metadata:
          name: not-used
      target:
        group: networking.k8s.io
        kind: NetworkPolicy
    # Increase the number of reconciliations that can be performed in parallel and bump the resources limits
    # https://fluxcd.io/flux/cheatsheets/bootstrap/#increase-the-number-of-workers
    - patch: |
        - op: add
          path: /spec/template/spec/containers/0/args/-
          value: --concurrent=8
        - op: add
          path: /spec/template/spec/containers/0/args/-
          value: --kube-api-qps=500
        - op: add
          path: /spec/template/spec/containers/0/args/-
          value: --kube-api-burst=1000
        - op: add
          path: /spec/template/spec/containers/0/args/-
          value: --requeue-dependency=5s
      target:
        kind: Deployment
        name: (kustomize-controller|helm-controller|source-controller)
    - patch: |
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: not-used
        spec:
          template:
            spec:
              containers:
                - name: manager
                  resources:
                    limits:
                      cpu: 2000m
                      memory: 2Gi
      target:
        kind: Deployment
        name: (kustomize-controller|helm-controller|source-controller)
    # Enable Helm near OOM detection
    # https://fluxcd.io/flux/cheatsheets/bootstrap/#enable-helm-near-oom-detection
    - patch: |
        - op: add
          path: /spec/template/spec/containers/0/args/-
          value: --feature-gates=OOMWatch=true
        - op: add
          path: /spec/template/spec/containers/0/args/-
          value: --oom-watch-memory-threshold=95
        - op: add
          path: /spec/template/spec/containers/0/args/-
          value: --oom-watch-interval=500ms
      target:
        kind: Deployment
        name: helm-controller

File: ./kubernetes/flux/config/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./flux.yaml
  - ./cluster.yaml

File: ./kubernetes/flux/config/cluster.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: GitRepository
metadata:
  name: thepatriots
  namespace: flux-system
spec:
  interval: 30m
  url: "https://github.com/Deenyoro/AG-Talos-Kubernetes"
  ref:
    branch: "main"
  ignore: |
    # exclude all
    /*
    # include kubernetes directory
    !/kubernetes
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: cluster
  namespace: flux-system
spec:
  interval: 30m
  path: ./kubernetes/flux
  prune: true
  wait: false
  sourceRef:
    kind: GitRepository
    name: thepatriots
  decryption:
    provider: sops
    secretRef:
      name: sops-age
  postBuild:
    substituteFrom:
      - kind: ConfigMap
        name: cluster-settings
      - kind: Secret
        name: cluster-secrets
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: cluster-apps
  namespace: flux-system
spec:
  interval: 10m0s
  retryInterval: 2m0s
  timeout: 5m
  path: ./kubernetes/apps
  prune: true
  wait: false
  sourceRef:
    kind: GitRepository
    name: thepatriots
  decryption:
    provider: sops
    secretRef:
      name: sops-age
  postBuild:
    substitute: {}
    substituteFrom:
      - kind: ConfigMap
        name: cluster-settings
        optional: false
      - kind: Secret
        name: cluster-secrets
        optional: false
  patches:
    - patch: |-
        apiVersion: kustomize.toolkit.fluxcd.io/v1
        kind: Kustomization
        metadata:
          name: not-used
          namespace: not-used
        spec:
          interval: 10m0s
          retryInterval: 2m0s
          timeout: 5m
          wait: true
          sourceRef:
            kind: GitRepository
            name: thepatriots
          decryption:
            provider: sops
            secretRef:
              name: sops-age
          patches:
            - patch: |-
                apiVersion: helm.toolkit.fluxcd.io/v2
                kind: HelmRelease
                metadata:
                  name: not-used
                  namespace: not-used
                spec:
                  interval: 10m
                  timeout: 5m
                  install:
                    createNamespace: true
                    remediation:
                      retries: 5
                  upgrade:
                    remediation:
                      retries: 5
              target:
                kind: HelmRelease
                group: helm.toolkit.fluxcd.io
                version: v2
          postBuild:
            substitute: {}
            substituteFrom:
              - kind: ConfigMap
                name: cluster-settings
                optional: false
              - kind: Secret
                name: cluster-secrets
                optional: false
      target:
        kind: Kustomization
        group: kustomize.toolkit.fluxcd.io
        version: v1

File: ./kubernetes/flux/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./config
  - ./repositories
  - ./vars

File: ./.archive/database/mariadb/operator/app/helm-release.yaml
---
apiVersion: helm.toolkit.fluxcd.io/v2beta2
kind: HelmRelease
metadata:
  name: mariadb-operator
  namespace: database
spec:
  interval: 30m
  chart:
    spec:
      chart: mariadb-operator
      version: 0.24.0
      sourceRef:
        kind: HelmRepository
        name: mariadb-operator
        namespace: flux-system
      interval: 30m
  values:
    image:
      repository: ghcr.io/mariadb-operator/mariadb-operator
      pullPolicy: IfNotPresent
      tag: v0.0.24
    metrics:
      enabled: true
      serviceMonitor:
        enabled: true
    webhook:
      certificate:
        certManager: false
      serviceMonitor:
        enabled: true

File: ./.archive/database/mariadb/operator/app/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helm-release.yaml

File: ./.archive/database/mariadb/operator/ks.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: mariadb-operator
  namespace: flux-system
spec:
  targetNamespace: database
  path: ./kubernetes/apps/database/mariadb/operator/app
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  dependsOn:
    - name: mariadb-operator-crds
  wait: false
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./.archive/database/mariadb/cluster/app/server.yaml
apiVersion: k8s.mariadb.com/v1alpha1
kind: MariaDB
metadata:
  name: mariadb-galera
spec:
  maxScaleRef:
    name: maxscale-galera
    namespace: database

  rootPasswordSecretKeyRef:
    name: mariadb-secret
    key: password
    generate: false

  storage:
    size: 1Gi

  replicas: 3

  galera:
    enabled: true

  myCnf: |
    [mariadb]
    bind-address=*
    default_storage_engine=InnoDB
    binlog_format=row
    innodb_autoinc_lock_mode=2
    innodb_buffer_pool_size=1024M
    max_allowed_packet=256M

  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      memory: 1Gi

  livenessProbe:
    initialDelaySeconds: 20
    periodSeconds: 10
    timeoutSeconds: 10

  readinessProbe:
    initialDelaySeconds: 20
    periodSeconds: 10
    timeoutSeconds: 10

  metrics:
    enabled: true

File: ./.archive/database/mariadb/cluster/app/maxscale.yaml
apiVersion: k8s.mariadb.com/v1alpha1
kind: MaxScale
metadata:
  name: maxscale-galera
spec:
  replicas: 3

  mariaDbRef:
    name: mariadb-galera
    namespace: database

  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      memory: 1Gi

File: ./.archive/database/mariadb/cluster/app/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
password:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./.archive/database/mariadb/cluster/app/kustomization.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/kustomization
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./secret.sops.yaml
  - ./server.yaml
  - ./maxscale.yaml
  - ./provisions

File: ./.archive/database/mariadb/cluster/app/provisions/zabbix/user.yaml
---
apiVersion: k8s.mariadb.com/v1alpha1
kind: User
metadata:
  name: zabbix
spec:
  mariaDbRef:
    name: mariadb-galera
  passwordSecretKeyRef:
    name: zabbix-db-credentials
    key: password
  maxUserConnections: 20

File: ./.archive/database/mariadb/cluster/app/provisions/zabbix/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
password:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./.archive/database/mariadb/cluster/app/provisions/zabbix/db.yaml
---
apiVersion: k8s.mariadb.com/v1alpha1
kind: Database
metadata:
  name: zabbix
spec:
  mariaDbRef:
    name: mariadb-galera
  characterSet: utf8
  collate: utf8_general_ci

File: ./.archive/database/mariadb/cluster/app/provisions/zabbix/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - secret.sops.yaml
  - db.yaml
  - grant.yaml
  - user.yaml

File: ./.archive/database/mariadb/cluster/app/provisions/zabbix/grant.yaml
---
apiVersion: k8s.mariadb.com/v1alpha1
kind: Grant
metadata:
  name: zabbix
spec:
  mariaDbRef:
    name: mariadb-galera
  privileges:
    - 'ALL'
  database: 'zabbix'
  table: '*'
  username: zabbix

File: ./.archive/database/mariadb/cluster/app/provisions/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - zabbix

File: ./.archive/database/mariadb/cluster/ks.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: mariadb
  namespace: flux-system
spec:
  targetNamespace: database
  path: ./kubernetes/apps/database/mariadb/cluster/app
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  dependsOn:
    - name: mariadb-operator
  wait: true
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./.archive/database/mariadb/operator-crds/app/helm-release.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app mariadb-operator-crds
spec:
  interval: 30m
  chart:
    spec:
      chart: mariadb-operator-crds
      version: 0.0.33
      sourceRef:
        kind: HelmRepository
        name: mariadb-operator
        namespace: flux-system
  maxHistory: 2
  install:
    crds: CreateReplace
    remediation:
      retries: 3
  upgrade:
    crds: CreateReplace
    cleanupOnFail: true
    remediation:
      retries: 3
  uninstall:
    keepHistory: false

File: ./.archive/database/mariadb/operator-crds/app/kustomization.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/kustomization
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./helm-release.yaml

File: ./.archive/database/mariadb/operator-crds/ks.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/kustomization-kustomize-v1.json
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: mariadb-operator-crds
  namespace: flux-system
spec:
  targetNamespace: database
  path: ./kubernetes/apps/database/mariadb/operator-crds/app
  prune: true
  sourceRef:
    kind: GitRepository
    name: thepatriots
  wait: true
  interval: 30m
  retryInterval: 1m
  timeout: 5m

File: ./.archive/database/mariadb/kustomization.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/kustomization
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./operator-crds/ks.yaml
  - ./operator/ks.yaml
  - ./cluster/ks.yaml

File: ./.archive/minio-system/minio/app/secret.sops.yaml
apiVersion:
kind:
metadata:
name:
stringData:
MINIO_ROOT_USER:
MINIO_ROOT_PASSWORD:
sops:
kms:
gcp_kms:
azure_kv:
hc_vault:
age:
enc:
enc:
lastmodified:
mac:
pgp:
encrypted_regex:
version:

File: ./.archive/minio-system/minio/app/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: minio-pvc-v1
spec:
  accessModes: ["ReadWriteOnce"]
  resources:
    requests:
      storage: 30Gi
  storageClassName: cephfs

File: ./.archive/minio-system/minio/app/helmrelease.yaml
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: minio
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 3.5.1
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  values:
    controllers:
      minio:
        annotations:
          reloader.stakater.com/auto: "true"
        containers:
          app:
            image:
              repository: quay.io/minio/minio
              tag: RELEASE.2024-10-13T13-34-11Z@sha256:9535594ad4122b7a78c6632788a989b96d9199b483d3bd71a5ceae73a922cdfa
            env:
              MINIO_API_CORS_ALLOW_ORIGIN: https://minio.${SECRET_DOMAIN},https://minio.minio-system.svc.cluster.local,https://s3.${SECRET_DOMAIN}
              MINIO_BROWSER_REDIRECT_URL: https://minio.${SECRET_DOMAIN}
              MINIO_PROMETHEUS_AUTH_TYPE: public
              MINIO_SERVER_URL: https://s3.${SECRET_DOMAIN}
              MINIO_UPDATE: "off"
            envFrom:
              - secretRef:
                  name: minio-secret
            args: ["server", "/data", "--console-address", ":9001"]
            probes:
              liveness: &probes
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /minio/health/live
                    port: 9000
                  initialDelaySeconds: 30
                  periodSeconds: 30
                  timeoutSeconds: 10
                  failureThreshold: 6
              readiness: *probes
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              capabilities: { drop: ["ALL"] }
            resources:
              requests:
                cpu: 100m
              limits:
                memory: 2Gi
    defaultPodOptions:
      securityContext:
        runAsNonRoot: true
        runAsUser: 568
        runAsGroup: 568
        fsGroup: 568
        fsGroupChangePolicy: OnRootMismatch
        supplementalGroups: [10000]
        seccompProfile: { type: RuntimeDefault }
    service:
      app:
        controller: minio
        ports:
          http:
            port: &port-console 9001
          s3:
            port: &port-api 9000
    serviceMonitor:
      app:
        serviceName: minio
        endpoints:
          - port: s3
            scheme: http
            path: /minio/v2/metrics/cluster
            interval: 1m
            scrapeTimeout: 10s
    ingress:
      main:
        className: internal
        annotations:
          hajimari.io/enable: "true"
          hajimari.io/appName: "Minio Console"
          hajimari.io/group: "storage"
          hajimari.io/icon: mdi:pail
        hosts:
          - host: &host-console "{{ .Release.Name }}.${SECRET_DOMAIN}"
            paths:
              - path: /
                pathType: Prefix
                service:
                  identifier: app
                  port: *port-console
        tls:
          - hosts:
              - *host-console

      s3:
        className: internal
        annotations:
          nginx.ingress.kubernetes.io/proxy-connect-timeout: "300"
          nginx.ingress.kubernetes.io/proxy-body-size: 5000m
          nginx.ingress.kubernetes.io/proxy-request-buffering: "off"
          nginx.ingress.kubernetes.io/configuration-snippet: |
            chunked_transfer_encoding off;
          hajimari.io/enable: "false"
        hosts:
          - host: &host-api "s3.${SECRET_DOMAIN}"
            paths:
              - path: /
                service:
                  identifier: app
                  port: http
        tls:
          - hosts:
              - *host-api
    persistence:
      data:
        existingClaim: minio-pvc-v1
        globalMounts:
          - path: /data

File: ./.archive/minio-system/minio/app/kustomization.yaml
---
# yaml-language-server: $schema=https://json.schemastore.org/kustomization
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ./secret.sops.yaml
  - ./pvc.yaml
  - ./helmrelease.yaml

File: ./.archive/minio-system/minio/ks.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: minio
  namespace: flux-system
spec:
  targetNamespace: minio-system
  path: ./kubernetes/apps/minio-system/minio/app/
  prune: false
  sourceRef:
    kind: GitRepository
    name: thepatriots
  interval: 30m
  retryInterval: 1m
  timeout: 3m
  healthChecks:
    - apiVersion: helm.toolkit.fluxcd.io/v2beta2
      kind: HelmRelease
      name: minio
      namespace: minio-system
  dependsOn:
    - name: rook-ceph-cluster

File: ./.archive/minio-system/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ns.yaml
  - minio/ks.yaml

File: ./.archive/minio-system/ns.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: minio-system
  annotations:
    volsync.backube/privileged-movers: "true"

File: ./.taskfiles/Talos/Taskfile.yaml
---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: "3"

vars:
  TALHELPER_CLUSTER_DIR: "{{.KUBERNETES_DIR}}/bootstrap/talos/clusterconfig"
  TALHELPER_SECRET_FILE: "{{.KUBERNETES_DIR}}/bootstrap/talos/talsecret.sops.yaml"
  TALHELPER_CONFIG_FILE: "{{.KUBERNETES_DIR}}/bootstrap/talos/talconfig.yaml"
  HELMFILE_FILE: "{{.KUBERNETES_DIR}}/bootstrap/helmfile.yaml"
  TALOSCONFIG_FILE: "{{.TALHELPER_CLUSTER_DIR}}/talosconfig"

env:
  TALOSCONFIG: "{{.TALOSCONFIG_FILE}}"

tasks:

  apply-config:
    desc: Apply configs to Talos cluster
    dir: "{{.KUBERNETES_DIR}}/bootstrap/talos"
    cmds:
      - |
        if [ ! -f "{{.TALHELPER_SECRET_FILE}}" ]; then
            talhelper gensecret > {{.TALHELPER_SECRET_FILE}}
            sops --encrypt --in-place {{.TALHELPER_SECRET_FILE}}
        fi
      - talhelper genconfig --config-file {{.TALHELPER_CONFIG_FILE}} --secret-file {{.TALHELPER_SECRET_FILE}} --out-dir {{.TALHELPER_CLUSTER_DIR}}
      - talhelper gencommand apply --config-file {{.TALHELPER_CONFIG_FILE}} --out-dir {{.TALHELPER_CLUSTER_DIR}} | bash
    preconditions:
      - msg: Missing talhelper config file
        sh: test -f {{.TALHELPER_CONFIG_FILE}}
      - msg: Missing Sops config file
        sh: test -f {{.SOPS_CONFIG_FILE}}
      - msg: Missing Sops Age key file
        sh: test -f {{.AGE_FILE}}

  generate:
    desc: Generate Talhelper config
    dir: "{{.KUBERNETES_DIR}}/bootstrap/talos"
    cmds:
      - |
        if [ ! -f "{{.TALHELPER_SECRET_FILE}}" ]; then
            talhelper gensecret > {{.TALHELPER_SECRET_FILE}}
            sops --encrypt --in-place {{.TALHELPER_SECRET_FILE}}
        fi
      - talhelper genconfig --config-file {{.TALHELPER_CONFIG_FILE}} --out-dir {{.TALHELPER_CLUSTER_DIR}}
    preconditions:
      - msg: Missing talhelper config file
        sh: test -f {{.TALHELPER_CONFIG_FILE}}
      - msg: Missing Sops config file
        sh: test -f {{.SOPS_CONFIG_FILE}}
      - msg: Missing Sops Age key file
        sh: test -f {{.AGE_FILE}}


  bootstrap:
    desc: Bootstrap the Talos cluster
    dir: "{{.KUBERNETES_DIR}}/bootstrap/talos"
    cmds:
      - |
        if [ ! -f "{{.TALHELPER_SECRET_FILE}}" ]; then
            talhelper gensecret > {{.TALHELPER_SECRET_FILE}}
            sops --encrypt --in-place {{.TALHELPER_SECRET_FILE}}
        fi
      - talhelper gencommand apply --config-file {{.TALHELPER_CONFIG_FILE}} --out-dir {{.TALHELPER_CLUSTER_DIR}} --extra-flags="--insecure" | bash
      - until talhelper gencommand bootstrap --config-file {{.TALHELPER_CONFIG_FILE}} --out-dir {{.TALHELPER_CLUSTER_DIR}} | bash; do sleep 10; done
      - task: fetch-kubeconfig
      - task: install-helm-apps
      - talosctl health --server=false
    preconditions:
      - msg: Missing talhelper config file
        sh: test -f {{.TALHELPER_CONFIG_FILE}}
      - msg: Missing Sops config file
        sh: test -f {{.SOPS_CONFIG_FILE}}
      - msg: Missing Sops Age key file
        sh: test -f {{.AGE_FILE}}

  fetch-kubeconfig:
    desc: Fetch kubeconfig
    dir: "{{.KUBERNETES_DIR}}/bootstrap/talos"
    cmd: until talhelper gencommand kubeconfig --config-file {{.TALHELPER_CONFIG_FILE}} --out-dir {{.TALHELPER_CLUSTER_DIR}} --extra-flags="{{.ROOT_DIR}} --force" | bash; do sleep 10; done
    preconditions:
      - msg: Missing talhelper config file
        sh: test -f {{.TALHELPER_CONFIG_FILE}}

  install-helm-apps:
    desc: Bootstrap core apps needed for Talos
    dir: "{{.KUBERNETES_DIR}}/bootstrap/talos"
    cmds:
      - until kubectl --kubeconfig {{.KUBECONFIG_FILE}} wait --for=condition=Ready=False nodes --all --timeout=600s; do sleep 10; done
      - helmfile --kubeconfig {{.KUBECONFIG_FILE}} --file {{.HELMFILE_FILE}} apply --skip-diff-on-install --suppress-diff
      - until kubectl --kubeconfig {{.KUBECONFIG_FILE}} wait --for=condition=Ready nodes --all --timeout=600s; do sleep 10; done
    preconditions:
      - msg: Missing kubeconfig
        sh: test -f {{.KUBECONFIG_FILE}}
      - msg: Missing helmfile
        sh: test -f {{.HELMFILE_FILE}}

  upgrade:
    desc: Upgrade Talos on a node
    dir: "{{.KUBERNETES_DIR}}/bootstrap/talos"
    cmds:
      - talosctl --nodes {{.node}} upgrade --image {{.image}} --wait=true --timeout=10m --preserve=true --reboot-mode={{.mode}}
      - talosctl --nodes {{.node}} health --wait-timeout=10m --server=false
    vars:
      mode: '{{.mode | default "default"}}'
    requires:
      vars: ["node", "image"]
    preconditions:
      - msg: Missing talosconfig
        sh: test -f {{.TALOSCONFIG_FILE}}
      - msg: Unable to retrieve Talos config
        sh: talosctl config info >/dev/null 2>&1
      - msg: Node not found
        sh: talosctl --nodes {{.node}} get machineconfig >/dev/null 2>&1

  upgrade-k8s:
    desc: Upgrade Kubernetes across the cluster
    dir: "{{.KUBERNETES_DIR}}/bootstrap/talos"
    cmd: talosctl --nodes {{.controller}} upgrade-k8s --to {{.to}}
    requires:
      vars: ["controller", "to"]
    preconditions:
      - msg: Missing talosconfig
        sh: test -f {{.TALOSCONFIG_FILE}}
      - msg: Unable to retrieve Talos config
        sh: talosctl config info >/dev/null 2>&1
      - msg: Node not found
        sh: talosctl --nodes {{.controller}} get machineconfig >/dev/null 2>&1

  nuke:
    desc: Resets nodes back to maintenance mode
    dir: "{{.KUBERNETES_DIR}}/bootstrap/talos"
    prompt: This will destroy your cluster and reset the nodes back to maintenance mode... continue?
    cmd: talhelper gencommand reset --config-file {{.TALHELPER_CONFIG_FILE}} --out-dir {{.TALHELPER_CLUSTER_DIR}} --extra-flags="--reboot {{- if eq .CLI_FORCE false }} --system-labels-to-wipe STATE --system-labels-to-wipe EPHEMERAL{{ end }} --graceful=false --wait=false" | bash

  .reset:
    internal: true
    cmd: rm -rf {{.TALHELPER_CLUSTER_DIR}} {{.TALHELPER_SECRET_FILE}} {{.TALHELPER_CONFIG_FILE}}

File: ./.taskfiles/Sops/Taskfile.yaml
---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: "3"

tasks:

  age-keygen:
    desc: Initialize Age Key for Sops
    cmd: age-keygen --output {{.AGE_FILE}}
    status: ["test -f {{.AGE_FILE}}"]

  encrypt:
    desc: Encrypt all Kubernetes SOPS secrets
    cmds:
      - for: { var: file }
        task: .encrypt-file
        vars:
          file: "{{.ITEM}}"
    vars:
      file:
        sh: find "{{.KUBERNETES_DIR}}" -type f -name "*.sops.*" -exec grep -L "ENC\[AES256_GCM" {} \;

  .encrypt-file:
    internal: true
    cmd: sops --encrypt --in-place {{.file}}
    requires:
      vars: ["file"]
    preconditions:
      - msg: Missing Sops config file
        sh: test -f {{.SOPS_CONFIG_FILE}}
      - msg: Missing Sops Age key file
        sh: test -f {{.AGE_FILE}}

  .reset:
    internal: true
    cmd: rm -rf {{.SOPS_CONFIG_FILE}}

File: ./.taskfiles/Workstation/Taskfile.yaml
---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: "3"

vars:
  ARCHFILE: "{{.ROOT_DIR}}/.taskfiles/Workstation/Archfile"
  BREWFILE: "{{.ROOT_DIR}}/.taskfiles/Workstation/Brewfile"
  GENERIC_BIN_DIR: "{{.ROOT_DIR}}/.bin"

tasks:

  direnv:
    desc: Run direnv hooks
    cmd: direnv allow .
    status:
      - "[[ $(direnv status --json | jq '.state.foundRC.allowed') == 0 ]]"
      - "[[ $(direnv status --json | jq '.state.loadedRC.allowed') == 0 ]]"

  venv:
    desc: Set up virtual environment
    cmds:
      - "{{.PYTHON_BIN}} -m venv {{.VIRTUAL_ENV}}"
      - '{{.VIRTUAL_ENV}}/bin/python3 -m pip install --upgrade pip setuptools wheel'
      - '{{.VIRTUAL_ENV}}/bin/python3 -m pip install --upgrade --requirement "{{.PIP_REQUIREMENTS_FILE}}"'
    sources:
      - "{{.PIP_REQUIREMENTS_FILE}}"
    generates:
      - "{{.VIRTUAL_ENV}}/pyvenv.cfg"
    preconditions:
      - { msg: "Missing Pip requirements file", sh: "test -f {{.PIP_REQUIREMENTS_FILE}}" }

  brew:
    desc: Install workstation dependencies with Brew
    cmd: brew bundle --file {{.BREWFILE}}
    preconditions:
      - { msg: "Missing Homebrew", sh: "command -v brew" }
      - { msg: "Missing Brewfile", sh: "test -f {{.BREWFILE}}" }

  arch:
    desc: Install Arch workstation dependencies with Paru Or Yay
    cmd: "{{.helper}} -Syu --needed --noconfirm --noprogressbar $(cat {{.ARCHFILE}} | xargs)"
    vars:
      helper:
        sh: "command -v yay || command -v paru"
    preconditions:
      - { msg: "Missing Archfile", sh: "test -f {{.ARCHFILE}}" }

  generic-linux:
    desc: Install CLI tools into the projects .bin directory using curl
    dir: "{{.GENERIC_BIN_DIR}}"
    platforms: ["linux/amd64", "linux/arm64"]
    cmds:
      - for:
          - budimanjojo/talhelper?as=talhelper&type=script
          - cloudflare/cloudflared?as=cloudflared&type=script
          - FiloSottile/age?as=age&type=script
          - fluxcd/flux2?as=flux&type=script
          - getsops/sops?as=sops&type=script
          - helmfile/helmfile?as=helmfile&type=script
          - jqlang/jq?as=jq&type=script
          - kubernetes-sigs/kustomize?as=kustomize&type=script
          - siderolabs/talos?as=talosctl&type=script
          - yannh/kubeconform?as=kubeconform&type=script
          - mikefarah/yq?as=yq&type=script
        cmd: curl -fsSL "https://i.jpillora.com/{{.ITEM}}" | bash
      - cmd: curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
        platforms: ["linux/amd64"]
      - cmd: curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl"
        platforms: ["linux/arm64"]
      - cmd: chmod +x kubectl
      - cmd: curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | USE_SUDO="false" HELM_INSTALL_DIR="." bash

File: ./.taskfiles/Repository/Taskfile.yaml
---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: "3"

tasks:

  clean:
    desc: Clean files and directories no longer needed after cluster bootstrap
    cmds:
      - mkdir -p {{.PRIVATE_DIR}}
      # Clean up CI
      - rm -rf {{.ROOT_DIR}}/.github/tests
      - rm -rf {{.ROOT_DIR}}/.github/workflows/e2e.yaml
      # Clean up devcontainer
      - rm -rf {{.ROOT_DIR}}/.devcontainer/ci
      - rm -rf {{.ROOT_DIR}}/.github/workflows/devcontainer.yaml
      # Move bootstrap directory to gitignored directory
      - mv {{.BOOTSTRAP_DIR}} {{.PRIVATE_DIR}}/bootstrap-{{now | date "150405"}}
      - mv {{.MAKEJINJA_CONFIG_FILE}} {{.PRIVATE_DIR}}/makejinja-{{now | date "150405"}}.toml
      # Update renovate.json5
      - sed -i {{if eq OS "darwin"}}''{{end}} 's/(..\.j2)\?//g' {{.ROOT_DIR}}/.github/renovate.json5
    preconditions:
      - msg: Missing bootstrap directory
        sh: test -d {{.BOOTSTRAP_DIR}}
      - msg: Missing Renovate config file
        sh: test -f {{.ROOT_DIR}}/.github/renovate.json5

  reset:
    desc: Reset templated configuration files
    prompt: Reset templated configuration files... continue?
    cmds:
      - task: :kubernetes:.reset
      - task: :sops:.reset
      - task: :talos:.reset

  force-reset:
    desc: Reset repo back to HEAD
    prompt: Reset repo back to HEAD... continue?
    cmds:
      - task: reset
      - git reset --hard HEAD
      - git clean -f -d
      - git pull origin main

File: ./.taskfiles/Kubernetes/Taskfile.yaml
---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: "3"

vars:
  KUBECONFORM_SCRIPT: "{{.SCRIPTS_DIR}}/kubeconform.sh"

tasks:

  resources:
    desc: Gather common resources in your cluster, useful when asking for support
    cmds:
      - for: { var: resource }
        cmd: kubectl get {{.ITEM}} {{.CLI_ARGS | default "-A"}}
    vars:
      resource: >-
        nodes
        gitrepositories
        kustomizations
        helmrepositories
        helmreleases
        certificates
        certificaterequests
        ingresses
        pods

  kubeconform:
    desc: Validate Kubernetes manifests with kubeconform
    cmd: bash {{.KUBECONFORM_SCRIPT}} {{.KUBERNETES_DIR}}
    preconditions:
      - msg: Missing kubeconform script
        sh: test -f {{.KUBECONFORM_SCRIPT}}

  .reset:
    internal: true
    cmd: rm -rf {{.KUBERNETES_DIR}}

File: ./.taskfiles/Flux/Taskfile.yaml
---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: "3"

vars:
  CLUSTER_SECRET_SOPS_FILE: "{{.KUBERNETES_DIR}}/flux/vars/cluster-secrets.sops.yaml"
  CLUSTER_SETTINGS_FILE: "{{.KUBERNETES_DIR}}/flux/vars/cluster-settings.yaml"
  GITHUB_DEPLOY_KEY_FILE: "{{.KUBERNETES_DIR}}/bootstrap/flux/github-deploy-key.sops.yaml"

tasks:

  bootstrap:
    desc: Bootstrap Flux into a Kubernetes cluster
    cmds:
      - kubectl apply --kubeconfig {{.KUBECONFIG_FILE}} --server-side --kustomize {{.KUBERNETES_DIR}}/bootstrap/flux
      - |
        if ! kubectl --kubeconfig {{.KUBECONFIG_FILE}} -n flux-system get secret sops-age >/dev/null 2>&1; then
          cat {{.AGE_FILE}} | kubectl --kubeconfig {{.KUBECONFIG_FILE}} -n flux-system create secret generic sops-age --from-file=age.agekey=/dev/stdin
        else
          echo "sops-age secret already exists, skipping creation."
        fi
      - sops --decrypt {{.CLUSTER_SECRET_SOPS_FILE}} | kubectl apply --kubeconfig {{.KUBECONFIG_FILE}} --server-side --filename -
      - kubectl apply --kubeconfig {{.KUBECONFIG_FILE}} --server-side --filename {{.CLUSTER_SETTINGS_FILE}}
      - kubectl apply --kubeconfig {{.KUBECONFIG_FILE}} --server-side --kustomize {{.KUBERNETES_DIR}}/flux/config
    preconditions:
      - msg: Missing kubeconfig
        sh: test -f {{.KUBECONFIG_FILE}}
      - msg: Missing Sops Age key file
        sh: test -f {{.AGE_FILE}}

  apply:
    desc: Apply a Flux Kustomization resource for a cluster
    summary: |
      Args:
        path: Path under apps containing the Flux Kustomization resource (ks.yaml) (required)
        ns: Namespace the Flux Kustomization exists in (default: flux-system)
    cmd: |
      flux --kubeconfig {{.KUBECONFIG_FILE}} build ks $(basename {{.path}}) \
          --namespace {{.ns}} \
          --kustomization-file {{.KUBERNETES_DIR}}/apps/{{.path}}/ks.yaml \
          --path {{.KUBERNETES_DIR}}/apps/{{.path}} \
          {{- if contains "not found" .ks }}--dry-run \{{ end }}
      | \
      kubectl apply --kubeconfig {{.KUBECONFIG_FILE}} --server-side \
          --field-manager=kustomize-controller -f -
    requires:
      vars: ["path"]
    vars:
      ns: '{{.ns | default "flux-system"}}'
      ks:
        sh: flux --kubeconfig {{.KUBECONFIG_FILE}} --namespace {{.ns}} get kustomizations $(basename {{.path}}) 2>&1
    preconditions:
      - msg: Missing kubeconfig
        sh: test -f {{.KUBECONFIG_FILE}}
      - msg: Missing Flux Kustomization for app {{.path}}
        sh: test -f {{.KUBERNETES_DIR}}/apps/{{.path}}/ks.yaml

  reconcile:
    desc: Force update Flux to pull in changes from your Git repository
    cmd: flux --kubeconfig {{.KUBECONFIG_FILE}} reconcile --namespace flux-system kustomization cluster --with-source
    preconditions:
      - msg: Missing kubeconfig
        sh: test -f {{.KUBECONFIG_FILE}}

  github-deploy-key:
    cmds:
      - kubectl create namespace flux-system --dry-run=client -o yaml | kubectl --kubeconfig {{.KUBECONFIG_FILE}} apply --filename -
      - sops --decrypt {{.GITHUB_DEPLOY_KEY_FILE}} | kubectl apply --kubeconfig {{.KUBECONFIG_FILE}} --server-side --filename -
    preconditions:
      - msg: Missing kubeconfig
        sh: test -f {{.KUBECONFIG_FILE}}
      - msg: Missing Sops Age key file
        sh: test -f {{.AGE_FILE}}
      - msg: Missing Github deploy key file
        sh: test -f {{.GITHUB_DEPLOY_KEY_FILE}}

File: ./.sops.yaml
creation_rules:
path_regex:
key_groups:
encrypted_regex:
key_groups:

