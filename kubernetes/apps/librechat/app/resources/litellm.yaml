---
model_list:
  - model_name: gpt-4o ### RECEIVED MODEL NAME ###
    litellm_params: # all params accepted by litellm.completion() - https://docs.litellm.ai/docs/completion/input
      model: openai/gpt-4o-2024-08-06 ### MODEL NAME sent to `litellm.completion()` ###
      api_key: os.environ/OPENAI_API_KEY # does os.getenv("OPENAI_API_KEY")
      max_tokens: 16384

  - model_name: gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini
      api_key: os.environ/OPENAI_API_KEY
      max_tokens: 16384

  - model_name: claude-3-5-sonnet
    litellm_params:
      model: claude-3-5-sonnet-20240620
      api_key: "os.environ/ANTHROPIC_API_KEY"
      max_tokens: 8192

  - model_name: deepseek-r1:14b
    litellm_params:
      model: "ollama/deepseek-r1:14b"
      api_base: "http://10.10.11.109:11434"
      max_tokens: 131000

litellm_settings: # module level litellm settings - https://github.com/BerriAI/litellm/blob/main/litellm/__init__.py
  drop_params: True
  default_max_tokens: 8192
  success_callback: ["langfuse"] # OPTIONAL - if you want to start sending LLM Logs to Langfuse. Make sure to set `LANGFUSE_PUBLIC_KEY` and `LANGFUSE_SECRET_KEY` in your env
  cache: True
  cache_params:
    type: redis
    supported_call_types:
      ["acompletion", "completion", "embedding", "aembedding"]
    host: dragonfly.database.svc.cluster.local
    port: 6379
    db: 7

router_settings:
  routing_strategy: usage-based-routing-v2
  redis_host: dragonfly.database.svc.cluster.local
  redis_port: 6379
  redis_db: 7
  default_max_tokens: 8192

general_settings:
  default_max_tokens: 8192
  # master_key: sk-1234 # [OPTIONAL] Only use this if you to require all calls to contain this key (Authorization: Bearer sk-1234)
  # alerting: ["slack"] # [OPTIONAL] If you want Slack Alerts for Hanging LLM requests, Slow llm responses, Budget Alerts. Make sure to set `SLACK_WEBHOOK_URL` in your env
