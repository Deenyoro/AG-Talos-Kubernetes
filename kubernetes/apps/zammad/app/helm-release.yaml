apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app zammad
  namespace: default
spec:
  interval: 30m
  chart:
    spec:
      chart: zammad
      version: 14.3.0
      sourceRef:
        kind: HelmRepository
        name: zammad
        namespace: flux-system
  maxHistory: 2
  install:
    createNamespace: true
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  uninstall:
    keepHistory: false
  values:
    image:
      repository: ghcr.io/zammad/zammad
      tag: "6.5@sha256:33f886768deb865da4705bfbfcf0317d20858867f54a79b3b72dc5d06f55d03b"

    service:
      type: ClusterIP
      port: 8080

    ingress:
      enabled: true
      className: "external"
      annotations:
        external-dns.alpha.kubernetes.io/target: "external.${SECRET_DOMAIN}"
        nginx.ingress.kubernetes.io/proxy-body-size: 100M
        nginx.ingress.kubernetes.io/enable-cors: "true"
        nginx.ingress.kubernetes.io/cors-allow-headers: "X-Forwarded-For"
      hosts:
        - host: &host helpdesk.${SECRET_DOMAIN}
          paths:
            - path: /
              pathType: ImplementationSpecific
      tls:
        - hosts:
            - *host

    # Fix Elasticsearch resource allocation - configure at top level for proper resource limits
    elasticsearch:
      # Workaround: switch to bitnami legacy image repository (https://github.com/bitnami/containers/issues/83267)
      image:
        repository: bitnamilegacy/elasticsearch
      global:
        security:
          allowInsecureImages: true
      clusterName: zammad
      coordinating:
        replicaCount: 0
      data:
        replicaCount: 0
      ingest:
        replicaCount: 0
      master:
        heapSize: 4g
        masterOnly: false
        replicaCount: 1
        resources:
          requests:
            cpu: 2
            memory: 6Gi
          limits:
            cpu: 4
            memory: 8Gi

    zammadConfig:
      elasticsearch:
        enabled: true
        host: zammad-elasticsearch-master
        initialisation: true
        pass: ""
        port: 9200
        reindex: false
        schema: http
        user: ""
      postgresql:
        enabled: false
        db: zammad
        host: postgres16-rw.database.svc.cluster.local
        user: zammad

      scheduler:
        # Redundancy: Run 2 scheduler replicas for high availability
        # Only one will be active (Zammad handles this internally), but provides failover
        replicas: 2

        # Fix for GitHub issue #5718: Background scheduler doesn't re-schedule things properly
        # after losing and regaining database connection. Add liveness probe to detect when
        # scheduler loses database connectivity and ensure proper recovery via pod restart.
        livenessProbe:
          exec:
            command:
              - /bin/sh
              - -c
              - |
                # Test database connectivity by attempting a simple query
                # This ensures the scheduler can actually communicate with the database
                # and will restart the pod if database connectivity is lost and not recovered
                bundle exec rails runner "
                begin
                  # Simple database connectivity test
                  ActiveRecord::Base.connection.execute('SELECT 1')
                  exit 0
                rescue => e
                  STDERR.puts \"Database connection failed: #{e.message}\"
                  exit 1
                end
                " 2>/dev/null || exit 1
          initialDelaySeconds: 120
          periodSeconds: 300
          timeoutSeconds: 30
          failureThreshold: 2

        # Add metrics sidecar for comprehensive monitoring
        sidecars:
          - name: metrics-exporter
            image: ghcr.io/zammad/zammad:6.5.0-101
            command:
              - /bin/sh
              - -c
              - |
                cd /opt/zammad
                ruby /opt/zammad/metrics-exporter.rb
            ports:
              - name: metrics
                containerPort: 9090
                protocol: TCP
            resources:
              requests:
                cpu: 10m
                memory: 32Mi
              limits:
                cpu: 50m
                memory: 64Mi
            volumeMounts:
              - name: zammad-metrics-config
                mountPath: /opt/zammad/metrics-exporter.rb
                subPath: metrics-exporter.rb
                readOnly: true
            securityContext:
              allowPrivilegeEscalation: false
              capabilities:
                drop:
                  - ALL
              readOnlyRootFilesystem: true
              privileged: false

        # Add volume for metrics exporter script and monitoring annotations
        podAnnotations:
          prometheus.io/scrape: "true"
          prometheus.io/port: "9090"
          prometheus.io/path: "/metrics"

    # Add custom volumes for metrics exporter
    customVolumes:
      - name: zammad-metrics-config
        configMap:
          name: zammad-scheduler-metrics
          defaultMode: 0755

    # Add custom volume mounts for all Zammad pods
    customVolumeMounts:
      - name: zammad-metrics-config
        mountPath: /opt/zammad/metrics-exporter.rb
        subPath: metrics-exporter.rb
        readOnly: true

    # Note: Passwords should not contain special characters requiring URL encoding
    secrets:
      autowizard:
        useExisting: false
        secretKey: autowizard
        secretName: autowizard
      elasticsearch:
        useExisting: false
        secretKey: password
        secretName: elastic-credentials
      postgresql:
        useExisting: true
        secretKey: postgresql-pass
        secretName: zammad-postgres-secrets
      redis:
        useExisting: false
        secretKey: redis-password
        secretName: redis-pass
