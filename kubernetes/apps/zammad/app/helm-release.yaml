apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app zammad
  namespace: default
spec:
  interval: 30m
  chart:
    spec:
      chart: zammad
      version: 14.3.0
      sourceRef:
        kind: HelmRepository
        name: zammad
        namespace: flux-system
  maxHistory: 2
  install:
    createNamespace: true
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  uninstall:
    keepHistory: false
  values:
    image:
      repository: ghcr.io/zammad/zammad
      tag: "6.5@sha256:7ac9b669ffb5a75389c69edac97f136c6befa12327bf0cd037b7ddabaf5aa62f"

    service:
      type: ClusterIP
      port: 8080

    ingress:
      enabled: true
      className: "external"
      annotations:
        external-dns.alpha.kubernetes.io/target: "external.${SECRET_DOMAIN}"
        nginx.ingress.kubernetes.io/proxy-body-size: 100M
        nginx.ingress.kubernetes.io/enable-cors: "true"
        nginx.ingress.kubernetes.io/cors-allow-headers: "X-Forwarded-For"
      hosts:
        - host: &host helpdesk.${SECRET_DOMAIN}
          paths:
            - path: /
              pathType: ImplementationSpecific
      tls:
        - hosts:
            - *host

    # Fix Elasticsearch resource allocation - configure at top level for proper resource limits
    elasticsearch:
      # Workaround: switch to bitnami legacy image repository (https://github.com/bitnami/containers/issues/83267)
      image:
        repository: bitnamilegacy/elasticsearch
      global:
        security:
          allowInsecureImages: true
      clusterName: zammad
      coordinating:
        replicaCount: 0
      data:
        replicaCount: 0
      ingest:
        replicaCount: 0
      master:
        heapSize: 4g
        masterOnly: false
        replicaCount: 1
        resources:
          requests:
            cpu: 2
            memory: 6Gi
          limits:
            cpu: 4
            memory: 8Gi

    zammadConfig:
      elasticsearch:
        enabled: true
        host: zammad-elasticsearch-master
        initialisation: true
        pass: ""
        port: 9200
        reindex: false
        schema: http
        user: ""
      postgresql:
        enabled: false
        db: zammad
        host: postgres16-rw.database.svc.cluster.local
        user: zammad

      scheduler:
        # Redundancy: Run 2 scheduler replicas for high availability
        # Only one will be active (Zammad handles this internally), but provides failover
        replicas: 2

        # Enhanced liveness probe: Detects database connectivity issues AND job processing problems
        # This prevents silent failures where the scheduler appears healthy but isn't processing jobs
        livenessProbe:
          exec:
            command:
              - /bin/sh
              - -c
              - |
                cd /opt/zammad
                bundle exec rails runner "
                begin
                  # Test database connectivity
                  ActiveRecord::Base.connection.execute('SELECT 1')

                  # Check for jobs stuck for too long (indicates scheduler not processing)
                  old_jobs = Delayed::Job.where(failed_at: nil, locked_at: nil)
                                        .where('run_at <= ?', Time.now - 900)  # 15 minutes
                                        .count

                  if old_jobs > 20
                    STDERR.puts \"LIVENESS FAIL: #{old_jobs} jobs stuck for >15 minutes\"
                    exit 1
                  end

                  # Check for excessive job backlog (indicates scheduler overwhelmed or stuck)
                  pending_jobs = Delayed::Job.where(failed_at: nil, locked_at: nil)
                                            .where('run_at <= ?', Time.now)
                                            .count

                  if pending_jobs > 200
                    STDERR.puts \"LIVENESS FAIL: Excessive job backlog (#{pending_jobs} jobs)\"
                    exit 1
                  end

                  puts 'Scheduler health: OK'
                  exit 0
                rescue => e
                  STDERR.puts \"Scheduler health check failed: #{e.message}\"
                  exit 1
                end
                " 2>/dev/null || exit 1
          initialDelaySeconds: 180
          periodSeconds: 300
          timeoutSeconds: 60
          failureThreshold: 2

    # Note: Passwords should not contain special characters requiring URL encoding
    secrets:
      autowizard:
        useExisting: false
        secretKey: autowizard
        secretName: autowizard
      elasticsearch:
        useExisting: false
        secretKey: password
        secretName: elastic-credentials
      postgresql:
        useExisting: true
        secretKey: postgresql-pass
        secretName: zammad-postgres-secrets
      redis:
        useExisting: false
        secretKey: redis-password
        secretName: redis-pass
